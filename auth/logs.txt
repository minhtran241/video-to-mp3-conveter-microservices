* 
* ==> Audit <==
* |--------------|------------------------|----------|---------|---------|---------------------|---------------------|
|   Command    |          Args          | Profile  |  User   | Version |     Start Time      |      End Time       |
|--------------|------------------------|----------|---------|---------|---------------------|---------------------|
| update-check |                        | minikube | macbook | v1.28.0 | 15 Nov 22 16:32 EST | 15 Nov 22 16:32 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 15 Nov 22 21:14 EST | 15 Nov 22 21:14 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 16 Nov 22 12:29 EST | 16 Nov 22 12:29 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 16 Nov 22 12:32 EST | 16 Nov 22 12:32 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 16 Nov 22 13:27 EST | 16 Nov 22 13:27 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 16 Nov 22 13:29 EST | 16 Nov 22 13:29 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 16 Nov 22 22:04 EST | 16 Nov 22 22:04 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 17 Nov 22 13:23 EST | 17 Nov 22 13:23 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 17 Nov 22 15:45 EST | 17 Nov 22 15:45 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 17 Nov 22 17:11 EST | 17 Nov 22 17:11 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 17 Nov 22 17:12 EST | 17 Nov 22 17:12 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 18 Nov 22 11:38 EST | 18 Nov 22 11:38 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 18 Nov 22 23:10 EST | 18 Nov 22 23:10 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 20 Nov 22 13:07 EST | 20 Nov 22 13:07 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 20 Nov 22 13:09 EST | 20 Nov 22 13:09 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 20 Nov 22 21:58 EST | 20 Nov 22 21:58 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 20 Nov 22 23:08 EST | 20 Nov 22 23:08 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 20 Nov 22 23:24 EST | 20 Nov 22 23:24 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 22 Nov 22 10:17 EST | 22 Nov 22 10:17 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 22 Nov 22 16:56 EST | 22 Nov 22 16:56 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 22 Nov 22 16:57 EST | 22 Nov 22 16:57 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 22 Nov 22 17:26 EST | 22 Nov 22 17:26 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 22 Nov 22 17:27 EST | 22 Nov 22 17:27 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 22 Nov 22 17:28 EST | 22 Nov 22 17:28 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 22 Nov 22 17:29 EST | 22 Nov 22 17:29 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 22 Nov 22 17:29 EST | 22 Nov 22 17:29 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 22 Nov 22 17:34 EST | 22 Nov 22 17:34 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 22 Nov 22 17:35 EST | 22 Nov 22 17:35 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 22 Nov 22 21:09 EST | 22 Nov 22 21:09 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 22 Nov 22 21:10 EST | 22 Nov 22 21:10 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 22 Nov 22 21:14 EST | 22 Nov 22 21:14 EST |
| service      | list                   | minikube | macbook | v1.28.0 | 22 Nov 22 22:35 EST |                     |
| start        |                        | minikube | macbook | v1.28.0 | 22 Nov 22 22:36 EST | 22 Nov 22 22:37 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 22 Nov 22 22:47 EST | 22 Nov 22 22:47 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 22 Nov 22 22:56 EST | 22 Nov 22 22:56 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 22 Nov 22 23:05 EST | 22 Nov 22 23:05 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 23 Nov 22 14:33 EST | 23 Nov 22 14:33 EST |
| addons       |                        | minikube | macbook | v1.28.0 | 23 Nov 22 22:36 EST | 23 Nov 22 22:36 EST |
| addons       | list                   | minikube | macbook | v1.28.0 | 23 Nov 22 22:36 EST | 23 Nov 22 22:36 EST |
| addons       | enable ingress         | minikube | macbook | v1.28.0 | 23 Nov 22 22:37 EST | 23 Nov 22 22:39 EST |
| addons       | list                   | minikube | macbook | v1.28.0 | 23 Nov 22 22:42 EST | 23 Nov 22 22:42 EST |
| tunnel       |                        | minikube | macbook | v1.28.0 | 23 Nov 22 22:43 EST | 23 Nov 22 22:44 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 23 Nov 22 22:48 EST | 23 Nov 22 22:48 EST |
| tunnel       |                        | minikube | macbook | v1.28.0 | 23 Nov 22 23:32 EST | 23 Nov 22 23:32 EST |
| tunnel       |                        | minikube | macbook | v1.28.0 | 23 Nov 22 23:33 EST | 23 Nov 22 23:35 EST |
| service      | list                   | minikube | macbook | v1.28.0 | 23 Nov 22 23:35 EST | 23 Nov 22 23:36 EST |
| service      | system-design-rabbitmq | minikube | macbook | v1.28.0 | 23 Nov 22 23:36 EST | 23 Nov 22 23:36 EST |
| addons       | list                   | minikube | macbook | v1.28.0 | 23 Nov 22 23:37 EST | 23 Nov 22 23:37 EST |
| tunnel       | --cleanup              | minikube | macbook | v1.28.0 | 23 Nov 22 23:43 EST | 23 Nov 22 23:43 EST |
| tunnel       |                        | minikube | macbook | v1.28.0 | 23 Nov 22 23:43 EST | 23 Nov 22 23:46 EST |
| tunnel       |                        | minikube | macbook | v1.28.0 | 23 Nov 22 23:48 EST | 23 Nov 22 23:49 EST |
| tunnel       |                        | minikube | macbook | v1.28.0 | 23 Nov 22 23:51 EST | 23 Nov 22 23:51 EST |
| tunnel       |                        | minikube | macbook | v1.28.0 | 23 Nov 22 23:54 EST |                     |
| update-check |                        | minikube | macbook | v1.28.0 | 24 Nov 22 11:02 EST | 24 Nov 22 11:02 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 24 Nov 22 11:03 EST | 24 Nov 22 11:03 EST |
| update-check |                        | minikube | macbook | v1.28.0 | 24 Nov 22 11:11 EST | 24 Nov 22 11:11 EST |
| ssh          |                        | minikube | macbook | v1.28.0 | 24 Nov 22 11:18 EST |                     |
| ssh          |                        | minikube | macbook | v1.28.0 | 24 Nov 22 11:24 EST |                     |
| update-check |                        | minikube | macbook | v1.28.0 | 24 Nov 22 11:36 EST | 24 Nov 22 11:36 EST |
| tunnel       |                        | minikube | macbook | v1.28.0 | 24 Nov 22 16:31 EST |                     |
|--------------|------------------------|----------|---------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2022/11/22 22:36:09
Running on machine: Mints-MacBook-Pro
Binary: Built with gc go1.19.3 for darwin/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1122 22:36:09.263441   29924 out.go:296] Setting OutFile to fd 1 ...
I1122 22:36:09.263736   29924 out.go:348] isatty.IsTerminal(1) = true
I1122 22:36:09.263740   29924 out.go:309] Setting ErrFile to fd 2...
I1122 22:36:09.263747   29924 out.go:348] isatty.IsTerminal(2) = true
I1122 22:36:09.263953   29924 root.go:334] Updating PATH: /Users/macbook/.minikube/bin
W1122 22:36:09.264134   29924 root.go:311] Error reading config file at /Users/macbook/.minikube/config/config.json: open /Users/macbook/.minikube/config/config.json: no such file or directory
I1122 22:36:09.266872   29924 out.go:303] Setting JSON to false
I1122 22:36:09.306152   29924 start.go:116] hostinfo: {"hostname":"Mints-MacBook-Pro.local","uptime":78824,"bootTime":1669095745,"procs":625,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"12.6","kernelVersion":"21.6.0","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"a7fbba91-024f-5fb4-b49c-76882cda6301"}
W1122 22:36:09.306312   29924 start.go:124] gopshost.Virtualization returned error: not implemented yet
I1122 22:36:09.331134   29924 out.go:177] 😄  minikube v1.28.0 on Darwin 12.6
I1122 22:36:09.377634   29924 notify.go:220] Checking for updates...
I1122 22:36:09.377739   29924 config.go:180] Loaded profile config "minikube": Driver=hyperkit, ContainerRuntime=docker, KubernetesVersion=v1.25.3
I1122 22:36:09.379251   29924 driver.go:365] Setting default libvirt URI to qemu:///system
I1122 22:36:09.379819   29924 main.go:134] libmachine: Found binary path at /Users/macbook/.minikube/bin/docker-machine-driver-hyperkit
I1122 22:36:09.379894   29924 main.go:134] libmachine: Launching plugin server for driver hyperkit
I1122 22:36:09.391936   29924 main.go:134] libmachine: Plugin server listening at address 127.0.0.1:60390
I1122 22:36:09.392524   29924 main.go:134] libmachine: () Calling .GetVersion
I1122 22:36:09.393175   29924 main.go:134] libmachine: Using API Version  1
I1122 22:36:09.393193   29924 main.go:134] libmachine: () Calling .SetConfigRaw
I1122 22:36:09.393498   29924 main.go:134] libmachine: () Calling .GetMachineName
I1122 22:36:09.393648   29924 main.go:134] libmachine: (minikube) Calling .DriverName
I1122 22:36:09.456843   29924 out.go:177] ✨  Using the hyperkit driver based on existing profile
I1122 22:36:09.500190   29924 start.go:282] selected driver: hyperkit
I1122 22:36:09.500591   29924 start.go:808] validating driver "hyperkit" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.28.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:hyperkit HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.64.2 Port:8443 KubernetesVersion:v1.25.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/var/run/socket_vmnet}
I1122 22:36:09.500870   29924 start.go:819] status for hyperkit: {Installed:true Healthy:true Running:true NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1122 22:36:09.501381   29924 install.go:52] acquiring lock: {Name:mk4023283b30b374c3f04c8805d539e68824c0b8 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1122 22:36:09.502015   29924 install.go:117] Validating docker-machine-driver-hyperkit, PATH=/Users/macbook/.minikube/bin:/Users/macbook/opt/anaconda3/bin:/Users/macbook/.local/bin:/Users/macbook/.sdkman/candidates/sbt/current/bin:/Library/Frameworks/Python.framework/Versions/3.9/bin:/Library/Frameworks/Python.framework/Versions/3.10/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/go/bin:/usr/local/share/dotnet:~/.dotnet/tools:/Library/Frameworks/Mono.framework/Versions/Current/Commands:/Users/macbook/.fig/bin:/Users/macbook/.local/bin:/Users/macbook/.local/bin:/usr/local/bin:/Users/macbook/Library/Application Support/Coursier/bin:/Users/macbook/.local/bin
I1122 22:36:09.514484   29924 install.go:137] /Users/macbook/.minikube/bin/docker-machine-driver-hyperkit version is 1.28.0
I1122 22:36:09.523241   29924 install.go:79] stdout: /Users/macbook/.minikube/bin/docker-machine-driver-hyperkit
I1122 22:36:09.523301   29924 install.go:81] /Users/macbook/.minikube/bin/docker-machine-driver-hyperkit looks good
I1122 22:36:09.523706   29924 cni.go:95] Creating CNI manager for ""
I1122 22:36:09.524022   29924 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I1122 22:36:09.524040   29924 start_flags.go:317] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.28.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:hyperkit HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.64.2 Port:8443 KubernetesVersion:v1.25.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/var/run/socket_vmnet}
I1122 22:36:09.524693   29924 iso.go:124] acquiring lock: {Name:mk51b157c2d0d60cf8f824566839db90e8977c71 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1122 22:36:09.569308   29924 out.go:177] 👍  Starting control plane node minikube in cluster minikube
I1122 22:36:09.590733   29924 preload.go:132] Checking if preload exists for k8s version v1.25.3 and runtime docker
I1122 22:36:09.591049   29924 preload.go:148] Found local preload: /Users/macbook/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.3-docker-overlay2-amd64.tar.lz4
I1122 22:36:09.591097   29924 cache.go:57] Caching tarball of preloaded images
I1122 22:36:09.591606   29924 preload.go:174] Found /Users/macbook/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1122 22:36:09.591625   29924 cache.go:60] Finished verifying existence of preloaded tar for  v1.25.3 on docker
I1122 22:36:09.591814   29924 profile.go:148] Saving config to /Users/macbook/.minikube/profiles/minikube/config.json ...
I1122 22:36:09.593039   29924 cache.go:208] Successfully downloaded all kic artifacts
I1122 22:36:09.593114   29924 start.go:364] acquiring machines lock for minikube: {Name:mk6ce91989a94dd2a31a6fb91199c65021eb1113 Clock:{} Delay:500ms Timeout:13m0s Cancel:<nil>}
I1122 22:36:09.593314   29924 start.go:368] acquired machines lock for "minikube" in 186.349µs
I1122 22:36:09.593345   29924 start.go:96] Skipping create...Using existing machine configuration
I1122 22:36:09.593361   29924 fix.go:55] fixHost starting: 
I1122 22:36:09.593726   29924 main.go:134] libmachine: Found binary path at /Users/macbook/.minikube/bin/docker-machine-driver-hyperkit
I1122 22:36:09.593754   29924 main.go:134] libmachine: Launching plugin server for driver hyperkit
I1122 22:36:09.606325   29924 main.go:134] libmachine: Plugin server listening at address 127.0.0.1:60393
I1122 22:36:09.607039   29924 main.go:134] libmachine: () Calling .GetVersion
I1122 22:36:09.607689   29924 main.go:134] libmachine: Using API Version  1
I1122 22:36:09.607710   29924 main.go:134] libmachine: () Calling .SetConfigRaw
I1122 22:36:09.608024   29924 main.go:134] libmachine: () Calling .GetMachineName
I1122 22:36:09.608186   29924 main.go:134] libmachine: (minikube) Calling .DriverName
I1122 22:36:09.608344   29924 main.go:134] libmachine: (minikube) Calling .GetState
I1122 22:36:09.608483   29924 main.go:134] libmachine: (minikube) DBG | exe=/Users/macbook/.minikube/bin/docker-machine-driver-hyperkit uid=0
I1122 22:36:09.608589   29924 main.go:134] libmachine: (minikube) DBG | hyperkit pid from json: 13838
I1122 22:36:09.610831   29924 main.go:134] libmachine: (minikube) DBG | hyperkit pid 13838 missing from process table
I1122 22:36:09.610884   29924 fix.go:103] recreateIfNeeded on minikube: state=Stopped err=<nil>
I1122 22:36:09.610921   29924 main.go:134] libmachine: (minikube) Calling .DriverName
W1122 22:36:09.611042   29924 fix.go:129] unexpected machine state, will restart: <nil>
I1122 22:36:09.632781   29924 out.go:177] 🔄  Restarting existing hyperkit VM for "minikube" ...
I1122 22:36:09.675728   29924 main.go:134] libmachine: (minikube) Calling .Start
I1122 22:36:09.676312   29924 main.go:134] libmachine: (minikube) DBG | exe=/Users/macbook/.minikube/bin/docker-machine-driver-hyperkit uid=0
I1122 22:36:09.676468   29924 main.go:134] libmachine: (minikube) minikube might have been shutdown in an unclean way, the hyperkit pid file still exists: /Users/macbook/.minikube/machines/minikube/hyperkit.pid
I1122 22:36:09.679211   29924 main.go:134] libmachine: (minikube) DBG | hyperkit pid 13838 missing from process table
I1122 22:36:09.679231   29924 main.go:134] libmachine: (minikube) DBG | pid 13838 is in state "Stopped"
I1122 22:36:09.679285   29924 main.go:134] libmachine: (minikube) DBG | Removing stale pid file /Users/macbook/.minikube/machines/minikube/hyperkit.pid...
I1122 22:36:09.680398   29924 main.go:134] libmachine: (minikube) DBG | Using UUID ee46850a-62cb-11ed-829f-f2189895b8a2
I1122 22:36:10.275782   29924 main.go:134] libmachine: (minikube) DBG | Generated MAC 76:cf:d0:5d:c6:48
I1122 22:36:10.275826   29924 main.go:134] libmachine: (minikube) DBG | Starting with cmdline: loglevel=3 console=ttyS0 console=tty0 noembed nomodeset norestore waitusb=10 systemd.legacy_systemd_cgroup_controller=yes random.trust_cpu=on hw_rng_model=virtio base host=minikube
I1122 22:36:10.276150   29924 main.go:134] libmachine: (minikube) DBG | 2022/11/22 22:36:10 DEBUG: hyperkit: Start &hyperkit.HyperKit{HyperKit:"/usr/local/bin/hyperkit", Argv0:"", StateDir:"/Users/macbook/.minikube/machines/minikube", VPNKitSock:"", VPNKitUUID:"", VPNKitPreferredIPv4:"", UUID:"ee46850a-62cb-11ed-829f-f2189895b8a2", Disks:[]hyperkit.Disk{(*hyperkit.RawDisk)(0xc00055af60)}, ISOImages:[]string{"/Users/macbook/.minikube/machines/minikube/boot2docker.iso"}, VSock:false, VSockDir:"", VSockPorts:[]int(nil), VSockGuestCID:3, VMNet:true, Sockets9P:[]hyperkit.Socket9P(nil), Kernel:"/Users/macbook/.minikube/machines/minikube/bzimage", Initrd:"/Users/macbook/.minikube/machines/minikube/initrd", Bootrom:"", CPUs:2, Memory:4000, Console:1, Serials:[]hyperkit.Serial(nil), Pid:0, Arguments:[]string(nil), CmdLine:"", process:(*os.Process)(nil)}
I1122 22:36:10.276209   29924 main.go:134] libmachine: (minikube) DBG | 2022/11/22 22:36:10 DEBUG: hyperkit: check &hyperkit.HyperKit{HyperKit:"/usr/local/bin/hyperkit", Argv0:"", StateDir:"/Users/macbook/.minikube/machines/minikube", VPNKitSock:"", VPNKitUUID:"", VPNKitPreferredIPv4:"", UUID:"ee46850a-62cb-11ed-829f-f2189895b8a2", Disks:[]hyperkit.Disk{(*hyperkit.RawDisk)(0xc00055af60)}, ISOImages:[]string{"/Users/macbook/.minikube/machines/minikube/boot2docker.iso"}, VSock:false, VSockDir:"", VSockPorts:[]int(nil), VSockGuestCID:3, VMNet:true, Sockets9P:[]hyperkit.Socket9P(nil), Kernel:"/Users/macbook/.minikube/machines/minikube/bzimage", Initrd:"/Users/macbook/.minikube/machines/minikube/initrd", Bootrom:"", CPUs:2, Memory:4000, Console:1, Serials:[]hyperkit.Serial(nil), Pid:0, Arguments:[]string(nil), CmdLine:"", process:(*os.Process)(nil)}
I1122 22:36:10.277430   29924 main.go:134] libmachine: (minikube) DBG | 2022/11/22 22:36:10 DEBUG: hyperkit: Arguments: []string{"-A", "-u", "-F", "/Users/macbook/.minikube/machines/minikube/hyperkit.pid", "-c", "2", "-m", "4000M", "-s", "0:0,hostbridge", "-s", "31,lpc", "-s", "1:0,virtio-net", "-U", "ee46850a-62cb-11ed-829f-f2189895b8a2", "-s", "2:0,virtio-blk,/Users/macbook/.minikube/machines/minikube/minikube.rawdisk", "-s", "3,ahci-cd,/Users/macbook/.minikube/machines/minikube/boot2docker.iso", "-s", "4,virtio-rnd", "-l", "com1,autopty=/Users/macbook/.minikube/machines/minikube/tty,log=/Users/macbook/.minikube/machines/minikube/console-ring", "-f", "kexec,/Users/macbook/.minikube/machines/minikube/bzimage,/Users/macbook/.minikube/machines/minikube/initrd,earlyprintk=serial loglevel=3 console=ttyS0 console=tty0 noembed nomodeset norestore waitusb=10 systemd.legacy_systemd_cgroup_controller=yes random.trust_cpu=on hw_rng_model=virtio base host=minikube"}
I1122 22:36:10.277526   29924 main.go:134] libmachine: (minikube) DBG | 2022/11/22 22:36:10 DEBUG: hyperkit: CmdLine: "/usr/local/bin/hyperkit -A -u -F /Users/macbook/.minikube/machines/minikube/hyperkit.pid -c 2 -m 4000M -s 0:0,hostbridge -s 31,lpc -s 1:0,virtio-net -U ee46850a-62cb-11ed-829f-f2189895b8a2 -s 2:0,virtio-blk,/Users/macbook/.minikube/machines/minikube/minikube.rawdisk -s 3,ahci-cd,/Users/macbook/.minikube/machines/minikube/boot2docker.iso -s 4,virtio-rnd -l com1,autopty=/Users/macbook/.minikube/machines/minikube/tty,log=/Users/macbook/.minikube/machines/minikube/console-ring -f kexec,/Users/macbook/.minikube/machines/minikube/bzimage,/Users/macbook/.minikube/machines/minikube/initrd,earlyprintk=serial loglevel=3 console=ttyS0 console=tty0 noembed nomodeset norestore waitusb=10 systemd.legacy_systemd_cgroup_controller=yes random.trust_cpu=on hw_rng_model=virtio base host=minikube"
I1122 22:36:10.277594   29924 main.go:134] libmachine: (minikube) DBG | 2022/11/22 22:36:10 DEBUG: hyperkit: Redirecting stdout/stderr to logger
I1122 22:36:10.282493   29924 main.go:134] libmachine: (minikube) DBG | 2022/11/22 22:36:10 DEBUG: hyperkit: Pid is 29941
I1122 22:36:10.283880   29924 main.go:134] libmachine: (minikube) DBG | Attempt 0
I1122 22:36:10.283900   29924 main.go:134] libmachine: (minikube) DBG | exe=/Users/macbook/.minikube/bin/docker-machine-driver-hyperkit uid=0
I1122 22:36:10.284061   29924 main.go:134] libmachine: (minikube) DBG | hyperkit pid from json: 29941
I1122 22:36:10.287414   29924 main.go:134] libmachine: (minikube) DBG | Searching for 76:cf:d0:5d:c6:48 in /var/db/dhcpd_leases ...
I1122 22:36:10.287485   29924 main.go:134] libmachine: (minikube) DBG | Found 1 entries in /var/db/dhcpd_leases!
I1122 22:36:10.287545   29924 main.go:134] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.2 HWAddress:76:cf:d0:5d:c6:48 ID:1,76:cf:d0:5d:c6:48 Lease:0x637cf745}
I1122 22:36:10.287603   29924 main.go:134] libmachine: (minikube) DBG | Found match: 76:cf:d0:5d:c6:48
I1122 22:36:10.287638   29924 main.go:134] libmachine: (minikube) DBG | IP: 192.168.64.2
I1122 22:36:10.287724   29924 main.go:134] libmachine: (minikube) Calling .GetConfigRaw
I1122 22:36:10.289981   29924 main.go:134] libmachine: (minikube) Calling .GetIP
I1122 22:36:10.290396   29924 profile.go:148] Saving config to /Users/macbook/.minikube/profiles/minikube/config.json ...
I1122 22:36:10.291194   29924 machine.go:88] provisioning docker machine ...
I1122 22:36:10.291212   29924 main.go:134] libmachine: (minikube) Calling .DriverName
I1122 22:36:10.291543   29924 main.go:134] libmachine: (minikube) Calling .GetMachineName
I1122 22:36:10.291762   29924 buildroot.go:166] provisioning hostname "minikube"
I1122 22:36:10.291780   29924 main.go:134] libmachine: (minikube) Calling .GetMachineName
I1122 22:36:10.291988   29924 main.go:134] libmachine: (minikube) Calling .GetSSHHostname
I1122 22:36:10.292171   29924 main.go:134] libmachine: (minikube) Calling .GetSSHPort
I1122 22:36:10.292376   29924 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I1122 22:36:10.292571   29924 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I1122 22:36:10.292762   29924 main.go:134] libmachine: (minikube) Calling .GetSSHUsername
I1122 22:36:10.293503   29924 main.go:134] libmachine: Using SSH client type: native
I1122 22:36:10.296892   29924 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003ed820] 0x1003f09a0 <nil>  [] 0s} 192.168.64.2 22 <nil> <nil>}
I1122 22:36:10.296911   29924 main.go:134] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1122 22:36:10.321919   29924 main.go:134] libmachine: (minikube) DBG | 2022/11/22 22:36:10 INFO : hyperkit: stderr: Using fd 5 for I/O notifications
I1122 22:36:10.439699   29924 main.go:134] libmachine: (minikube) DBG | 2022/11/22 22:36:10 INFO : hyperkit: stderr: /Users/macbook/.minikube/machines/minikube/boot2docker.iso: fcntl(F_PUNCHHOLE) Operation not permitted: block device will not support TRIM/DISCARD
I1122 22:36:10.441901   29924 main.go:134] libmachine: (minikube) DBG | 2022/11/22 22:36:10 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 2 bit: 22 unspecified don't care: bit is 0
I1122 22:36:10.441925   29924 main.go:134] libmachine: (minikube) DBG | 2022/11/22 22:36:10 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 4 bit: 12 unspecified don't care: bit is 0
I1122 22:36:10.441942   29924 main.go:134] libmachine: (minikube) DBG | 2022/11/22 22:36:10 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 4 bit: 20 unspecified don't care: bit is 0
I1122 22:36:10.441976   29924 main.go:134] libmachine: (minikube) DBG | 2022/11/22 22:36:10 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 3 bit: 13 unspecified don't care: bit is 0
I1122 22:36:11.560503   29924 main.go:134] libmachine: (minikube) DBG | 2022/11/22 22:36:11 INFO : hyperkit: stderr: rdmsr to register 0x3a on vcpu 0
I1122 22:36:11.560527   29924 main.go:134] libmachine: (minikube) DBG | 2022/11/22 22:36:11 INFO : hyperkit: stderr: rdmsr to register 0x140 on vcpu 0
I1122 22:36:11.566682   29924 main.go:134] libmachine: (minikube) DBG | 2022/11/22 22:36:11 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 2 bit: 22 unspecified don't care: bit is 0
I1122 22:36:11.566711   29924 main.go:134] libmachine: (minikube) DBG | 2022/11/22 22:36:11 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 4 bit: 12 unspecified don't care: bit is 0
I1122 22:36:11.566725   29924 main.go:134] libmachine: (minikube) DBG | 2022/11/22 22:36:11 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 4 bit: 20 unspecified don't care: bit is 0
I1122 22:36:11.566738   29924 main.go:134] libmachine: (minikube) DBG | 2022/11/22 22:36:11 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 3 bit: 13 unspecified don't care: bit is 0
I1122 22:36:11.567998   29924 main.go:134] libmachine: (minikube) DBG | 2022/11/22 22:36:11 INFO : hyperkit: stderr: rdmsr to register 0x3a on vcpu 1
I1122 22:36:11.568021   29924 main.go:134] libmachine: (minikube) DBG | 2022/11/22 22:36:11 INFO : hyperkit: stderr: rdmsr to register 0x140 on vcpu 1
I1122 22:36:22.403582   29924 main.go:134] libmachine: (minikube) DBG | 2022/11/22 22:36:22 INFO : hyperkit: stderr: rdmsr to register 0x64d on vcpu 1
I1122 22:36:22.403812   29924 main.go:134] libmachine: (minikube) DBG | 2022/11/22 22:36:22 INFO : hyperkit: stderr: rdmsr to register 0x64e on vcpu 1
I1122 22:36:22.403823   29924 main.go:134] libmachine: (minikube) DBG | 2022/11/22 22:36:22 INFO : hyperkit: stderr: rdmsr to register 0x34 on vcpu 1
I1122 22:36:45.453098   29924 main.go:134] libmachine: SSH cmd err, output: <nil>: minikube

I1122 22:36:45.453401   29924 main.go:134] libmachine: (minikube) Calling .GetSSHHostname
I1122 22:36:45.453722   29924 main.go:134] libmachine: (minikube) Calling .GetSSHPort
I1122 22:36:45.453904   29924 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I1122 22:36:45.454122   29924 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I1122 22:36:45.454359   29924 main.go:134] libmachine: (minikube) Calling .GetSSHUsername
I1122 22:36:45.454649   29924 main.go:134] libmachine: Using SSH client type: native
I1122 22:36:45.455109   29924 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003ed820] 0x1003f09a0 <nil>  [] 0s} 192.168.64.2 22 <nil> <nil>}
I1122 22:36:45.455123   29924 main.go:134] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1122 22:36:45.578765   29924 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I1122 22:36:45.579194   29924 buildroot.go:172] set auth options {CertDir:/Users/macbook/.minikube CaCertPath:/Users/macbook/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/macbook/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/macbook/.minikube/machines/server.pem ServerKeyPath:/Users/macbook/.minikube/machines/server-key.pem ClientKeyPath:/Users/macbook/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/macbook/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/macbook/.minikube}
I1122 22:36:45.579281   29924 buildroot.go:174] setting up certificates
I1122 22:36:45.579901   29924 provision.go:83] configureAuth start
I1122 22:36:45.579922   29924 main.go:134] libmachine: (minikube) Calling .GetMachineName
I1122 22:36:45.580248   29924 main.go:134] libmachine: (minikube) Calling .GetIP
I1122 22:36:45.580532   29924 main.go:134] libmachine: (minikube) Calling .GetSSHHostname
I1122 22:36:45.580716   29924 provision.go:138] copyHostCerts
I1122 22:36:45.583315   29924 exec_runner.go:144] found /Users/macbook/.minikube/cert.pem, removing ...
I1122 22:36:45.583776   29924 exec_runner.go:207] rm: /Users/macbook/.minikube/cert.pem
I1122 22:36:45.584501   29924 exec_runner.go:151] cp: /Users/macbook/.minikube/certs/cert.pem --> /Users/macbook/.minikube/cert.pem (1123 bytes)
I1122 22:36:45.585325   29924 exec_runner.go:144] found /Users/macbook/.minikube/key.pem, removing ...
I1122 22:36:45.585344   29924 exec_runner.go:207] rm: /Users/macbook/.minikube/key.pem
I1122 22:36:45.585552   29924 exec_runner.go:151] cp: /Users/macbook/.minikube/certs/key.pem --> /Users/macbook/.minikube/key.pem (1679 bytes)
I1122 22:36:45.586166   29924 exec_runner.go:144] found /Users/macbook/.minikube/ca.pem, removing ...
I1122 22:36:45.586172   29924 exec_runner.go:207] rm: /Users/macbook/.minikube/ca.pem
I1122 22:36:45.586327   29924 exec_runner.go:151] cp: /Users/macbook/.minikube/certs/ca.pem --> /Users/macbook/.minikube/ca.pem (1078 bytes)
I1122 22:36:45.586722   29924 provision.go:112] generating server cert: /Users/macbook/.minikube/machines/server.pem ca-key=/Users/macbook/.minikube/certs/ca.pem private-key=/Users/macbook/.minikube/certs/ca-key.pem org=macbook.minikube san=[192.168.64.2 192.168.64.2 localhost 127.0.0.1 minikube minikube]
I1122 22:36:45.809620   29924 provision.go:172] copyRemoteCerts
I1122 22:36:45.810088   29924 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1122 22:36:45.810112   29924 main.go:134] libmachine: (minikube) Calling .GetSSHHostname
I1122 22:36:45.810380   29924 main.go:134] libmachine: (minikube) Calling .GetSSHPort
I1122 22:36:45.810605   29924 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I1122 22:36:45.810764   29924 main.go:134] libmachine: (minikube) Calling .GetSSHUsername
I1122 22:36:45.810918   29924 sshutil.go:53] new ssh client: &{IP:192.168.64.2 Port:22 SSHKeyPath:/Users/macbook/.minikube/machines/minikube/id_rsa Username:docker}
I1122 22:36:45.901248   29924 ssh_runner.go:362] scp /Users/macbook/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I1122 22:36:45.958247   29924 ssh_runner.go:362] scp /Users/macbook/.minikube/machines/server.pem --> /etc/docker/server.pem (1204 bytes)
I1122 22:36:46.009150   29924 ssh_runner.go:362] scp /Users/macbook/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1122 22:36:46.054432   29924 provision.go:86] duration metric: configureAuth took 473.792661ms
I1122 22:36:46.054450   29924 buildroot.go:189] setting minikube options for container-runtime
I1122 22:36:46.054872   29924 config.go:180] Loaded profile config "minikube": Driver=hyperkit, ContainerRuntime=docker, KubernetesVersion=v1.25.3
I1122 22:36:46.054905   29924 main.go:134] libmachine: (minikube) Calling .DriverName
I1122 22:36:46.055126   29924 main.go:134] libmachine: (minikube) Calling .GetSSHHostname
I1122 22:36:46.055302   29924 main.go:134] libmachine: (minikube) Calling .GetSSHPort
I1122 22:36:46.055442   29924 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I1122 22:36:46.055560   29924 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I1122 22:36:46.055700   29924 main.go:134] libmachine: (minikube) Calling .GetSSHUsername
I1122 22:36:46.055903   29924 main.go:134] libmachine: Using SSH client type: native
I1122 22:36:46.056053   29924 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003ed820] 0x1003f09a0 <nil>  [] 0s} 192.168.64.2 22 <nil> <nil>}
I1122 22:36:46.056061   29924 main.go:134] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1122 22:36:46.163820   29924 main.go:134] libmachine: SSH cmd err, output: <nil>: tmpfs

I1122 22:36:46.163831   29924 buildroot.go:70] root file system type: tmpfs
I1122 22:36:46.164388   29924 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1122 22:36:46.164423   29924 main.go:134] libmachine: (minikube) Calling .GetSSHHostname
I1122 22:36:46.164634   29924 main.go:134] libmachine: (minikube) Calling .GetSSHPort
I1122 22:36:46.164804   29924 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I1122 22:36:46.165011   29924 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I1122 22:36:46.165171   29924 main.go:134] libmachine: (minikube) Calling .GetSSHUsername
I1122 22:36:46.165561   29924 main.go:134] libmachine: Using SSH client type: native
I1122 22:36:46.165809   29924 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003ed820] 0x1003f09a0 <nil>  [] 0s} 192.168.64.2 22 <nil> <nil>}
I1122 22:36:46.165884   29924 main.go:134] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=hyperkit --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1122 22:36:46.300952   29924 main.go:134] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=hyperkit --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1122 22:36:46.301295   29924 main.go:134] libmachine: (minikube) Calling .GetSSHHostname
I1122 22:36:46.301544   29924 main.go:134] libmachine: (minikube) Calling .GetSSHPort
I1122 22:36:46.301778   29924 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I1122 22:36:46.302033   29924 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I1122 22:36:46.302227   29924 main.go:134] libmachine: (minikube) Calling .GetSSHUsername
I1122 22:36:46.302470   29924 main.go:134] libmachine: Using SSH client type: native
I1122 22:36:46.302754   29924 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003ed820] 0x1003f09a0 <nil>  [] 0s} 192.168.64.2 22 <nil> <nil>}
I1122 22:36:46.302786   29924 main.go:134] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1122 22:36:50.386051   29924 main.go:134] libmachine: SSH cmd err, output: <nil>: diff: can't stat '/lib/systemd/system/docker.service': No such file or directory
Created symlink /etc/systemd/system/multi-user.target.wants/docker.service → /usr/lib/systemd/system/docker.service.

I1122 22:36:50.386066   29924 machine.go:91] provisioned docker machine in 40.095084191s
I1122 22:36:50.386643   29924 start.go:300] post-start starting for "minikube" (driver="hyperkit")
I1122 22:36:50.386656   29924 start.go:328] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1122 22:36:50.386675   29924 main.go:134] libmachine: (minikube) Calling .DriverName
I1122 22:36:50.387057   29924 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1122 22:36:50.387076   29924 main.go:134] libmachine: (minikube) Calling .GetSSHHostname
I1122 22:36:50.387266   29924 main.go:134] libmachine: (minikube) Calling .GetSSHPort
I1122 22:36:50.387451   29924 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I1122 22:36:50.387628   29924 main.go:134] libmachine: (minikube) Calling .GetSSHUsername
I1122 22:36:50.387770   29924 sshutil.go:53] new ssh client: &{IP:192.168.64.2 Port:22 SSHKeyPath:/Users/macbook/.minikube/machines/minikube/id_rsa Username:docker}
I1122 22:36:50.472779   29924 ssh_runner.go:195] Run: cat /etc/os-release
I1122 22:36:50.481129   29924 info.go:137] Remote host: Buildroot 2021.02.12
I1122 22:36:50.481398   29924 filesync.go:126] Scanning /Users/macbook/.minikube/addons for local assets ...
I1122 22:36:50.481617   29924 filesync.go:126] Scanning /Users/macbook/.minikube/files for local assets ...
I1122 22:36:50.481798   29924 start.go:303] post-start completed in 95.142965ms
I1122 22:36:50.481811   29924 fix.go:57] fixHost completed within 40.888682201s
I1122 22:36:50.482110   29924 main.go:134] libmachine: (minikube) Calling .GetSSHHostname
I1122 22:36:50.482433   29924 main.go:134] libmachine: (minikube) Calling .GetSSHPort
I1122 22:36:50.482733   29924 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I1122 22:36:50.482953   29924 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I1122 22:36:50.483197   29924 main.go:134] libmachine: (minikube) Calling .GetSSHUsername
I1122 22:36:50.483465   29924 main.go:134] libmachine: Using SSH client type: native
I1122 22:36:50.483668   29924 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003ed820] 0x1003f09a0 <nil>  [] 0s} 192.168.64.2 22 <nil> <nil>}
I1122 22:36:50.483675   29924 main.go:134] libmachine: About to run SSH command:
date +%!s(MISSING).%!N(MISSING)
I1122 22:36:50.598339   29924 main.go:134] libmachine: SSH cmd err, output: <nil>: 1669174610.908130726

I1122 22:36:50.598352   29924 fix.go:207] guest clock: 1669174610.908130726
I1122 22:36:50.598366   29924 fix.go:220] Guest: 2022-11-22 22:36:50.908130726 -0500 EST Remote: 2022-11-22 22:36:50.481813 -0500 EST m=+41.322425261 (delta=426.317726ms)
I1122 22:36:50.599223   29924 fix.go:191] guest clock delta is within tolerance: 426.317726ms
I1122 22:36:50.599232   29924 start.go:83] releasing machines lock for "minikube", held for 41.00613468s
I1122 22:36:50.599257   29924 main.go:134] libmachine: (minikube) Calling .DriverName
I1122 22:36:50.599498   29924 main.go:134] libmachine: (minikube) Calling .GetIP
I1122 22:36:50.599950   29924 main.go:134] libmachine: (minikube) Calling .DriverName
I1122 22:36:50.600796   29924 main.go:134] libmachine: (minikube) Calling .DriverName
I1122 22:36:50.601037   29924 main.go:134] libmachine: (minikube) Calling .DriverName
I1122 22:36:50.601295   29924 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1122 22:36:50.601872   29924 main.go:134] libmachine: (minikube) Calling .GetSSHHostname
I1122 22:36:50.602047   29924 ssh_runner.go:195] Run: systemctl --version
I1122 22:36:50.602057   29924 main.go:134] libmachine: (minikube) Calling .GetSSHPort
I1122 22:36:50.602062   29924 main.go:134] libmachine: (minikube) Calling .GetSSHHostname
I1122 22:36:50.602289   29924 main.go:134] libmachine: (minikube) Calling .GetSSHPort
I1122 22:36:50.602301   29924 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I1122 22:36:50.602516   29924 main.go:134] libmachine: (minikube) Calling .GetSSHUsername
I1122 22:36:50.602527   29924 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I1122 22:36:50.602774   29924 sshutil.go:53] new ssh client: &{IP:192.168.64.2 Port:22 SSHKeyPath:/Users/macbook/.minikube/machines/minikube/id_rsa Username:docker}
I1122 22:36:50.602808   29924 main.go:134] libmachine: (minikube) Calling .GetSSHUsername
I1122 22:36:50.603018   29924 sshutil.go:53] new ssh client: &{IP:192.168.64.2 Port:22 SSHKeyPath:/Users/macbook/.minikube/machines/minikube/id_rsa Username:docker}
I1122 22:36:50.678217   29924 preload.go:132] Checking if preload exists for k8s version v1.25.3 and runtime docker
I1122 22:36:50.679692   29924 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1122 22:36:51.941617   29924 ssh_runner.go:235] Completed: docker images --format {{.Repository}}:{{.Tag}}: (1.261862279s)
I1122 22:36:51.941771   29924 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.340467074s)
I1122 22:36:51.942951   29924 docker.go:613] Got preloaded images: -- stdout --
nginx:alpine
mongo:latest
registry.k8s.io/kube-apiserver:v1.25.3
registry.k8s.io/kube-controller-manager:v1.25.3
registry.k8s.io/kube-scheduler:v1.25.3
registry.k8s.io/kube-proxy:v1.25.3
kubernetesui/dashboard:<none>
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
kubernetesui/metrics-scraper:<none>
registry.k8s.io/coredns/coredns:v1.9.3
k8s.gcr.io/metrics-server/metrics-server:<none>
mongo-express:latest
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5
nginx:1.16

-- /stdout --
I1122 22:36:51.944642   29924 docker.go:543] Images already preloaded, skipping extraction
I1122 22:36:51.945599   29924 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1122 22:36:51.986658   29924 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1122 22:36:52.041732   29924 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1122 22:36:52.094166   29924 ssh_runner.go:195] Run: sudo systemctl stop -f crio
I1122 22:36:52.171275   29924 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1122 22:36:52.205161   29924 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
image-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1122 22:36:52.254309   29924 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1122 22:36:52.825068   29924 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1122 22:36:53.194423   29924 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1122 22:36:53.541574   29924 ssh_runner.go:195] Run: sudo systemctl restart docker
I1122 22:36:55.699887   29924 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.158293748s)
I1122 22:36:55.699998   29924 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1122 22:36:56.045573   29924 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1122 22:36:56.349298   29924 ssh_runner.go:195] Run: sudo systemctl start cri-docker.socket
I1122 22:36:56.391582   29924 start.go:451] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1122 22:36:56.391818   29924 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1122 22:36:56.406939   29924 start.go:472] Will wait 60s for crictl version
I1122 22:36:56.407151   29924 ssh_runner.go:195] Run: sudo crictl version
I1122 22:36:56.684111   29924 start.go:481] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  20.10.20
RuntimeApiVersion:  1.41.0
I1122 22:36:56.684257   29924 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1122 22:36:56.769325   29924 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1122 22:36:56.862665   29924 out.go:204] 🐳  Preparing Kubernetes v1.25.3 on Docker 20.10.20 ...
I1122 22:36:56.868950   29924 ssh_runner.go:195] Run: grep 192.168.64.1	host.minikube.internal$ /etc/hosts
I1122 22:36:56.882597   29924 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.64.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1122 22:36:56.925116   29924 preload.go:132] Checking if preload exists for k8s version v1.25.3 and runtime docker
I1122 22:36:56.925336   29924 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1122 22:36:56.979916   29924 docker.go:613] Got preloaded images: -- stdout --
nginx:alpine
mongo:latest
registry.k8s.io/kube-apiserver:v1.25.3
registry.k8s.io/kube-controller-manager:v1.25.3
registry.k8s.io/kube-scheduler:v1.25.3
registry.k8s.io/kube-proxy:v1.25.3
kubernetesui/dashboard:<none>
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
kubernetesui/metrics-scraper:<none>
registry.k8s.io/coredns/coredns:v1.9.3
k8s.gcr.io/metrics-server/metrics-server:<none>
mongo-express:latest
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5
nginx:1.16

-- /stdout --
I1122 22:36:56.979976   29924 docker.go:543] Images already preloaded, skipping extraction
I1122 22:36:56.980369   29924 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1122 22:36:57.054084   29924 docker.go:613] Got preloaded images: -- stdout --
nginx:alpine
mongo:latest
registry.k8s.io/kube-apiserver:v1.25.3
registry.k8s.io/kube-controller-manager:v1.25.3
registry.k8s.io/kube-scheduler:v1.25.3
registry.k8s.io/kube-proxy:v1.25.3
kubernetesui/dashboard:<none>
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
kubernetesui/metrics-scraper:<none>
registry.k8s.io/coredns/coredns:v1.9.3
k8s.gcr.io/metrics-server/metrics-server:<none>
mongo-express:latest
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5
nginx:1.16

-- /stdout --
I1122 22:36:57.054812   29924 cache_images.go:84] Images are preloaded, skipping loading
I1122 22:36:57.055319   29924 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1122 22:36:57.129790   29924 cni.go:95] Creating CNI manager for ""
I1122 22:36:57.129812   29924 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I1122 22:36:57.130906   29924 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1122 22:36:57.130947   29924 kubeadm.go:156] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.64.2 APIServerPort:8443 KubernetesVersion:v1.25.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.64.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.64.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false}
I1122 22:36:57.132178   29924 kubeadm.go:161] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.64.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.64.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.64.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.25.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1122 22:36:57.132496   29924 kubeadm.go:962] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.25.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/var/run/cri-dockerd.sock --hostname-override=minikube --image-service-endpoint=/var/run/cri-dockerd.sock --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.64.2 --runtime-request-timeout=15m

[Install]
 config:
{KubernetesVersion:v1.25.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1122 22:36:57.132605   29924 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.25.3
I1122 22:36:57.158307   29924 binaries.go:44] Found k8s binaries, skipping transfer
I1122 22:36:57.158497   29924 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1122 22:36:57.183750   29924 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (470 bytes)
I1122 22:36:57.229581   29924 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1122 22:36:57.271297   29924 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2030 bytes)
I1122 22:36:57.318907   29924 ssh_runner.go:195] Run: grep 192.168.64.2	control-plane.minikube.internal$ /etc/hosts
I1122 22:36:57.327014   29924 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.64.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1122 22:36:57.355526   29924 certs.go:54] Setting up /Users/macbook/.minikube/profiles/minikube for IP: 192.168.64.2
I1122 22:36:57.356145   29924 certs.go:182] skipping minikubeCA CA generation: /Users/macbook/.minikube/ca.key
I1122 22:36:57.356508   29924 certs.go:182] skipping proxyClientCA CA generation: /Users/macbook/.minikube/proxy-client-ca.key
I1122 22:36:57.356971   29924 certs.go:298] skipping minikube-user signed cert generation: /Users/macbook/.minikube/profiles/minikube/client.key
I1122 22:36:57.357476   29924 certs.go:298] skipping minikube signed cert generation: /Users/macbook/.minikube/profiles/minikube/apiserver.key.a30f3483
I1122 22:36:57.357824   29924 certs.go:298] skipping aggregator signed cert generation: /Users/macbook/.minikube/profiles/minikube/proxy-client.key
I1122 22:36:57.358277   29924 certs.go:388] found cert: /Users/macbook/.minikube/certs/Users/macbook/.minikube/certs/ca-key.pem (1675 bytes)
I1122 22:36:57.358466   29924 certs.go:388] found cert: /Users/macbook/.minikube/certs/Users/macbook/.minikube/certs/ca.pem (1078 bytes)
I1122 22:36:57.358682   29924 certs.go:388] found cert: /Users/macbook/.minikube/certs/Users/macbook/.minikube/certs/cert.pem (1123 bytes)
I1122 22:36:57.358919   29924 certs.go:388] found cert: /Users/macbook/.minikube/certs/Users/macbook/.minikube/certs/key.pem (1679 bytes)
I1122 22:36:57.370934   29924 ssh_runner.go:362] scp /Users/macbook/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1122 22:36:57.438996   29924 ssh_runner.go:362] scp /Users/macbook/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1122 22:36:57.502187   29924 ssh_runner.go:362] scp /Users/macbook/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1122 22:36:57.570050   29924 ssh_runner.go:362] scp /Users/macbook/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1122 22:36:57.625084   29924 ssh_runner.go:362] scp /Users/macbook/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1122 22:36:57.682462   29924 ssh_runner.go:362] scp /Users/macbook/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1122 22:36:57.735631   29924 ssh_runner.go:362] scp /Users/macbook/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1122 22:36:57.808865   29924 ssh_runner.go:362] scp /Users/macbook/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1122 22:36:57.862347   29924 ssh_runner.go:362] scp /Users/macbook/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1122 22:36:57.923275   29924 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1122 22:36:57.964198   29924 ssh_runner.go:195] Run: openssl version
I1122 22:36:57.975993   29924 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1122 22:36:58.008698   29924 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1122 22:36:58.021224   29924 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Nov 12 20:53 /usr/share/ca-certificates/minikubeCA.pem
I1122 22:36:58.021337   29924 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1122 22:36:58.034109   29924 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1122 22:36:58.059212   29924 kubeadm.go:396] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.28.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:hyperkit HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.64.2 Port:8443 KubernetesVersion:v1.25.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/var/run/socket_vmnet}
I1122 22:36:58.059429   29924 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1122 22:36:58.129346   29924 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1122 22:36:58.165647   29924 kubeadm.go:411] found existing configuration files, will attempt cluster restart
I1122 22:36:58.165671   29924 kubeadm.go:627] restartCluster start
I1122 22:36:58.166303   29924 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1122 22:36:58.192672   29924 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1122 22:36:58.195399   29924 kubeconfig.go:92] found "minikube" server: "https://192.168.64.2:8443"
I1122 22:36:58.218195   29924 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1122 22:36:58.242468   29924 api_server.go:165] Checking apiserver status ...
I1122 22:36:58.242593   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1122 22:36:58.288484   29924 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1122 22:36:58.489480   29924 api_server.go:165] Checking apiserver status ...
I1122 22:36:58.489636   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1122 22:36:58.516158   29924 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1122 22:36:58.689622   29924 api_server.go:165] Checking apiserver status ...
I1122 22:36:58.689792   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1122 22:36:58.730248   29924 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1122 22:36:58.889187   29924 api_server.go:165] Checking apiserver status ...
I1122 22:36:58.889294   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1122 22:36:58.918474   29924 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1122 22:36:59.089507   29924 api_server.go:165] Checking apiserver status ...
I1122 22:36:59.089630   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1122 22:36:59.118049   29924 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1122 22:36:59.289883   29924 api_server.go:165] Checking apiserver status ...
I1122 22:36:59.313815   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1122 22:36:59.367743   29924 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1122 22:36:59.497364   29924 api_server.go:165] Checking apiserver status ...
I1122 22:36:59.497486   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1122 22:36:59.557778   29924 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1122 22:36:59.689881   29924 api_server.go:165] Checking apiserver status ...
I1122 22:36:59.690091   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1122 22:36:59.727009   29924 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1122 22:36:59.889145   29924 api_server.go:165] Checking apiserver status ...
I1122 22:36:59.889261   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1122 22:36:59.921523   29924 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1122 22:37:00.100974   29924 api_server.go:165] Checking apiserver status ...
I1122 22:37:00.101130   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1122 22:37:00.145341   29924 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1122 22:37:00.288789   29924 api_server.go:165] Checking apiserver status ...
I1122 22:37:00.288990   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1122 22:37:00.320720   29924 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1122 22:37:00.489403   29924 api_server.go:165] Checking apiserver status ...
I1122 22:37:00.489539   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1122 22:37:00.527960   29924 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1122 22:37:00.690046   29924 api_server.go:165] Checking apiserver status ...
I1122 22:37:00.690363   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1122 22:37:00.721203   29924 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1122 22:37:00.889463   29924 api_server.go:165] Checking apiserver status ...
I1122 22:37:00.889693   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1122 22:37:00.938692   29924 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1122 22:37:01.089525   29924 api_server.go:165] Checking apiserver status ...
I1122 22:37:01.089959   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1122 22:37:01.127797   29924 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1122 22:37:01.289465   29924 api_server.go:165] Checking apiserver status ...
I1122 22:37:01.289668   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1122 22:37:01.321955   29924 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1122 22:37:01.321966   29924 api_server.go:165] Checking apiserver status ...
I1122 22:37:01.322102   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1122 22:37:01.360527   29924 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1122 22:37:01.360544   29924 kubeadm.go:602] needs reconfigure: apiserver error: timed out waiting for the condition
I1122 22:37:01.360559   29924 kubeadm.go:1114] stopping kube-system containers ...
I1122 22:37:01.360654   29924 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1122 22:37:01.434694   29924 docker.go:444] Stopping containers: [32a4b73e7a7e 3071baf60ebe 98f8ffccac4b edd84e0488fc ea37dd91861a ce516e00992b e82e8b5c79e0 0bb512b86d29 cf2290771ee0 7a95cf96a9e5 2bacd201bbb7 aa4341287054 eddc3882ef12 57bb99562170 26c72c709165 ff24f7a0c595 50d817f6d02a 9aa8a4556cf3 903c15ebe9e0]
I1122 22:37:01.434881   29924 ssh_runner.go:195] Run: docker stop 32a4b73e7a7e 3071baf60ebe 98f8ffccac4b edd84e0488fc ea37dd91861a ce516e00992b e82e8b5c79e0 0bb512b86d29 cf2290771ee0 7a95cf96a9e5 2bacd201bbb7 aa4341287054 eddc3882ef12 57bb99562170 26c72c709165 ff24f7a0c595 50d817f6d02a 9aa8a4556cf3 903c15ebe9e0
I1122 22:37:01.499662   29924 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I1122 22:37:01.532883   29924 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1122 22:37:01.552654   29924 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1122 22:37:01.552763   29924 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1122 22:37:01.575827   29924 kubeadm.go:704] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I1122 22:37:01.575840   29924 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I1122 22:37:01.840939   29924 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I1122 22:37:03.530057   29924 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (1.689087912s)
I1122 22:37:03.530094   29924 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I1122 22:37:03.987153   29924 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I1122 22:37:04.168012   29924 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I1122 22:37:04.324695   29924 api_server.go:51] waiting for apiserver process to appear ...
I1122 22:37:04.324813   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1122 22:37:04.862895   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1122 22:37:05.362944   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1122 22:37:05.862965   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1122 22:37:06.362849   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1122 22:37:06.863151   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1122 22:37:07.362863   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1122 22:37:07.862911   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1122 22:37:08.363954   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1122 22:37:08.862822   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1122 22:37:09.363836   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1122 22:37:09.862783   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1122 22:37:10.362979   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1122 22:37:10.405990   29924 api_server.go:71] duration metric: took 6.081324747s to wait for apiserver process to appear ...
I1122 22:37:10.406016   29924 api_server.go:87] waiting for apiserver healthz status ...
I1122 22:37:10.406299   29924 api_server.go:252] Checking apiserver healthz at https://192.168.64.2:8443/healthz ...
I1122 22:37:15.407810   29924 api_server.go:268] stopped: https://192.168.64.2:8443/healthz: Get "https://192.168.64.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1122 22:37:15.907924   29924 api_server.go:252] Checking apiserver healthz at https://192.168.64.2:8443/healthz ...
I1122 22:37:20.908623   29924 api_server.go:268] stopped: https://192.168.64.2:8443/healthz: Get "https://192.168.64.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1122 22:37:21.408027   29924 api_server.go:252] Checking apiserver healthz at https://192.168.64.2:8443/healthz ...
I1122 22:37:23.094640   29924 api_server.go:278] https://192.168.64.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1122 22:37:23.094660   29924 api_server.go:102] status: https://192.168.64.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1122 22:37:23.408059   29924 api_server.go:252] Checking apiserver healthz at https://192.168.64.2:8443/healthz ...
I1122 22:37:23.495722   29924 api_server.go:278] https://192.168.64.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1122 22:37:23.495739   29924 api_server.go:102] status: https://192.168.64.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1122 22:37:23.907890   29924 api_server.go:252] Checking apiserver healthz at https://192.168.64.2:8443/healthz ...
I1122 22:37:23.958122   29924 api_server.go:278] https://192.168.64.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1122 22:37:23.958155   29924 api_server.go:102] status: https://192.168.64.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1122 22:37:24.407950   29924 api_server.go:252] Checking apiserver healthz at https://192.168.64.2:8443/healthz ...
I1122 22:37:24.427603   29924 api_server.go:278] https://192.168.64.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1122 22:37:24.427634   29924 api_server.go:102] status: https://192.168.64.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1122 22:37:24.907929   29924 api_server.go:252] Checking apiserver healthz at https://192.168.64.2:8443/healthz ...
I1122 22:37:24.922800   29924 api_server.go:278] https://192.168.64.2:8443/healthz returned 200:
ok
I1122 22:37:24.949365   29924 api_server.go:140] control plane version: v1.25.3
I1122 22:37:24.949377   29924 api_server.go:130] duration metric: took 14.543435873s to wait for apiserver health ...
I1122 22:37:24.949403   29924 cni.go:95] Creating CNI manager for ""
I1122 22:37:24.949411   29924 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I1122 22:37:24.949687   29924 system_pods.go:43] waiting for kube-system pods to appear ...
I1122 22:37:24.978547   29924 system_pods.go:59] 8 kube-system pods found
I1122 22:37:24.978570   29924 system_pods.go:61] "coredns-565d847f94-dnz9v" [43062ecb-3ad1-4d34-bc7a-274385cecdd0] Running
I1122 22:37:24.978577   29924 system_pods.go:61] "etcd-minikube" [06e9edda-1700-4025-801f-3ce005d84918] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1122 22:37:24.978584   29924 system_pods.go:61] "kube-apiserver-minikube" [bc58b482-9725-42c9-8396-dad58b99ab35] Running
I1122 22:37:24.978588   29924 system_pods.go:61] "kube-controller-manager-minikube" [8474f280-c06f-4164-8e45-cb7f860b9579] Running
I1122 22:37:24.978592   29924 system_pods.go:61] "kube-proxy-tvntg" [94f03b72-6e6e-4824-b18b-623db750462f] Running
I1122 22:37:24.978596   29924 system_pods.go:61] "kube-scheduler-minikube" [df12115d-4178-4830-887d-3fa5043e65b6] Running
I1122 22:37:24.978602   29924 system_pods.go:61] "metrics-server-769cd898cd-tj8zr" [59032a3d-73ba-4656-b8b6-26c6b5421a71] Running / Ready:ContainersNotReady (containers with unready status: [metrics-server]) / ContainersReady:ContainersNotReady (containers with unready status: [metrics-server])
I1122 22:37:24.978607   29924 system_pods.go:61] "storage-provisioner" [3e8ecef5-102c-4480-8778-3c00d413ea53] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1122 22:37:24.978612   29924 system_pods.go:74] duration metric: took 28.918058ms to wait for pod list to return data ...
I1122 22:37:24.978621   29924 node_conditions.go:102] verifying NodePressure condition ...
I1122 22:37:24.992362   29924 node_conditions.go:122] node storage ephemeral capacity is 17784752Ki
I1122 22:37:24.993094   29924 node_conditions.go:123] node cpu capacity is 2
I1122 22:37:24.993211   29924 node_conditions.go:105] duration metric: took 14.583969ms to run NodePressure ...
I1122 22:37:24.993454   29924 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I1122 22:37:26.454592   29924 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml": (1.461123404s)
I1122 22:37:26.454622   29924 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1122 22:37:26.562747   29924 ops.go:34] apiserver oom_adj: -16
I1122 22:37:26.562760   29924 kubeadm.go:631] restartCluster took 28.39723809s
I1122 22:37:26.562767   29924 kubeadm.go:398] StartCluster complete in 28.503724812s
I1122 22:37:26.563084   29924 settings.go:142] acquiring lock: {Name:mk389ad7a51095b6890cf7f0435146582bf7af89 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1122 22:37:26.563395   29924 settings.go:150] Updating kubeconfig:  /Users/macbook/.kube/config
I1122 22:37:26.564551   29924 lock.go:35] WriteFile acquiring /Users/macbook/.kube/config: {Name:mkc1a148b938e1c425ff35fc4001885529b53a85 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1122 22:37:26.577444   29924 kapi.go:244] deployment "coredns" in namespace "kube-system" and context "minikube" rescaled to 1
I1122 22:37:26.577960   29924 start.go:212] Will wait 6m0s for node &{Name: IP:192.168.64.2 Port:8443 KubernetesVersion:v1.25.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I1122 22:37:26.623287   29924 out.go:177] 🔎  Verifying Kubernetes components...
I1122 22:37:26.579461   29924 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.25.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1122 22:37:26.579603   29924 config.go:180] Loaded profile config "minikube": Driver=hyperkit, ContainerRuntime=docker, KubernetesVersion=v1.25.3
I1122 22:37:26.580724   29924 addons.go:486] enableAddons start: toEnable=map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true], additional=[]
I1122 22:37:26.688281   29924 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1122 22:37:26.688378   29924 addons.go:65] Setting default-storageclass=true in profile "minikube"
I1122 22:37:26.688380   29924 addons.go:65] Setting dashboard=true in profile "minikube"
I1122 22:37:26.688603   29924 addons.go:227] Setting addon dashboard=true in "minikube"
I1122 22:37:26.688612   29924 addons.go:65] Setting metrics-server=true in profile "minikube"
W1122 22:37:26.688618   29924 addons.go:236] addon dashboard should already be in state true
I1122 22:37:26.688627   29924 addons.go:227] Setting addon metrics-server=true in "minikube"
I1122 22:37:26.688627   29924 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1122 22:37:26.688628   29924 addons.go:65] Setting storage-provisioner=true in profile "minikube"
I1122 22:37:26.688656   29924 addons.go:227] Setting addon storage-provisioner=true in "minikube"
W1122 22:37:26.688651   29924 addons.go:236] addon metrics-server should already be in state true
W1122 22:37:26.688663   29924 addons.go:236] addon storage-provisioner should already be in state true
I1122 22:37:26.690069   29924 main.go:134] libmachine: Found binary path at /Users/macbook/.minikube/bin/docker-machine-driver-hyperkit
I1122 22:37:26.690101   29924 main.go:134] libmachine: Launching plugin server for driver hyperkit
I1122 22:37:26.690224   29924 host.go:66] Checking if "minikube" exists ...
I1122 22:37:26.690227   29924 host.go:66] Checking if "minikube" exists ...
I1122 22:37:26.690224   29924 host.go:66] Checking if "minikube" exists ...
I1122 22:37:26.690864   29924 main.go:134] libmachine: Found binary path at /Users/macbook/.minikube/bin/docker-machine-driver-hyperkit
I1122 22:37:26.690915   29924 main.go:134] libmachine: Launching plugin server for driver hyperkit
I1122 22:37:26.694758   29924 main.go:134] libmachine: Found binary path at /Users/macbook/.minikube/bin/docker-machine-driver-hyperkit
I1122 22:37:26.694813   29924 main.go:134] libmachine: Found binary path at /Users/macbook/.minikube/bin/docker-machine-driver-hyperkit
I1122 22:37:26.694924   29924 main.go:134] libmachine: Launching plugin server for driver hyperkit
I1122 22:37:26.694941   29924 main.go:134] libmachine: Launching plugin server for driver hyperkit
I1122 22:37:26.728754   29924 main.go:134] libmachine: Plugin server listening at address 127.0.0.1:60584
I1122 22:37:26.728778   29924 main.go:134] libmachine: Plugin server listening at address 127.0.0.1:60585
I1122 22:37:26.729224   29924 main.go:134] libmachine: Plugin server listening at address 127.0.0.1:60588
I1122 22:37:26.730895   29924 main.go:134] libmachine: () Calling .GetVersion
I1122 22:37:26.731145   29924 main.go:134] libmachine: Plugin server listening at address 127.0.0.1:60590
I1122 22:37:26.731144   29924 main.go:134] libmachine: () Calling .GetVersion
I1122 22:37:26.731191   29924 main.go:134] libmachine: () Calling .GetVersion
I1122 22:37:26.731820   29924 main.go:134] libmachine: Using API Version  1
I1122 22:37:26.731836   29924 main.go:134] libmachine: () Calling .SetConfigRaw
I1122 22:37:26.732158   29924 main.go:134] libmachine: Using API Version  1
I1122 22:37:26.732173   29924 main.go:134] libmachine: () Calling .SetConfigRaw
I1122 22:37:26.732206   29924 main.go:134] libmachine: () Calling .GetVersion
I1122 22:37:26.732436   29924 main.go:134] libmachine: Using API Version  1
I1122 22:37:26.732465   29924 main.go:134] libmachine: () Calling .SetConfigRaw
I1122 22:37:26.732588   29924 main.go:134] libmachine: () Calling .GetMachineName
I1122 22:37:26.732795   29924 main.go:134] libmachine: () Calling .GetMachineName
I1122 22:37:26.732852   29924 main.go:134] libmachine: (minikube) Calling .GetState
I1122 22:37:26.733003   29924 main.go:134] libmachine: () Calling .GetMachineName
I1122 22:37:26.733021   29924 main.go:134] libmachine: Using API Version  1
I1122 22:37:26.733043   29924 main.go:134] libmachine: () Calling .SetConfigRaw
I1122 22:37:26.733051   29924 main.go:134] libmachine: (minikube) DBG | exe=/Users/macbook/.minikube/bin/docker-machine-driver-hyperkit uid=0
I1122 22:37:26.733283   29924 main.go:134] libmachine: (minikube) DBG | hyperkit pid from json: 29941
I1122 22:37:26.733482   29924 main.go:134] libmachine: () Calling .GetMachineName
I1122 22:37:26.733741   29924 main.go:134] libmachine: Found binary path at /Users/macbook/.minikube/bin/docker-machine-driver-hyperkit
I1122 22:37:26.733789   29924 main.go:134] libmachine: Launching plugin server for driver hyperkit
I1122 22:37:26.733887   29924 main.go:134] libmachine: Found binary path at /Users/macbook/.minikube/bin/docker-machine-driver-hyperkit
I1122 22:37:26.733931   29924 main.go:134] libmachine: Launching plugin server for driver hyperkit
I1122 22:37:26.736594   29924 main.go:134] libmachine: Found binary path at /Users/macbook/.minikube/bin/docker-machine-driver-hyperkit
I1122 22:37:26.736795   29924 main.go:134] libmachine: Launching plugin server for driver hyperkit
I1122 22:37:26.761960   29924 main.go:134] libmachine: Plugin server listening at address 127.0.0.1:60593
I1122 22:37:26.762085   29924 main.go:134] libmachine: Plugin server listening at address 127.0.0.1:60594
I1122 22:37:26.762307   29924 main.go:134] libmachine: Plugin server listening at address 127.0.0.1:60592
I1122 22:37:26.762983   29924 main.go:134] libmachine: () Calling .GetVersion
I1122 22:37:26.763122   29924 main.go:134] libmachine: () Calling .GetVersion
I1122 22:37:26.763190   29924 main.go:134] libmachine: () Calling .GetVersion
I1122 22:37:26.763872   29924 main.go:134] libmachine: Using API Version  1
I1122 22:37:26.763891   29924 main.go:134] libmachine: () Calling .SetConfigRaw
I1122 22:37:26.764027   29924 main.go:134] libmachine: Using API Version  1
I1122 22:37:26.764050   29924 main.go:134] libmachine: () Calling .SetConfigRaw
I1122 22:37:26.764150   29924 main.go:134] libmachine: Using API Version  1
I1122 22:37:26.764166   29924 main.go:134] libmachine: () Calling .SetConfigRaw
I1122 22:37:26.764435   29924 main.go:134] libmachine: () Calling .GetMachineName
I1122 22:37:26.764546   29924 main.go:134] libmachine: () Calling .GetMachineName
I1122 22:37:26.764645   29924 main.go:134] libmachine: () Calling .GetMachineName
I1122 22:37:26.764699   29924 main.go:134] libmachine: (minikube) Calling .GetState
I1122 22:37:26.764769   29924 main.go:134] libmachine: (minikube) Calling .GetState
I1122 22:37:26.764855   29924 main.go:134] libmachine: (minikube) Calling .GetState
I1122 22:37:26.764883   29924 main.go:134] libmachine: (minikube) DBG | exe=/Users/macbook/.minikube/bin/docker-machine-driver-hyperkit uid=0
I1122 22:37:26.765109   29924 main.go:134] libmachine: (minikube) DBG | hyperkit pid from json: 29941
I1122 22:37:26.765132   29924 main.go:134] libmachine: (minikube) DBG | exe=/Users/macbook/.minikube/bin/docker-machine-driver-hyperkit uid=0
I1122 22:37:26.765168   29924 main.go:134] libmachine: (minikube) DBG | exe=/Users/macbook/.minikube/bin/docker-machine-driver-hyperkit uid=0
I1122 22:37:26.765206   29924 main.go:134] libmachine: (minikube) DBG | hyperkit pid from json: 29941
I1122 22:37:26.765230   29924 main.go:134] libmachine: (minikube) DBG | hyperkit pid from json: 29941
I1122 22:37:26.768720   29924 main.go:134] libmachine: (minikube) Calling .DriverName
I1122 22:37:26.769116   29924 main.go:134] libmachine: (minikube) Calling .DriverName
I1122 22:37:26.801996   29924 out.go:177]     ▪ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I1122 22:37:26.822082   29924 out.go:177]     ▪ Using image k8s.gcr.io/metrics-server/metrics-server:v0.6.1
I1122 22:37:26.770671   29924 main.go:134] libmachine: (minikube) Calling .DriverName
I1122 22:37:26.829900   29924 addons.go:227] Setting addon default-storageclass=true in "minikube"
I1122 22:37:26.860611   29924 addons.go:419] installing /etc/kubernetes/addons/metrics-apiservice.yaml
I1122 22:37:26.898386   29924 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-apiservice.yaml (424 bytes)
I1122 22:37:26.898358   29924 out.go:177]     ▪ Using image docker.io/kubernetesui/dashboard:v2.7.0
I1122 22:37:26.898417   29924 main.go:134] libmachine: (minikube) Calling .GetSSHHostname
I1122 22:37:26.881567   29924 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
W1122 22:37:26.881574   29924 addons.go:236] addon default-storageclass should already be in state true
I1122 22:37:26.898848   29924 main.go:134] libmachine: (minikube) Calling .GetSSHPort
I1122 22:37:26.943616   29924 addons.go:419] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1122 22:37:26.943629   29924 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1122 22:37:26.911364   29924 addons.go:419] installing /etc/kubernetes/addons/dashboard-ns.yaml
I1122 22:37:26.943650   29924 main.go:134] libmachine: (minikube) Calling .GetSSHHostname
I1122 22:37:26.943678   29924 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I1122 22:37:26.911401   29924 host.go:66] Checking if "minikube" exists ...
I1122 22:37:26.943704   29924 main.go:134] libmachine: (minikube) Calling .GetSSHHostname
I1122 22:37:26.911810   29924 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I1122 22:37:26.943991   29924 main.go:134] libmachine: (minikube) Calling .GetSSHPort
I1122 22:37:26.944052   29924 main.go:134] libmachine: (minikube) Calling .GetSSHPort
I1122 22:37:26.944056   29924 main.go:134] libmachine: (minikube) Calling .GetSSHUsername
I1122 22:37:26.944328   29924 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I1122 22:37:26.944400   29924 sshutil.go:53] new ssh client: &{IP:192.168.64.2 Port:22 SSHKeyPath:/Users/macbook/.minikube/machines/minikube/id_rsa Username:docker}
I1122 22:37:26.944400   29924 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I1122 22:37:26.944467   29924 main.go:134] libmachine: Found binary path at /Users/macbook/.minikube/bin/docker-machine-driver-hyperkit
I1122 22:37:26.944529   29924 main.go:134] libmachine: Launching plugin server for driver hyperkit
I1122 22:37:26.944597   29924 main.go:134] libmachine: (minikube) Calling .GetSSHUsername
I1122 22:37:26.944681   29924 main.go:134] libmachine: (minikube) Calling .GetSSHUsername
I1122 22:37:26.944852   29924 sshutil.go:53] new ssh client: &{IP:192.168.64.2 Port:22 SSHKeyPath:/Users/macbook/.minikube/machines/minikube/id_rsa Username:docker}
I1122 22:37:26.946184   29924 sshutil.go:53] new ssh client: &{IP:192.168.64.2 Port:22 SSHKeyPath:/Users/macbook/.minikube/machines/minikube/id_rsa Username:docker}
I1122 22:37:26.962705   29924 main.go:134] libmachine: Plugin server listening at address 127.0.0.1:60601
I1122 22:37:26.963605   29924 main.go:134] libmachine: () Calling .GetVersion
I1122 22:37:26.964786   29924 main.go:134] libmachine: Using API Version  1
I1122 22:37:26.964810   29924 main.go:134] libmachine: () Calling .SetConfigRaw
I1122 22:37:26.965709   29924 main.go:134] libmachine: () Calling .GetMachineName
I1122 22:37:26.966735   29924 main.go:134] libmachine: Found binary path at /Users/macbook/.minikube/bin/docker-machine-driver-hyperkit
I1122 22:37:26.966788   29924 main.go:134] libmachine: Launching plugin server for driver hyperkit
I1122 22:37:26.984614   29924 main.go:134] libmachine: Plugin server listening at address 127.0.0.1:60603
I1122 22:37:26.985977   29924 main.go:134] libmachine: () Calling .GetVersion
I1122 22:37:26.987307   29924 main.go:134] libmachine: Using API Version  1
I1122 22:37:26.987335   29924 main.go:134] libmachine: () Calling .SetConfigRaw
I1122 22:37:26.988042   29924 main.go:134] libmachine: () Calling .GetMachineName
I1122 22:37:26.988350   29924 main.go:134] libmachine: (minikube) Calling .GetState
I1122 22:37:26.988582   29924 main.go:134] libmachine: (minikube) DBG | exe=/Users/macbook/.minikube/bin/docker-machine-driver-hyperkit uid=0
I1122 22:37:26.988817   29924 main.go:134] libmachine: (minikube) DBG | hyperkit pid from json: 29941
I1122 22:37:26.992666   29924 main.go:134] libmachine: (minikube) Calling .DriverName
I1122 22:37:26.993124   29924 addons.go:419] installing /etc/kubernetes/addons/storageclass.yaml
I1122 22:37:26.993135   29924 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1122 22:37:26.993160   29924 main.go:134] libmachine: (minikube) Calling .GetSSHHostname
I1122 22:37:26.993509   29924 main.go:134] libmachine: (minikube) Calling .GetSSHPort
I1122 22:37:26.993865   29924 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I1122 22:37:26.994145   29924 main.go:134] libmachine: (minikube) Calling .GetSSHUsername
I1122 22:37:26.994456   29924 sshutil.go:53] new ssh client: &{IP:192.168.64.2 Port:22 SSHKeyPath:/Users/macbook/.minikube/machines/minikube/id_rsa Username:docker}
I1122 22:37:27.325218   29924 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1122 22:37:27.765131   29924 addons.go:419] installing /etc/kubernetes/addons/metrics-server-deployment.yaml
I1122 22:37:27.765142   29924 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-deployment.yaml (1902 bytes)
I1122 22:37:27.911535   29924 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1122 22:37:28.010449   29924 addons.go:419] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I1122 22:37:28.010461   29924 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I1122 22:37:28.452760   29924 addons.go:419] installing /etc/kubernetes/addons/metrics-server-rbac.yaml
I1122 22:37:28.452780   29924 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-rbac.yaml (2175 bytes)
I1122 22:37:28.494191   29924 addons.go:419] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I1122 22:37:28.494205   29924 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I1122 22:37:29.048358   29924 addons.go:419] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I1122 22:37:29.048374   29924 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I1122 22:37:29.158921   29924 addons.go:419] installing /etc/kubernetes/addons/metrics-server-service.yaml
I1122 22:37:29.158944   29924 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-service.yaml (446 bytes)
I1122 22:37:29.471833   29924 addons.go:419] installing /etc/kubernetes/addons/dashboard-dp.yaml
I1122 22:37:29.471851   29924 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4298 bytes)
I1122 22:37:29.891459   29924 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I1122 22:37:30.339284   29924 addons.go:419] installing /etc/kubernetes/addons/dashboard-role.yaml
I1122 22:37:30.339297   29924 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I1122 22:37:30.450527   29924 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.25.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (3.827187092s)
I1122 22:37:30.450548   29924 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (3.762251152s)
I1122 22:37:30.450594   29924 api_server.go:51] waiting for apiserver process to appear ...
I1122 22:37:30.450710   29924 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1122 22:37:30.451030   29924 start.go:806] CoreDNS already contains "host.minikube.internal" host record, skipping...
I1122 22:37:30.779900   29924 addons.go:419] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I1122 22:37:30.779912   29924 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I1122 22:37:30.859577   29924 addons.go:419] installing /etc/kubernetes/addons/dashboard-sa.yaml
I1122 22:37:30.859599   29924 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I1122 22:37:31.249945   29924 addons.go:419] installing /etc/kubernetes/addons/dashboard-secret.yaml
I1122 22:37:31.249962   29924 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I1122 22:37:31.475120   29924 addons.go:419] installing /etc/kubernetes/addons/dashboard-svc.yaml
I1122 22:37:31.475133   29924 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I1122 22:37:31.586976   29924 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I1122 22:37:35.693441   29924 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (8.368239853s)
I1122 22:37:35.693793   29924 main.go:134] libmachine: Making call to close driver server
I1122 22:37:35.693802   29924 main.go:134] libmachine: (minikube) Calling .Close
I1122 22:37:35.694017   29924 main.go:134] libmachine: Successfully made call to close driver server
I1122 22:37:35.694027   29924 main.go:134] libmachine: Making call to close connection to plugin binary
I1122 22:37:35.694037   29924 main.go:134] libmachine: Making call to close driver server
I1122 22:37:35.694045   29924 main.go:134] libmachine: (minikube) Calling .Close
I1122 22:37:35.694297   29924 main.go:134] libmachine: Successfully made call to close driver server
I1122 22:37:35.694309   29924 main.go:134] libmachine: Making call to close connection to plugin binary
I1122 22:37:35.694321   29924 main.go:134] libmachine: (minikube) DBG | Closing plugin on server side
I1122 22:37:35.694335   29924 main.go:134] libmachine: Making call to close driver server
I1122 22:37:35.694358   29924 main.go:134] libmachine: (minikube) Calling .Close
I1122 22:37:35.694596   29924 main.go:134] libmachine: (minikube) DBG | Closing plugin on server side
I1122 22:37:35.694623   29924 main.go:134] libmachine: Successfully made call to close driver server
I1122 22:37:35.694631   29924 main.go:134] libmachine: Making call to close connection to plugin binary
I1122 22:37:35.948237   29924 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (8.036707637s)
I1122 22:37:35.948305   29924 main.go:134] libmachine: Making call to close driver server
I1122 22:37:35.948331   29924 main.go:134] libmachine: (minikube) Calling .Close
I1122 22:37:35.948724   29924 main.go:134] libmachine: Successfully made call to close driver server
I1122 22:37:35.948737   29924 main.go:134] libmachine: Making call to close connection to plugin binary
I1122 22:37:35.948747   29924 main.go:134] libmachine: Making call to close driver server
I1122 22:37:35.948757   29924 main.go:134] libmachine: (minikube) Calling .Close
I1122 22:37:35.949086   29924 main.go:134] libmachine: Successfully made call to close driver server
I1122 22:37:35.949101   29924 main.go:134] libmachine: Making call to close connection to plugin binary
I1122 22:37:35.949172   29924 main.go:134] libmachine: (minikube) DBG | Closing plugin on server side
I1122 22:37:36.339929   29924 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: (6.448447225s)
I1122 22:37:36.339961   29924 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (5.889254012s)
I1122 22:37:36.339974   29924 main.go:134] libmachine: Making call to close driver server
I1122 22:37:36.339975   29924 api_server.go:71] duration metric: took 9.762027558s to wait for apiserver process to appear ...
I1122 22:37:36.339994   29924 main.go:134] libmachine: (minikube) Calling .Close
I1122 22:37:36.340016   29924 api_server.go:87] waiting for apiserver healthz status ...
I1122 22:37:36.340025   29924 api_server.go:252] Checking apiserver healthz at https://192.168.64.2:8443/healthz ...
I1122 22:37:36.340377   29924 main.go:134] libmachine: Successfully made call to close driver server
I1122 22:37:36.340391   29924 main.go:134] libmachine: Making call to close connection to plugin binary
I1122 22:37:36.340452   29924 main.go:134] libmachine: (minikube) DBG | Closing plugin on server side
I1122 22:37:36.340588   29924 main.go:134] libmachine: Making call to close driver server
I1122 22:37:36.340608   29924 main.go:134] libmachine: (minikube) Calling .Close
I1122 22:37:36.341103   29924 main.go:134] libmachine: (minikube) DBG | Closing plugin on server side
I1122 22:37:36.341141   29924 main.go:134] libmachine: Successfully made call to close driver server
I1122 22:37:36.341150   29924 main.go:134] libmachine: Making call to close connection to plugin binary
I1122 22:37:36.341158   29924 addons.go:457] Verifying addon metrics-server=true in "minikube"
I1122 22:37:36.372811   29924 api_server.go:278] https://192.168.64.2:8443/healthz returned 200:
ok
I1122 22:37:36.376331   29924 api_server.go:140] control plane version: v1.25.3
I1122 22:37:36.376346   29924 api_server.go:130] duration metric: took 36.32437ms to wait for apiserver health ...
I1122 22:37:36.376352   29924 system_pods.go:43] waiting for kube-system pods to appear ...
I1122 22:37:36.398814   29924 system_pods.go:59] 8 kube-system pods found
I1122 22:37:36.398845   29924 system_pods.go:61] "coredns-565d847f94-dnz9v" [43062ecb-3ad1-4d34-bc7a-274385cecdd0] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1122 22:37:36.398857   29924 system_pods.go:61] "etcd-minikube" [06e9edda-1700-4025-801f-3ce005d84918] Running
I1122 22:37:36.398864   29924 system_pods.go:61] "kube-apiserver-minikube" [bc58b482-9725-42c9-8396-dad58b99ab35] Running
I1122 22:37:36.398871   29924 system_pods.go:61] "kube-controller-manager-minikube" [8474f280-c06f-4164-8e45-cb7f860b9579] Running
I1122 22:37:36.398887   29924 system_pods.go:61] "kube-proxy-tvntg" [94f03b72-6e6e-4824-b18b-623db750462f] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1122 22:37:36.398895   29924 system_pods.go:61] "kube-scheduler-minikube" [df12115d-4178-4830-887d-3fa5043e65b6] Running
I1122 22:37:36.398905   29924 system_pods.go:61] "metrics-server-769cd898cd-tj8zr" [59032a3d-73ba-4656-b8b6-26c6b5421a71] Running / Ready:ContainersNotReady (containers with unready status: [metrics-server]) / ContainersReady:ContainersNotReady (containers with unready status: [metrics-server])
I1122 22:37:36.398914   29924 system_pods.go:61] "storage-provisioner" [3e8ecef5-102c-4480-8778-3c00d413ea53] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1122 22:37:36.398921   29924 system_pods.go:74] duration metric: took 22.563576ms to wait for pod list to return data ...
I1122 22:37:36.398932   29924 kubeadm.go:573] duration metric: took 9.82098389s to wait for : map[apiserver:true system_pods:true] ...
I1122 22:37:36.398947   29924 node_conditions.go:102] verifying NodePressure condition ...
I1122 22:37:36.415399   29924 node_conditions.go:122] node storage ephemeral capacity is 17784752Ki
I1122 22:37:36.415416   29924 node_conditions.go:123] node cpu capacity is 2
I1122 22:37:36.415426   29924 node_conditions.go:105] duration metric: took 16.475017ms to run NodePressure ...
I1122 22:37:36.415438   29924 start.go:217] waiting for startup goroutines ...
I1122 22:37:36.796183   29924 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (5.209197264s)
I1122 22:37:36.796225   29924 main.go:134] libmachine: Making call to close driver server
I1122 22:37:36.796235   29924 main.go:134] libmachine: (minikube) Calling .Close
I1122 22:37:36.796528   29924 main.go:134] libmachine: (minikube) DBG | Closing plugin on server side
I1122 22:37:36.796560   29924 main.go:134] libmachine: Successfully made call to close driver server
I1122 22:37:36.796571   29924 main.go:134] libmachine: Making call to close connection to plugin binary
I1122 22:37:36.796589   29924 main.go:134] libmachine: Making call to close driver server
I1122 22:37:36.796598   29924 main.go:134] libmachine: (minikube) Calling .Close
I1122 22:37:36.796970   29924 main.go:134] libmachine: Successfully made call to close driver server
I1122 22:37:36.796978   29924 main.go:134] libmachine: (minikube) DBG | Closing plugin on server side
I1122 22:37:36.796989   29924 main.go:134] libmachine: Making call to close connection to plugin binary
I1122 22:37:36.825411   29924 out.go:177] 💡  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server	


I1122 22:37:36.868776   29924 out.go:177] 🌟  Enabled addons: default-storageclass, storage-provisioner, metrics-server, dashboard
I1122 22:37:36.917860   29924 addons.go:488] enableAddons completed in 10.338743893s
I1122 22:37:36.920225   29924 ssh_runner.go:195] Run: rm -f paused
I1122 22:37:36.995094   29924 start.go:506] kubectl: 1.24.1, cluster: 1.25.3 (minor skew: 1)
I1122 22:37:37.016942   29924 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* -- Journal begins at Thu 2022-11-24 02:37:43 UTC, ends at Thu 2022-11-24 21:32:02 UTC. --
Nov 24 21:18:40 minikube dockerd[909]: time="2022-11-24T21:18:40.676839145Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Nov 24 21:18:40 minikube dockerd[909]: time="2022-11-24T21:18:40.678324630Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Nov 24 21:18:40 minikube dockerd[909]: time="2022-11-24T21:18:40.683898246Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Nov 24 21:18:40 minikube dockerd[909]: time="2022-11-24T21:18:40.687162995Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/bd01f27dc23bfaf6af0b2ad4f3071b1f648df9ec7a1c0474859132427221d18f pid=1078784 runtime=io.containerd.runc.v2
Nov 24 21:18:40 minikube dockerd[909]: time="2022-11-24T21:18:40.763270969Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Nov 24 21:18:40 minikube dockerd[909]: time="2022-11-24T21:18:40.763601121Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Nov 24 21:18:40 minikube dockerd[909]: time="2022-11-24T21:18:40.763886806Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Nov 24 21:18:40 minikube dockerd[909]: time="2022-11-24T21:18:40.767036342Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/526995c41b2369f7b4c58c17a8b07ec21b749170fb60bde8b9b482b35787fa93 pid=1078828 runtime=io.containerd.runc.v2
Nov 24 21:20:09 minikube dockerd[903]: time="2022-11-24T21:20:09.897098933Z" level=error msg="Not continuing with pull after error: context canceled"
Nov 24 21:20:10 minikube dockerd[909]: time="2022-11-24T21:20:10.509116306Z" level=info msg="shim disconnected" id=4a05d639a33875094e699a8a1fb01426056333d599ae4feb283645620743e27a
Nov 24 21:20:10 minikube dockerd[909]: time="2022-11-24T21:20:10.509316006Z" level=warning msg="cleaning up after shim disconnected" id=4a05d639a33875094e699a8a1fb01426056333d599ae4feb283645620743e27a namespace=moby
Nov 24 21:20:10 minikube dockerd[909]: time="2022-11-24T21:20:10.509356886Z" level=info msg="cleaning up dead shim"
Nov 24 21:20:10 minikube dockerd[903]: time="2022-11-24T21:20:10.514080919Z" level=info msg="ignoring event" container=4a05d639a33875094e699a8a1fb01426056333d599ae4feb283645620743e27a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 24 21:20:10 minikube dockerd[909]: time="2022-11-24T21:20:10.636120577Z" level=warning msg="cleanup warnings time=\"2022-11-24T21:20:10Z\" level=info msg=\"starting signal loop\" namespace=moby pid=1080498 runtime=io.containerd.runc.v2\n"
Nov 24 21:22:04 minikube dockerd[909]: time="2022-11-24T21:22:04.732731730Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Nov 24 21:22:04 minikube dockerd[909]: time="2022-11-24T21:22:04.735328105Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Nov 24 21:22:04 minikube dockerd[909]: time="2022-11-24T21:22:04.735494565Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Nov 24 21:22:04 minikube dockerd[909]: time="2022-11-24T21:22:04.736969809Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/a456e4ecf649dd2336c0f8c897ba481e681bf468e2a45c6df856db3431d09cc9 pid=1082646 runtime=io.containerd.runc.v2
Nov 24 21:22:04 minikube dockerd[909]: time="2022-11-24T21:22:04.797227289Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Nov 24 21:22:04 minikube dockerd[909]: time="2022-11-24T21:22:04.797906625Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Nov 24 21:22:04 minikube dockerd[909]: time="2022-11-24T21:22:04.797970709Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Nov 24 21:22:04 minikube dockerd[909]: time="2022-11-24T21:22:04.854996068Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/ee8258d15acc3dbc3f67a8ac9b9543b6a44683e2859f17e79b8e601ab9a72374 pid=1082652 runtime=io.containerd.runc.v2
Nov 24 21:22:05 minikube dockerd[909]: time="2022-11-24T21:22:05.041611029Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Nov 24 21:22:05 minikube dockerd[909]: time="2022-11-24T21:22:05.041893757Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Nov 24 21:22:05 minikube dockerd[909]: time="2022-11-24T21:22:05.041929648Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Nov 24 21:22:05 minikube dockerd[909]: time="2022-11-24T21:22:05.044530484Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/9b766eefbf80229cc6ea5cf1dafb1ddf1478a4744d726b43cd77f36e2134f904 pid=1082700 runtime=io.containerd.runc.v2
Nov 24 21:22:05 minikube dockerd[909]: time="2022-11-24T21:22:05.065426750Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Nov 24 21:22:05 minikube dockerd[909]: time="2022-11-24T21:22:05.066381306Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Nov 24 21:22:05 minikube dockerd[909]: time="2022-11-24T21:22:05.067256483Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Nov 24 21:22:05 minikube dockerd[909]: time="2022-11-24T21:22:05.078268672Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/eb1b5e985be350b2dcdbd7aa933273b89e5f82031051a9399f3aa5d3de346430 pid=1082704 runtime=io.containerd.runc.v2
Nov 24 21:22:08 minikube dockerd[903]: time="2022-11-24T21:22:08.998576705Z" level=error msg="Not continuing with pull after error: context canceled"
Nov 24 21:22:15 minikube dockerd[909]: time="2022-11-24T21:22:15.539582471Z" level=info msg="shim disconnected" id=d4c2a6df7c8601f5e9de84a818bd1ccb69d04bb0d5ec7c6bcee897f40278985f
Nov 24 21:22:15 minikube dockerd[909]: time="2022-11-24T21:22:15.544010289Z" level=warning msg="cleaning up after shim disconnected" id=d4c2a6df7c8601f5e9de84a818bd1ccb69d04bb0d5ec7c6bcee897f40278985f namespace=moby
Nov 24 21:22:15 minikube dockerd[909]: time="2022-11-24T21:22:15.544119450Z" level=info msg="cleaning up dead shim"
Nov 24 21:22:15 minikube dockerd[903]: time="2022-11-24T21:22:15.547387882Z" level=info msg="ignoring event" container=d4c2a6df7c8601f5e9de84a818bd1ccb69d04bb0d5ec7c6bcee897f40278985f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 24 21:22:15 minikube dockerd[909]: time="2022-11-24T21:22:15.668399551Z" level=warning msg="cleanup warnings time=\"2022-11-24T21:22:15Z\" level=info msg=\"starting signal loop\" namespace=moby pid=1083072 runtime=io.containerd.runc.v2\n"
Nov 24 21:24:07 minikube dockerd[903]: time="2022-11-24T21:24:07.958749780Z" level=error msg="Not continuing with pull after error: context canceled"
Nov 24 21:24:09 minikube dockerd[909]: time="2022-11-24T21:24:09.258903463Z" level=info msg="shim disconnected" id=d3a3e7dfc258eb363a9c4b2f4ce41ef4feb5289509d69c0739431c6d9eb10645
Nov 24 21:24:09 minikube dockerd[909]: time="2022-11-24T21:24:09.261652824Z" level=warning msg="cleaning up after shim disconnected" id=d3a3e7dfc258eb363a9c4b2f4ce41ef4feb5289509d69c0739431c6d9eb10645 namespace=moby
Nov 24 21:24:09 minikube dockerd[909]: time="2022-11-24T21:24:09.262487857Z" level=info msg="cleaning up dead shim"
Nov 24 21:24:09 minikube dockerd[903]: time="2022-11-24T21:24:09.266513603Z" level=info msg="ignoring event" container=d3a3e7dfc258eb363a9c4b2f4ce41ef4feb5289509d69c0739431c6d9eb10645 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 24 21:24:09 minikube dockerd[909]: time="2022-11-24T21:24:09.342437996Z" level=warning msg="cleanup warnings time=\"2022-11-24T21:24:09Z\" level=info msg=\"starting signal loop\" namespace=moby pid=1085226 runtime=io.containerd.runc.v2\ntime=\"2022-11-24T21:24:09Z\" level=warning msg=\"failed to remove runc container\" error=\"runc did not terminate successfully: exit status 255: \" runtime=io.containerd.runc.v2\n"
Nov 24 21:26:06 minikube dockerd[903]: time="2022-11-24T21:26:06.923944124Z" level=error msg="Not continuing with pull after error: context canceled"
Nov 24 21:26:09 minikube dockerd[903]: time="2022-11-24T21:26:09.433583498Z" level=info msg="ignoring event" container=42446be571110ac318453cb9b4e8b455d15c8c95841d6d068b10038f765fb805 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 24 21:26:09 minikube dockerd[909]: time="2022-11-24T21:26:09.466535225Z" level=info msg="shim disconnected" id=42446be571110ac318453cb9b4e8b455d15c8c95841d6d068b10038f765fb805
Nov 24 21:26:09 minikube dockerd[909]: time="2022-11-24T21:26:09.472112307Z" level=warning msg="cleaning up after shim disconnected" id=42446be571110ac318453cb9b4e8b455d15c8c95841d6d068b10038f765fb805 namespace=moby
Nov 24 21:26:09 minikube dockerd[909]: time="2022-11-24T21:26:09.473843726Z" level=info msg="cleaning up dead shim"
Nov 24 21:26:09 minikube dockerd[909]: time="2022-11-24T21:26:09.634098889Z" level=warning msg="cleanup warnings time=\"2022-11-24T21:26:09Z\" level=info msg=\"starting signal loop\" namespace=moby pid=1087370 runtime=io.containerd.runc.v2\n"
Nov 24 21:28:06 minikube dockerd[903]: time="2022-11-24T21:28:05.998995702Z" level=error msg="Not continuing with pull after error: context canceled"
Nov 24 21:28:07 minikube dockerd[909]: time="2022-11-24T21:28:07.203996141Z" level=info msg="shim disconnected" id=bd01f27dc23bfaf6af0b2ad4f3071b1f648df9ec7a1c0474859132427221d18f
Nov 24 21:28:07 minikube dockerd[909]: time="2022-11-24T21:28:07.204234230Z" level=warning msg="cleaning up after shim disconnected" id=bd01f27dc23bfaf6af0b2ad4f3071b1f648df9ec7a1c0474859132427221d18f namespace=moby
Nov 24 21:28:07 minikube dockerd[909]: time="2022-11-24T21:28:07.204352375Z" level=info msg="cleaning up dead shim"
Nov 24 21:28:07 minikube dockerd[903]: time="2022-11-24T21:28:07.226116618Z" level=info msg="ignoring event" container=bd01f27dc23bfaf6af0b2ad4f3071b1f648df9ec7a1c0474859132427221d18f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 24 21:28:07 minikube dockerd[909]: time="2022-11-24T21:28:07.268599313Z" level=warning msg="cleanup warnings time=\"2022-11-24T21:28:07Z\" level=info msg=\"starting signal loop\" namespace=moby pid=1089346 runtime=io.containerd.runc.v2\n"
Nov 24 21:30:05 minikube dockerd[903]: time="2022-11-24T21:30:05.009633911Z" level=error msg="Not continuing with pull after error: context canceled"
Nov 24 21:30:07 minikube dockerd[909]: time="2022-11-24T21:30:07.287971358Z" level=info msg="shim disconnected" id=526995c41b2369f7b4c58c17a8b07ec21b749170fb60bde8b9b482b35787fa93
Nov 24 21:30:07 minikube dockerd[909]: time="2022-11-24T21:30:07.288356893Z" level=warning msg="cleaning up after shim disconnected" id=526995c41b2369f7b4c58c17a8b07ec21b749170fb60bde8b9b482b35787fa93 namespace=moby
Nov 24 21:30:07 minikube dockerd[909]: time="2022-11-24T21:30:07.288416245Z" level=info msg="cleaning up dead shim"
Nov 24 21:30:07 minikube dockerd[903]: time="2022-11-24T21:30:07.331305234Z" level=info msg="ignoring event" container=526995c41b2369f7b4c58c17a8b07ec21b749170fb60bde8b9b482b35787fa93 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 24 21:30:07 minikube dockerd[909]: time="2022-11-24T21:30:07.469268642Z" level=warning msg="cleanup warnings time=\"2022-11-24T21:30:07Z\" level=info msg=\"starting signal loop\" namespace=moby pid=1091294 runtime=io.containerd.runc.v2\n"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                                   CREATED             STATE               NAME                        ATTEMPT             POD ID
dd368e0e79de8       6e38f40d628db                                                                                                           2 hours ago         Running             storage-provisioner         80                  187a9808d2fd5
88e0ed44a68f9       e57a417f15d36                                                                                                           2 hours ago         Running             metrics-server              62                  47956a6aa545a
ab152c6ac6e47       6e38f40d628db                                                                                                           16 hours ago        Exited              storage-provisioner         79                  187a9808d2fd5
3ca4e41f0c013       115053965e86b                                                                                                           16 hours ago        Running             dashboard-metrics-scraper   3                   8812e4d6b03c5
f868aab1f485d       e57a417f15d36                                                                                                           16 hours ago        Exited              metrics-server              61                  47956a6aa545a
794bbcc4f6340       minh123456/system-design-gateway@sha256:d5e465848dfd3cfc92d85f819525b969ffb96fce1381532002b83da0e8131271                16 hours ago        Running             system-design-gateway       0                   c1b7443e9c99b
105436f4853d4       minh123456/system-design-gateway@sha256:d5e465848dfd3cfc92d85f819525b969ffb96fce1381532002b83da0e8131271                16 hours ago        Running             system-design-gateway       0                   d3b715a49ed2a
6e98c5168242b       a97320763e5e2                                                                                                           17 hours ago        Running             rabbitmq                    0                   97b2ace11c030
bcb600ef65bfb       k8s.gcr.io/ingress-nginx/controller@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8             18 hours ago        Running             controller                  0                   a31025a8041e9
1094b831f3ffb       c41e9fcadf5a2                                                                                                           18 hours ago        Exited              patch                       1                   169ce347efed3
856a99a8d3e82       k8s.gcr.io/ingress-nginx/kube-webhook-certgen@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660   18 hours ago        Exited              create                      0                   9c161f55a8bb3
8d3dba6ac3714       minh123456/system-design-auth@sha256:a19986dd475a59fa3b58ae92d8428c7a93e9c4e7eb44a320e1c9d91a530090fc                   27 hours ago        Running             system-design-auth          0                   ea523a35e7084
a37d4bad20d6d       minh123456/system-design-auth@sha256:a19986dd475a59fa3b58ae92d8428c7a93e9c4e7eb44a320e1c9d91a530090fc                   27 hours ago        Running             system-design-auth          0                   706f9d2d5e15b
32bd48af83965       07655ddf2eebe                                                                                                           37 hours ago        Running             kubernetes-dashboard        3                   a2a9c8dfe3658
459875c3c00ad       115053965e86b                                                                                                           42 hours ago        Exited              dashboard-metrics-scraper   2                   8812e4d6b03c5
cb360f92f96ea       07655ddf2eebe                                                                                                           42 hours ago        Exited              kubernetes-dashboard        2                   a2a9c8dfe3658
3c55d35dda6bf       5185b96f0becf                                                                                                           42 hours ago        Running             coredns                     2                   07c6cd79e5029
7f71ad8cd10d2       beaaf00edd38a                                                                                                           42 hours ago        Running             kube-proxy                  1                   a3a50ab0dca27
d14fb251a8c35       a8a176a5d5d69                                                                                                           42 hours ago        Running             etcd                        1                   59d81589201a0
799f3e27e3a50       6d23ec0e8b87e                                                                                                           42 hours ago        Running             kube-scheduler              1                   4ddf40a7ff6d8
f96b5cbf6d2fb       0346dbd74bcb9                                                                                                           42 hours ago        Running             kube-apiserver              1                   7b50596e2f394
e7d461b3fb9d1       6039992312758                                                                                                           42 hours ago        Running             kube-controller-manager     1                   baaf73e7a5f55
ea37dd91861a9       5185b96f0becf                                                                                                           2 days ago          Exited              coredns                     1                   cf2290771ee07
0bb512b86d290       beaaf00edd38a                                                                                                           12 days ago         Exited              kube-proxy                  0                   7a95cf96a9e56
aa4341287054f       a8a176a5d5d69                                                                                                           12 days ago         Exited              etcd                        0                   ff24f7a0c595c
eddc3882ef125       6d23ec0e8b87e                                                                                                           12 days ago         Exited              kube-scheduler              0                   903c15ebe9e09
57bb99562170f       0346dbd74bcb9                                                                                                           12 days ago         Exited              kube-apiserver              0                   50d817f6d02a0
26c72c7091656       6039992312758                                                                                                           12 days ago         Exited              kube-controller-manager     0                   9aa8a4556cf38

* 
* ==> coredns [3c55d35dda6b] <==
* [WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.33819017s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.06268919s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.267575196s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.27783956s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.483908874s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.081532263s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.325118707s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.032083s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.319156335s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.400738504s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.324060907s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.78037643s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.871508496s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.538136559s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.838978574s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 3.355508843s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.330571091s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.411344486s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 4.341478633s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 4.100444647s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.253868261s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.693976263s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": dial tcp :8080: i/o timeout (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.300112513s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.292639405s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.438822762s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.189295252s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.17419465s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.85994267s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.169328844s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.067319183s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 4.184502625s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.153187317s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.528048835s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.478737752s
W1124 05:27:16.712916       1 reflector.go:442] pkg/mod/k8s.io/client-go@v0.24.0/tools/cache/reflector.go:167: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1124 05:27:16.748428       1 reflector.go:442] pkg/mod/k8s.io/client-go@v0.24.0/tools/cache/reflector.go:167: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1124 05:27:16.984166       1 reflector.go:442] pkg/mod/k8s.io/client-go@v0.24.0/tools/cache/reflector.go:167: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1124 05:27:35.855619       1 reflector.go:324] pkg/mod/k8s.io/client-go@v0.24.0/tools/cache/reflector.go:167: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?resourceVersion=296005": net/http: TLS handshake timeout
W1124 05:27:35.864086       1 reflector.go:324] pkg/mod/k8s.io/client-go@v0.24.0/tools/cache/reflector.go:167: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=296024": net/http: TLS handshake timeout
I1124 05:27:35.865736       1 trace.go:205] Trace[1922784672]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.24.0/tools/cache/reflector.go:167 (24-Nov-2022 05:27:18.453) (total time: 17410ms):
Trace[1922784672]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=296024": net/http: TLS handshake timeout 16036ms (05:27:34.488)
Trace[1922784672]: [17.410860217s] [17.410860217s] END
I1124 05:27:35.873201       1 trace.go:205] Trace[2111901464]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.24.0/tools/cache/reflector.go:167 (24-Nov-2022 05:27:18.188) (total time: 17421ms):
Trace[2111901464]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?resourceVersion=296005": net/http: TLS handshake timeout 17403ms (05:27:35.855)
Trace[2111901464]: [17.421481488s] [17.421481488s] END
E1124 05:27:36.025592       1 reflector.go:138] pkg/mod/k8s.io/client-go@v0.24.0/tools/cache/reflector.go:167: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?resourceVersion=296005": net/http: TLS handshake timeout
E1124 05:27:36.026223       1 reflector.go:138] pkg/mod/k8s.io/client-go@v0.24.0/tools/cache/reflector.go:167: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=296024": net/http: TLS handshake timeout
W1124 05:27:37.545170       1 reflector.go:324] pkg/mod/k8s.io/client-go@v0.24.0/tools/cache/reflector.go:167: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?resourceVersion=295988": net/http: TLS handshake timeout
I1124 05:27:37.545359       1 trace.go:205] Trace[1320162554]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.24.0/tools/cache/reflector.go:167 (24-Nov-2022 05:27:18.236) (total time: 19308ms):
Trace[1320162554]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?resourceVersion=295988": net/http: TLS handshake timeout 19308ms (05:27:37.545)
Trace[1320162554]: [19.30850191s] [19.30850191s] END
E1124 05:27:37.545413       1 reflector.go:138] pkg/mod/k8s.io/client-go@v0.24.0/tools/cache/reflector.go:167: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?resourceVersion=295988": net/http: TLS handshake timeout

* 
* ==> coredns [ea37dd91861a] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = 7135f430aea492809ab227b028bd16c96f6629e00404d9ec4f44cae029eb3743d1cfe4a9d0cc8fbbd4cfa53556972f2bbf615e7c9e8412e85d290539257166ad
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.074634725s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.012394526s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.597672627s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.670080487s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.415749801s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.491789002s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.234976857s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.476181146s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 7.471688878s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.084816771s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.300361659s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.306972969s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.266114859s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.823711876s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.177533781s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=986b1ebd987211ed16f8cc10aed7d2c42fc8392f
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2022_11_12T15_53_44_0700
                    minikube.k8s.io/version=v1.28.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sat, 12 Nov 2022 20:53:40 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 24 Nov 2022 21:32:07 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 24 Nov 2022 21:32:06 +0000   Thu, 24 Nov 2022 20:00:30 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 24 Nov 2022 21:32:06 +0000   Thu, 24 Nov 2022 20:00:30 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 24 Nov 2022 21:32:06 +0000   Thu, 24 Nov 2022 20:00:30 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 24 Nov 2022 21:32:06 +0000   Thu, 24 Nov 2022 20:00:30 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.64.2
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  17784752Ki
  hugepages-2Mi:      0
  memory:             3914660Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  17784752Ki
  hugepages-2Mi:      0
  memory:             3914660Ki
  pods:               110
System Info:
  Machine ID:                 52d4983627024f3b88d65f39b07a9339
  System UUID:                ee4611ed-0000-0000-829f-f2189895b8a2
  Boot ID:                    1fbde959-7a6a-4113-97a4-f040870a6bb1
  Kernel Version:             5.10.57
  OS Image:                   Buildroot 2021.02.12
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.20
  Kubelet Version:            v1.25.3
  Kube-Proxy Version:         v1.25.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (21 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     system-design-auth-7848db97b5-2mdpm          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         27h
  default                     system-design-auth-7848db97b5-4d7q7          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         27h
  default                     system-design-converter-688d4c979c-2qk9c     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
  default                     system-design-converter-688d4c979c-6rd58     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
  default                     system-design-converter-688d4c979c-9gqj9     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
  default                     system-design-converter-688d4c979c-fqzll     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         13m
  default                     system-design-converter-688d4c979c-vnnb9     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
  default                     system-design-gateway-578d8dcc55-2ww66       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         16h
  default                     system-design-gateway-578d8dcc55-l7wqz       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         16h
  default                     system-design-rabbitmq-0                     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         17h
  ingress-nginx               ingress-nginx-controller-5959f988fd-4vs7d    100m (5%!)(MISSING)     0 (0%!)(MISSING)      90Mi (2%!)(MISSING)        0 (0%!)(MISSING)         17h
  kube-system                 coredns-565d847f94-dnz9v                     100m (5%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     12d
  kube-system                 etcd-minikube                                100m (5%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         12d
  kube-system                 kube-apiserver-minikube                      250m (12%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         12d
  kube-system                 kube-controller-manager-minikube             200m (10%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         12d
  kube-system                 kube-proxy-tvntg                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         12d
  kube-system                 kube-scheduler-minikube                      100m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         12d
  kube-system                 metrics-server-769cd898cd-tj8zr              100m (5%!)(MISSING)     0 (0%!)(MISSING)      200Mi (5%!)(MISSING)       0 (0%!)(MISSING)         11d
  kube-system                 storage-provisioner                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         12d
  kubernetes-dashboard        dashboard-metrics-scraper-b74747df5-trpw4    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11d
  kubernetes-dashboard        kubernetes-dashboard-57bbdc5f89-6jzqz        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                950m (47%!)(MISSING)   0 (0%!)(MISSING)
  memory             460Mi (12%!)(MISSING)  170Mi (4%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
Events:              <none>

* 
* ==> dmesg <==
* [Nov24 00:10] ERROR: earlyprintk= earlyser already used
[  +0.000000] You have booted with nomodeset. This means your GPU drivers are DISABLED
[  +0.000001] Any video related functionality will be severely degraded, and you may not even be able to suspend the system properly
[  +0.000001] Unless you actually understand what nomodeset does, you should reboot without enabling it
[  +0.248372] ACPI BIOS Warning (bug): Incorrect checksum in table [DSDT] - 0xBE, should be 0x1B (20200925/tbprint-173)
[Nov24 00:11] ACPI Error: Could not enable RealTimeClock event (20200925/evxfevnt-182)
[  +0.000034] ACPI Warning: Could not enable fixed event - RealTimeClock (4) (20200925/evxface-618)
[  +0.019693] platform regulatory.0: Direct firmware load for regulatory.db failed with error -2
[  +4.355340] systemd-fstab-generator[124]: Ignoring "noauto" for root device
[  +0.121411] systemd[1]: systemd-journald.service: unit configures an IP firewall, but the local system does not support BPF/cgroup firewalling.
[  +0.000004] systemd[1]: (This warning is only shown for the first unit using IP firewalling.)
[  +3.212991] NFSD: Using /var/lib/nfs/v4recovery as the NFSv4 state recovery directory
[  +0.000008] NFSD: unable to find recovery directory /var/lib/nfs/v4recovery
[  +0.000001] NFSD: Unable to initialize client recovery tracking! (-2)
[ +16.358085] systemd-fstab-generator[526]: Ignoring "noauto" for root device
[  +0.237750] systemd-fstab-generator[537]: Ignoring "noauto" for root device
[  +5.609703] systemd-fstab-generator[871]: Ignoring "noauto" for root device
[  +0.575791] systemd-fstab-generator[882]: Ignoring "noauto" for root device
[  +0.354028] systemd-fstab-generator[893]: Ignoring "noauto" for root device
[  +1.947556] kauditd_printk_skb: 28 callbacks suppressed
[  +0.576537] systemd-fstab-generator[1051]: Ignoring "noauto" for root device
[  +0.317641] systemd-fstab-generator[1062]: Ignoring "noauto" for root device
[  +7.652489] systemd-fstab-generator[1301]: Ignoring "noauto" for root device
[  +0.886181] kauditd_printk_skb: 29 callbacks suppressed
[Nov24 00:12] kauditd_printk_skb: 8 callbacks suppressed
[ +16.862996] kauditd_printk_skb: 14 callbacks suppressed
[Nov24 00:16] hrtimer: interrupt took 1195578 ns
[Nov24 00:27] clocksource: timekeeping watchdog on CPU0: hpet retried 2 times before success
[Nov24 00:45] clocksource: timekeeping watchdog on CPU0: hpet retried 2 times before success
[Nov24 01:01] clocksource: timekeeping watchdog on CPU1: hpet retried 2 times before success
[Nov24 02:37] clocksource: timekeeping watchdog on CPU1: hpet retried 2 times before success
[Nov24 04:51] clocksource: timekeeping watchdog on CPU0: Marking clocksource 'tsc' as unstable because the skew is too large:
[  +0.010394] clocksource:                       'hpet' wd_now: 84741202 wd_last: 83502ac6 mask: ffffffff
[  +0.000106] clocksource:                       'tsc' cs_now: ad9cc1b64dd3 cs_last: 9bed72e28bb3 mask: ffffffffffffffff
[  +0.758011] TSC found unstable after boot, most likely due to broken BIOS. Use 'tsc=unstable'.
[  +0.024625] clocksource: Checking clocksource tsc synchronization from CPU 0.

* 
* ==> etcd [aa4341287054] <==
* {"level":"warn","ts":"2022-11-22T05:39:11.434Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"105.594737ms","expected-duration":"100ms","prefix":"","request":"header:<ID:12750399101386746886 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:259154 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:513 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:18"}
{"level":"info","ts":"2022-11-22T05:39:11.453Z","caller":"traceutil/trace.go:171","msg":"trace[45435503] transaction","detail":"{read_only:false; response_revision:259157; number_of_response:1; }","duration":"2.923811326s","start":"2022-11-22T05:39:08.529Z","end":"2022-11-22T05:39:11.453Z","steps":["trace[45435503] 'process raft request'  (duration: 2.799374359s)"],"step_count":1}
{"level":"warn","ts":"2022-11-22T05:39:11.453Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-22T05:39:08.529Z","time spent":"2.924496265s","remote":"127.0.0.1:59886","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":586,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:259154 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:513 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2022-11-22T05:39:11.508Z","caller":"traceutil/trace.go:171","msg":"trace[762804516] transaction","detail":"{read_only:false; response_revision:259158; number_of_response:1; }","duration":"107.622916ms","start":"2022-11-22T05:39:11.400Z","end":"2022-11-22T05:39:11.508Z","steps":["trace[762804516] 'process raft request'  (duration: 52.093876ms)"],"step_count":1}
{"level":"info","ts":"2022-11-22T05:39:11.509Z","caller":"traceutil/trace.go:171","msg":"trace[533151158] linearizableReadLoop","detail":"{readStateIndex:334185; appliedIndex:334186; }","duration":"120.488039ms","start":"2022-11-22T05:39:11.388Z","end":"2022-11-22T05:39:11.509Z","steps":["trace[533151158] 'read index received'  (duration: 120.176642ms)","trace[533151158] 'applied index is now lower than readState.Index'  (duration: 19.987µs)"],"step_count":2}
{"level":"warn","ts":"2022-11-22T05:39:11.763Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"323.094602ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/csistoragecapacities/\" range_end:\"/registry/csistoragecapacities0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-22T05:39:11.764Z","caller":"traceutil/trace.go:171","msg":"trace[1113813238] range","detail":"{range_begin:/registry/csistoragecapacities/; range_end:/registry/csistoragecapacities0; response_count:0; response_revision:259158; }","duration":"332.370162ms","start":"2022-11-22T05:39:11.431Z","end":"2022-11-22T05:39:11.764Z","steps":["trace[1113813238] 'agreement among raft nodes before linearized reading'  (duration: 190.154472ms)","trace[1113813238] 'count revisions from in-memory index tree'  (duration: 132.782632ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-22T05:39:11.764Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-22T05:39:11.431Z","time spent":"332.645382ms","remote":"127.0.0.1:59504","response type":"/etcdserverpb.KV/Range","request count":0,"request size":68,"response count":0,"response size":29,"request content":"key:\"/registry/csistoragecapacities/\" range_end:\"/registry/csistoragecapacities0\" count_only:true "}
{"level":"warn","ts":"2022-11-22T05:39:11.776Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"387.88491ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-22T05:39:11.777Z","caller":"traceutil/trace.go:171","msg":"trace[2124020695] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:259158; }","duration":"388.202204ms","start":"2022-11-22T05:39:11.388Z","end":"2022-11-22T05:39:11.776Z","steps":["trace[2124020695] 'agreement among raft nodes before linearized reading'  (duration: 135.492568ms)","trace[2124020695] 'range keys from in-memory index tree'  (duration: 178.45669ms)","trace[2124020695] 'assemble the response'  (duration: 50.500315ms)"],"step_count":3}
{"level":"warn","ts":"2022-11-22T05:39:11.777Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-22T05:39:11.388Z","time spent":"388.590851ms","remote":"127.0.0.1:60252","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2022-11-22T05:39:11.902Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"461.50397ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-22T05:39:11.902Z","caller":"traceutil/trace.go:171","msg":"trace[1569869487] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:259158; }","duration":"468.744156ms","start":"2022-11-22T05:39:11.433Z","end":"2022-11-22T05:39:11.902Z","steps":["trace[1569869487] 'agreement among raft nodes before linearized reading'  (duration: 203.554246ms)","trace[1569869487] 'range keys from in-memory index tree'  (duration: 257.499817ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-22T05:39:11.903Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-22T05:39:11.433Z","time spent":"469.224942ms","remote":"127.0.0.1:59228","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2022-11-22T05:39:11.911Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"442.760776ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/prioritylevelconfigurations/\" range_end:\"/registry/prioritylevelconfigurations0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"warn","ts":"2022-11-22T05:39:11.950Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"499.807827ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/networkpolicies/\" range_end:\"/registry/networkpolicies0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-22T05:39:11.951Z","caller":"traceutil/trace.go:171","msg":"trace[1314801992] range","detail":"{range_begin:/registry/networkpolicies/; range_end:/registry/networkpolicies0; response_count:0; response_revision:259158; }","duration":"500.312698ms","start":"2022-11-22T05:39:11.450Z","end":"2022-11-22T05:39:11.951Z","steps":["trace[1314801992] 'agreement among raft nodes before linearized reading'  (duration: 186.295385ms)","trace[1314801992] 'count revisions from in-memory index tree'  (duration: 257.397783ms)","trace[1314801992] 'filter and sort the key-value pairs'  (duration: 32.272085ms)"],"step_count":3}
{"level":"warn","ts":"2022-11-22T05:39:11.951Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-22T05:39:11.450Z","time spent":"500.806677ms","remote":"127.0.0.1:50030","response type":"/etcdserverpb.KV/Range","request count":0,"request size":58,"response count":0,"response size":29,"request content":"key:\"/registry/networkpolicies/\" range_end:\"/registry/networkpolicies0\" count_only:true "}
{"level":"warn","ts":"2022-11-22T05:39:11.805Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"313.768275ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/leases/\" range_end:\"/registry/leases0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2022-11-22T05:39:11.975Z","caller":"traceutil/trace.go:171","msg":"trace[607193275] range","detail":"{range_begin:/registry/leases/; range_end:/registry/leases0; response_count:0; response_revision:259158; }","duration":"484.613221ms","start":"2022-11-22T05:39:11.490Z","end":"2022-11-22T05:39:11.975Z","steps":["trace[607193275] 'agreement among raft nodes before linearized reading'  (duration: 146.043397ms)","trace[607193275] 'count revisions from in-memory index tree'  (duration: 167.492529ms)"],"step_count":2}
{"level":"info","ts":"2022-11-22T05:39:11.912Z","caller":"traceutil/trace.go:171","msg":"trace[892896859] range","detail":"{range_begin:/registry/prioritylevelconfigurations/; range_end:/registry/prioritylevelconfigurations0; response_count:0; response_revision:259158; }","duration":"443.186698ms","start":"2022-11-22T05:39:11.468Z","end":"2022-11-22T05:39:11.911Z","steps":["trace[892896859] 'agreement among raft nodes before linearized reading'  (duration: 168.35047ms)","trace[892896859] 'count revisions from in-memory index tree'  (duration: 235.823132ms)","trace[892896859] 'filter and sort the key-value pairs'  (duration: 38.018543ms)"],"step_count":3}
{"level":"warn","ts":"2022-11-22T05:39:11.975Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-22T05:39:11.490Z","time spent":"484.912726ms","remote":"127.0.0.1:48972","response type":"/etcdserverpb.KV/Range","request count":0,"request size":40,"response count":1,"response size":31,"request content":"key:\"/registry/leases/\" range_end:\"/registry/leases0\" count_only:true "}
{"level":"warn","ts":"2022-11-22T05:39:11.975Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-22T05:39:11.468Z","time spent":"507.18972ms","remote":"127.0.0.1:59458","response type":"/etcdserverpb.KV/Range","request count":0,"request size":82,"response count":8,"response size":31,"request content":"key:\"/registry/prioritylevelconfigurations/\" range_end:\"/registry/prioritylevelconfigurations0\" count_only:true "}
{"level":"warn","ts":"2022-11-22T05:39:13.318Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"119.187864ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-22T05:39:13.318Z","caller":"traceutil/trace.go:171","msg":"trace[775343486] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:259158; }","duration":"119.422716ms","start":"2022-11-22T05:39:13.198Z","end":"2022-11-22T05:39:13.318Z","steps":["trace[775343486] 'assemble the response'  (duration: 54.188903ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-22T05:39:13.723Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"273.348194ms","expected-duration":"100ms","prefix":"","request":"header:<ID:12750399101386746899 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.64.2\" mod_revision:259155 > success:<request_put:<key:\"/registry/masterleases/192.168.64.2\" value_size:65 lease:3527027064531971088 >> failure:<request_range:<key:\"/registry/masterleases/192.168.64.2\" > >>","response":"size:18"}
{"level":"info","ts":"2022-11-22T05:39:13.958Z","caller":"traceutil/trace.go:171","msg":"trace[421343859] transaction","detail":"{read_only:false; response_revision:259160; number_of_response:1; }","duration":"681.379245ms","start":"2022-11-22T05:39:13.276Z","end":"2022-11-22T05:39:13.958Z","steps":["trace[421343859] 'process raft request'  (duration: 168.657154ms)","trace[421343859] 'compare'  (duration: 236.661114ms)","trace[421343859] 'store kv pair into bolt db' {req_type:put; key:/registry/masterleases/192.168.64.2; req_size:114; } (duration: 11.601357ms)"],"step_count":3}
{"level":"warn","ts":"2022-11-22T05:39:14.137Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-22T05:39:13.276Z","time spent":"724.660851ms","remote":"127.0.0.1:40964","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":116,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.64.2\" mod_revision:259155 > success:<request_put:<key:\"/registry/masterleases/192.168.64.2\" value_size:65 lease:3527027064531971088 >> failure:<request_range:<key:\"/registry/masterleases/192.168.64.2\" > >"}
{"level":"info","ts":"2022-11-22T05:39:15.118Z","caller":"traceutil/trace.go:171","msg":"trace[2019000104] range","detail":"{range_begin:/registry/ingressclasses/; range_end:/registry/ingressclasses0; response_count:0; response_revision:259160; }","duration":"148.196744ms","start":"2022-11-22T05:39:14.969Z","end":"2022-11-22T05:39:15.093Z","steps":["trace[2019000104] 'agreement among raft nodes before linearized reading'  (duration: 103.258963ms)"],"step_count":1}
{"level":"info","ts":"2022-11-22T05:39:15.249Z","caller":"traceutil/trace.go:171","msg":"trace[1775450629] linearizableReadLoop","detail":"{readStateIndex:334189; appliedIndex:334189; }","duration":"134.177633ms","start":"2022-11-22T05:39:15.115Z","end":"2022-11-22T05:39:15.249Z","steps":["trace[1775450629] 'read index received'  (duration: 134.106577ms)","trace[1775450629] 'applied index is now lower than readState.Index'  (duration: 34.179µs)"],"step_count":2}
{"level":"warn","ts":"2022-11-22T05:39:15.250Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"134.891492ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:421"}
{"level":"info","ts":"2022-11-22T05:39:15.250Z","caller":"traceutil/trace.go:171","msg":"trace[63289971] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:259160; }","duration":"135.032206ms","start":"2022-11-22T05:39:15.115Z","end":"2022-11-22T05:39:15.250Z","steps":["trace[63289971] 'agreement among raft nodes before linearized reading'  (duration: 134.667431ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-22T05:39:15.860Z","caller":"etcdserver/v3_server.go:840","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":12750399101386746902,"retry-timeout":"500ms"}
{"level":"info","ts":"2022-11-22T05:39:15.994Z","caller":"traceutil/trace.go:171","msg":"trace[985187550] linearizableReadLoop","detail":"{readStateIndex:334189; appliedIndex:334189; }","duration":"637.896114ms","start":"2022-11-22T05:39:15.356Z","end":"2022-11-22T05:39:15.994Z","steps":["trace[985187550] 'read index received'  (duration: 550.450555ms)","trace[985187550] 'applied index is now lower than readState.Index'  (duration: 87.391293ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-22T05:39:16.019Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"662.439607ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-22T05:39:16.020Z","caller":"traceutil/trace.go:171","msg":"trace[196462134] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:259160; }","duration":"663.53344ms","start":"2022-11-22T05:39:15.356Z","end":"2022-11-22T05:39:16.020Z","steps":["trace[196462134] 'agreement among raft nodes before linearized reading'  (duration: 662.146585ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-22T05:39:16.020Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-22T05:39:15.356Z","time spent":"663.984464ms","remote":"127.0.0.1:60252","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2022-11-22T05:39:16.659Z","caller":"etcdserver/v3_server.go:817","msg":"ignored out-of-date read index response; local node read indexes queueing up and waiting to be in sync with leader","sent-request-id":12750399101386746903,"received-request-id":12750399101386746902}
{"level":"warn","ts":"2022-11-22T05:39:17.012Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"155.394914ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/endpointslices/default/kubernetes\" ","response":"range_response_count:1 size:480"}
{"level":"info","ts":"2022-11-22T05:39:17.080Z","caller":"traceutil/trace.go:171","msg":"trace[2078271339] range","detail":"{range_begin:/registry/endpointslices/default/kubernetes; range_end:; response_count:1; response_revision:259160; }","duration":"230.071303ms","start":"2022-11-22T05:39:16.849Z","end":"2022-11-22T05:39:17.079Z","steps":["trace[2078271339] 'agreement among raft nodes before linearized reading'  (duration: 29.932255ms)","trace[2078271339] 'range keys from bolt db'  (duration: 44.618502ms)","trace[2078271339] 'filter and sort the key-value pairs'  (duration: 55.437907ms)"],"step_count":3}
{"level":"warn","ts":"2022-11-22T05:39:17.691Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"262.810996ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-22T05:39:17.702Z","caller":"traceutil/trace.go:171","msg":"trace[1962859588] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:259160; }","duration":"273.917802ms","start":"2022-11-22T05:39:17.428Z","end":"2022-11-22T05:39:17.702Z","steps":["trace[1962859588] 'agreement among raft nodes before linearized reading'  (duration: 66.061783ms)","trace[1962859588] 'range keys from in-memory index tree'  (duration: 195.531148ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-22T05:39:17.732Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"211.33962ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:605"}
{"level":"info","ts":"2022-11-22T05:39:17.735Z","caller":"traceutil/trace.go:171","msg":"trace[1093633216] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:259160; }","duration":"213.749367ms","start":"2022-11-22T05:39:17.521Z","end":"2022-11-22T05:39:17.735Z","steps":["trace[1093633216] 'agreement among raft nodes before linearized reading'  (duration: 60.349326ms)","trace[1093633216] 'range keys from in-memory index tree'  (duration: 135.258376ms)","trace[1093633216] 'filter and sort the key-value pairs'  (duration: 14.457785ms)"],"step_count":3}
{"level":"warn","ts":"2022-11-22T05:39:21.612Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"118.840743ms","expected-duration":"100ms","prefix":"","request":"header:<ID:12750399101386746918 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:259157 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:513 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:18"}
{"level":"info","ts":"2022-11-22T05:39:21.612Z","caller":"traceutil/trace.go:171","msg":"trace[1496771087] transaction","detail":"{read_only:false; response_revision:259161; number_of_response:1; }","duration":"160.013687ms","start":"2022-11-22T05:39:21.452Z","end":"2022-11-22T05:39:21.612Z","steps":["trace[1496771087] 'process raft request'  (duration: 40.810554ms)","trace[1496771087] 'compare'  (duration: 16.600881ms)","trace[1496771087] 'store kv pair into bolt db' {req_type:put; key:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; req_size:583; } (duration: 77.714272ms)"],"step_count":3}
{"level":"info","ts":"2022-11-22T05:39:34.552Z","caller":"traceutil/trace.go:171","msg":"trace[1890357203] linearizableReadLoop","detail":"{readStateIndex:334203; appliedIndex:334203; }","duration":"118.506664ms","start":"2022-11-22T05:39:34.159Z","end":"2022-11-22T05:39:34.278Z","steps":["trace[1890357203] 'read index received'  (duration: 997.567µs)","trace[1890357203] 'applied index is now lower than readState.Index'  (duration: 60.157549ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-22T05:40:49.992Z","caller":"wal/wal.go:802","msg":"slow fdatasync","took":"1.261134271s","expected-duration":"1s"}
{"level":"warn","ts":"2022-11-22T05:40:50.294Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"291.849229ms","expected-duration":"100ms","prefix":"","request":"header:<ID:12750399101386747309 > lease_revoke:<id:30f2846d9e9ded86>","response":"size:29"}
{"level":"warn","ts":"2022-11-22T05:40:51.757Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"292.848632ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/apiregistration.k8s.io/apiservices/\" range_end:\"/registry/apiregistration.k8s.io/apiservices0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2022-11-22T05:40:51.758Z","caller":"traceutil/trace.go:171","msg":"trace[540612932] range","detail":"{range_begin:/registry/apiregistration.k8s.io/apiservices/; range_end:/registry/apiregistration.k8s.io/apiservices0; response_count:0; response_revision:259220; }","duration":"380.316256ms","start":"2022-11-22T05:40:51.377Z","end":"2022-11-22T05:40:51.757Z","steps":["trace[540612932] 'agreement among raft nodes before linearized reading'  (duration: 335.838217ms)","trace[540612932] 'count revisions from in-memory index tree'  (duration: 43.694631ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-22T05:40:51.758Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-22T05:40:51.377Z","time spent":"381.497165ms","remote":"127.0.0.1:34074","response type":"/etcdserverpb.KV/Range","request count":0,"request size":96,"response count":24,"response size":31,"request content":"key:\"/registry/apiregistration.k8s.io/apiservices/\" range_end:\"/registry/apiregistration.k8s.io/apiservices0\" count_only:true "}
{"level":"warn","ts":"2022-11-22T05:40:54.574Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"518.370783ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/ingress/\" range_end:\"/registry/ingress0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-22T05:40:54.574Z","caller":"traceutil/trace.go:171","msg":"trace[1108399334] range","detail":"{range_begin:/registry/ingress/; range_end:/registry/ingress0; response_count:0; response_revision:259220; }","duration":"518.727905ms","start":"2022-11-22T05:40:54.055Z","end":"2022-11-22T05:40:54.574Z","steps":["trace[1108399334] 'agreement among raft nodes before linearized reading'  (duration: 517.812963ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-22T05:40:54.574Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-22T05:40:54.055Z","time spent":"519.068334ms","remote":"127.0.0.1:34084","response type":"/etcdserverpb.KV/Range","request count":0,"request size":42,"response count":0,"response size":29,"request content":"key:\"/registry/ingress/\" range_end:\"/registry/ingress0\" count_only:true "}
{"level":"warn","ts":"2022-11-22T05:40:54.787Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"184.493471ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/\" range_end:\"/registry/clusterroles0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2022-11-22T05:40:54.810Z","caller":"traceutil/trace.go:171","msg":"trace[1790557755] range","detail":"{range_begin:/registry/clusterroles/; range_end:/registry/clusterroles0; response_count:0; response_revision:259220; }","duration":"201.728646ms","start":"2022-11-22T05:40:54.601Z","end":"2022-11-22T05:40:54.803Z","steps":["trace[1790557755] 'count revisions from in-memory index tree'  (duration: 176.898472ms)"],"step_count":1}
WARNING: 2022/11/22 05:40:56 [core] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
{"level":"warn","ts":"2022-11-22T05:40:58.159Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"206.394819ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/default/kubernetes\" ","response":"range_response_count:1 size:705"}
{"level":"info","ts":"2022-11-22T05:40:58.175Z","caller":"traceutil/trace.go:171","msg":"trace[1929143280] range","detail":"{range_begin:/registry/services/specs/default/kubernetes; range_end:; response_count:1; response_revision:259222; }","duration":"222.589816ms","start":"2022-11-22T05:40:57.952Z","end":"2022-11-22T05:40:58.175Z","steps":["trace[1929143280] 'range keys from bolt db'  (duration: 203.568833ms)"],"step_count":1}

* 
* ==> etcd [d14fb251a8c3] <==
* {"level":"info","ts":"2022-11-24T21:01:08.941Z","caller":"traceutil/trace.go:171","msg":"trace[1092041507] linearizableReadLoop","detail":"{readStateIndex:400797; appliedIndex:400794; }","duration":"138.062896ms","start":"2022-11-24T21:01:08.803Z","end":"2022-11-24T21:01:08.941Z","steps":["trace[1092041507] 'read index received'  (duration: 70.920317ms)","trace[1092041507] 'applied index is now lower than readState.Index'  (duration: 67.110697ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-24T21:01:08.941Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"138.553914ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/minions/minikube\" ","response":"range_response_count:1 size:6350"}
{"level":"info","ts":"2022-11-24T21:01:08.941Z","caller":"traceutil/trace.go:171","msg":"trace[881173109] range","detail":"{range_begin:/registry/minions/minikube; range_end:; response_count:1; response_revision:311957; }","duration":"138.676544ms","start":"2022-11-24T21:01:08.803Z","end":"2022-11-24T21:01:08.941Z","steps":["trace[881173109] 'agreement among raft nodes before linearized reading'  (duration: 138.395405ms)"],"step_count":1}
{"level":"info","ts":"2022-11-24T21:01:09.169Z","caller":"traceutil/trace.go:171","msg":"trace[870802985] transaction","detail":"{read_only:false; number_of_response:1; response_revision:311958; }","duration":"134.803767ms","start":"2022-11-24T21:01:09.035Z","end":"2022-11-24T21:01:09.169Z","steps":["trace[870802985] 'process raft request'  (duration: 96.146714ms)","trace[870802985] 'compare'  (duration: 33.934334ms)"],"step_count":2}
{"level":"info","ts":"2022-11-24T21:01:10.316Z","caller":"traceutil/trace.go:171","msg":"trace[834605921] transaction","detail":"{read_only:false; response_revision:311961; number_of_response:1; }","duration":"155.484482ms","start":"2022-11-24T21:01:10.161Z","end":"2022-11-24T21:01:10.316Z","steps":["trace[834605921] 'process raft request'  (duration: 146.308353ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-24T21:01:21.402Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"129.676895ms","expected-duration":"100ms","prefix":"","request":"header:<ID:12750399328769869180 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:311969 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1021 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:18"}
{"level":"info","ts":"2022-11-24T21:01:21.403Z","caller":"traceutil/trace.go:171","msg":"trace[196607449] transaction","detail":"{read_only:false; response_revision:311971; number_of_response:1; }","duration":"133.049903ms","start":"2022-11-24T21:01:21.270Z","end":"2022-11-24T21:01:21.403Z","steps":["trace[196607449] 'check requests'  (duration: 123.145966ms)"],"step_count":1}
{"level":"info","ts":"2022-11-24T21:02:23.437Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":311767}
{"level":"info","ts":"2022-11-24T21:02:23.461Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":311767,"took":"9.025207ms"}
{"level":"warn","ts":"2022-11-24T21:03:12.683Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"111.912248ms","expected-duration":"100ms","prefix":"","request":"header:<ID:12750399328769869781 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.64.2\" mod_revision:312063 > success:<request_put:<key:\"/registry/masterleases/192.168.64.2\" value_size:65 lease:3527027291915093971 >> failure:<request_range:<key:\"/registry/masterleases/192.168.64.2\" > >>","response":"size:18"}
{"level":"info","ts":"2022-11-24T21:03:12.683Z","caller":"traceutil/trace.go:171","msg":"trace[788896577] transaction","detail":"{read_only:false; response_revision:312070; number_of_response:1; }","duration":"142.446854ms","start":"2022-11-24T21:03:12.541Z","end":"2022-11-24T21:03:12.683Z","steps":["trace[788896577] 'process raft request'  (duration: 29.850611ms)","trace[788896577] 'compare'  (duration: 111.152816ms)"],"step_count":2}
{"level":"info","ts":"2022-11-24T21:07:23.477Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":312025}
{"level":"info","ts":"2022-11-24T21:07:23.506Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":312025,"took":"29.035762ms"}
{"level":"warn","ts":"2022-11-24T21:08:02.801Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"106.597323ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-24T21:08:02.807Z","caller":"traceutil/trace.go:171","msg":"trace[1074729617] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:312324; }","duration":"124.538253ms","start":"2022-11-24T21:08:02.682Z","end":"2022-11-24T21:08:02.807Z","steps":["trace[1074729617] 'agreement among raft nodes before linearized reading'  (duration: 37.070546ms)","trace[1074729617] 'range keys from in-memory index tree'  (duration: 81.109192ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-24T21:08:10.019Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"102.947527ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-24T21:08:10.019Z","caller":"traceutil/trace.go:171","msg":"trace[1001234919] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:312328; }","duration":"140.197603ms","start":"2022-11-24T21:08:09.836Z","end":"2022-11-24T21:08:10.019Z","steps":["trace[1001234919] 'agreement among raft nodes before linearized reading'  (duration: 139.610103ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-24T21:10:54.008Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"145.210053ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-24T21:10:54.008Z","caller":"traceutil/trace.go:171","msg":"trace[1510389875] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:312481; }","duration":"145.990029ms","start":"2022-11-24T21:10:53.862Z","end":"2022-11-24T21:10:54.008Z","steps":["trace[1510389875] 'agreement among raft nodes before linearized reading'  (duration: 145.063749ms)"],"step_count":1}
{"level":"info","ts":"2022-11-24T21:11:38.589Z","caller":"traceutil/trace.go:171","msg":"trace[2082225001] linearizableReadLoop","detail":"{readStateIndex:401495; appliedIndex:401495; }","duration":"149.794009ms","start":"2022-11-24T21:11:38.440Z","end":"2022-11-24T21:11:38.589Z","steps":["trace[2082225001] 'read index received'  (duration: 149.581562ms)","trace[2082225001] 'applied index is now lower than readState.Index'  (duration: 175.639µs)"],"step_count":2}
{"level":"warn","ts":"2022-11-24T21:11:38.590Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"150.404643ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-24T21:11:38.590Z","caller":"traceutil/trace.go:171","msg":"trace[1806633338] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:312518; }","duration":"150.856043ms","start":"2022-11-24T21:11:38.439Z","end":"2022-11-24T21:11:38.590Z","steps":["trace[1806633338] 'agreement among raft nodes before linearized reading'  (duration: 150.156389ms)"],"step_count":1}
{"level":"info","ts":"2022-11-24T21:11:42.884Z","caller":"traceutil/trace.go:171","msg":"trace[21541488] linearizableReadLoop","detail":"{readStateIndex:401497; appliedIndex:401497; }","duration":"357.072922ms","start":"2022-11-24T21:11:42.527Z","end":"2022-11-24T21:11:42.884Z","steps":["trace[21541488] 'read index received'  (duration: 356.884394ms)","trace[21541488] 'applied index is now lower than readState.Index'  (duration: 154.923µs)"],"step_count":2}
{"level":"warn","ts":"2022-11-24T21:11:42.904Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"376.803507ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-24T21:11:42.963Z","caller":"traceutil/trace.go:171","msg":"trace[1613360468] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:312519; }","duration":"435.665013ms","start":"2022-11-24T21:11:42.527Z","end":"2022-11-24T21:11:42.963Z","steps":["trace[1613360468] 'agreement among raft nodes before linearized reading'  (duration: 376.22492ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-24T21:11:42.979Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-24T21:11:42.527Z","time spent":"435.931252ms","remote":"127.0.0.1:42994","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2022-11-24T21:11:43.033Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"127.317366ms","expected-duration":"100ms","prefix":"","request":"header:<ID:12750399328769872512 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.64.2\" mod_revision:312513 > success:<request_put:<key:\"/registry/masterleases/192.168.64.2\" value_size:65 lease:3527027291915096702 >> failure:<request_range:<key:\"/registry/masterleases/192.168.64.2\" > >>","response":"size:18"}
{"level":"info","ts":"2022-11-24T21:11:43.054Z","caller":"traceutil/trace.go:171","msg":"trace[1762195674] transaction","detail":"{read_only:false; response_revision:312520; number_of_response:1; }","duration":"144.506736ms","start":"2022-11-24T21:11:42.889Z","end":"2022-11-24T21:11:43.033Z","steps":["trace[1762195674] 'process raft request'  (duration: 15.265109ms)","trace[1762195674] 'compare'  (duration: 75.400608ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-24T21:11:53.602Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"111.031001ms","expected-duration":"100ms","prefix":"","request":"header:<ID:12750399328769872560 username:\"kube-apiserver-etcd-client\" auth_revision:1 > lease_grant:<ttl:15-second id:30f284a28fb512af>","response":"size:41"}
{"level":"warn","ts":"2022-11-24T21:11:53.978Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"213.989793ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-24T21:11:53.979Z","caller":"traceutil/trace.go:171","msg":"trace[1138765927] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:312529; }","duration":"214.832794ms","start":"2022-11-24T21:11:53.764Z","end":"2022-11-24T21:11:53.979Z","steps":["trace[1138765927] 'range keys from in-memory index tree'  (duration: 212.689892ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-24T21:11:53.995Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"252.387228ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:421"}
{"level":"info","ts":"2022-11-24T21:11:53.995Z","caller":"traceutil/trace.go:171","msg":"trace[838681126] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:312529; }","duration":"252.617629ms","start":"2022-11-24T21:11:53.742Z","end":"2022-11-24T21:11:53.995Z","steps":["trace[838681126] 'agreement among raft nodes before linearized reading'  (duration: 22.974521ms)","trace[838681126] 'range keys from in-memory index tree'  (duration: 229.266307ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-24T21:12:00.639Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"125.205434ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-24T21:12:00.639Z","caller":"traceutil/trace.go:171","msg":"trace[981135217] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:312533; }","duration":"125.97858ms","start":"2022-11-24T21:12:00.513Z","end":"2022-11-24T21:12:00.639Z","steps":["trace[981135217] 'agreement among raft nodes before linearized reading'  (duration: 98.241144ms)","trace[981135217] 'filter and sort the key-value pairs'  (duration: 22.88488ms)"],"step_count":2}
{"level":"info","ts":"2022-11-24T21:12:23.539Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":312292}
{"level":"info","ts":"2022-11-24T21:12:23.554Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":312292,"took":"14.243664ms"}
{"level":"info","ts":"2022-11-24T21:14:28.654Z","caller":"traceutil/trace.go:171","msg":"trace[112205363] linearizableReadLoop","detail":"{readStateIndex:401683; appliedIndex:401683; }","duration":"135.920057ms","start":"2022-11-24T21:14:28.518Z","end":"2022-11-24T21:14:28.653Z","steps":["trace[112205363] 'read index received'  (duration: 135.858497ms)","trace[112205363] 'applied index is now lower than readState.Index'  (duration: 26.383µs)"],"step_count":2}
{"level":"warn","ts":"2022-11-24T21:14:28.655Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"137.121158ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-24T21:14:28.655Z","caller":"traceutil/trace.go:171","msg":"trace[359549707] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:312669; }","duration":"137.3586ms","start":"2022-11-24T21:14:28.517Z","end":"2022-11-24T21:14:28.655Z","steps":["trace[359549707] 'agreement among raft nodes before linearized reading'  (duration: 136.393242ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-24T21:14:35.787Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"167.091709ms","expected-duration":"100ms","prefix":"","request":"header:<ID:12750399328769873445 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:312675 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1021 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:18"}
{"level":"info","ts":"2022-11-24T21:14:35.788Z","caller":"traceutil/trace.go:171","msg":"trace[496160412] transaction","detail":"{read_only:false; response_revision:312676; number_of_response:1; }","duration":"214.509269ms","start":"2022-11-24T21:14:35.573Z","end":"2022-11-24T21:14:35.788Z","steps":["trace[496160412] 'store kv pair into bolt db' {req_type:put; key:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; req_size:1091; } (duration: 166.328207ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-24T21:15:09.984Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"194.179586ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-24T21:15:09.984Z","caller":"traceutil/trace.go:171","msg":"trace[270736223] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:312708; }","duration":"194.995988ms","start":"2022-11-24T21:15:09.789Z","end":"2022-11-24T21:15:09.984Z","steps":["trace[270736223] 'filter and sort the key-value pairs'  (duration: 155.229162ms)","trace[270736223] 'assemble the response'  (duration: 37.162938ms)"],"step_count":2}
{"level":"info","ts":"2022-11-24T21:17:04.997Z","caller":"traceutil/trace.go:171","msg":"trace[1894200848] linearizableReadLoop","detail":"{readStateIndex:401860; appliedIndex:401860; }","duration":"115.538036ms","start":"2022-11-24T21:17:04.881Z","end":"2022-11-24T21:17:04.997Z","steps":["trace[1894200848] 'read index received'  (duration: 13.847255ms)","trace[1894200848] 'applied index is now lower than readState.Index'  (duration: 57.523311ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-24T21:17:05.078Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"197.679106ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/\" range_end:\"/registry/serviceaccounts0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2022-11-24T21:17:05.079Z","caller":"traceutil/trace.go:171","msg":"trace[1507864967] range","detail":"{range_begin:/registry/serviceaccounts/; range_end:/registry/serviceaccounts0; response_count:0; response_revision:312814; }","duration":"197.876831ms","start":"2022-11-24T21:17:04.881Z","end":"2022-11-24T21:17:05.079Z","steps":["trace[1507864967] 'agreement among raft nodes before linearized reading'  (duration: 85.98643ms)","trace[1507864967] 'count revisions from in-memory index tree'  (duration: 111.583218ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-24T21:17:07.919Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"385.801824ms","expected-duration":"100ms","prefix":"","request":"header:<ID:12750399328769874267 > lease_revoke:<id:30f284a28fb5190a>","response":"size:29"}
{"level":"info","ts":"2022-11-24T21:17:23.585Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":312555}
{"level":"info","ts":"2022-11-24T21:17:23.595Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":312555,"took":"7.897384ms"}
{"level":"info","ts":"2022-11-24T21:22:23.631Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":312831}
{"level":"info","ts":"2022-11-24T21:22:23.635Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":312831,"took":"3.77548ms"}
{"level":"info","ts":"2022-11-24T21:27:23.652Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":313182}
{"level":"info","ts":"2022-11-24T21:27:23.657Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":313182,"took":"3.804682ms"}
{"level":"warn","ts":"2022-11-24T21:29:41.039Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"321.527017ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/\" range_end:\"/registry/replicasets0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2022-11-24T21:29:41.045Z","caller":"traceutil/trace.go:171","msg":"trace[1513247407] range","detail":"{range_begin:/registry/replicasets/; range_end:/registry/replicasets0; response_count:0; response_revision:313556; }","duration":"334.328279ms","start":"2022-11-24T21:29:40.705Z","end":"2022-11-24T21:29:41.040Z","steps":["trace[1513247407] 'agreement among raft nodes before linearized reading'  (duration: 300.328382ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-24T21:29:41.045Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-24T21:29:40.705Z","time spent":"340.080268ms","remote":"127.0.0.1:51332","response type":"/etcdserverpb.KV/Range","request count":0,"request size":50,"response count":8,"response size":31,"request content":"key:\"/registry/replicasets/\" range_end:\"/registry/replicasets0\" count_only:true "}
{"level":"info","ts":"2022-11-24T21:30:09.637Z","caller":"traceutil/trace.go:171","msg":"trace[917337163] linearizableReadLoop","detail":"{readStateIndex:402802; appliedIndex:402802; }","duration":"106.799616ms","start":"2022-11-24T21:30:09.530Z","end":"2022-11-24T21:30:09.637Z","steps":["trace[917337163] 'read index received'  (duration: 106.739152ms)","trace[917337163] 'applied index is now lower than readState.Index'  (duration: 27.139µs)"],"step_count":2}
{"level":"warn","ts":"2022-11-24T21:30:09.638Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"107.684217ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-24T21:30:09.638Z","caller":"traceutil/trace.go:171","msg":"trace[306833361] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:313584; }","duration":"107.808836ms","start":"2022-11-24T21:30:09.530Z","end":"2022-11-24T21:30:09.638Z","steps":["trace[306833361] 'agreement among raft nodes before linearized reading'  (duration: 107.433185ms)"],"step_count":1}

* 
* ==> kernel <==
*  21:32:10 up 21:21,  0 users,  load average: 6.72, 6.02, 5.92
Linux minikube 5.10.57 #1 SMP Fri Oct 28 21:02:11 UTC 2022 x86_64 GNU/Linux
PRETTY_NAME="Buildroot 2021.02.12"

* 
* ==> kube-apiserver [57bb99562170] <==
* I1122 05:39:14.781344       1 trace.go:205] Trace[106020775]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.64.2,type:*v1.Endpoints (22-Nov-2022 05:39:12.664) (total time: 2108ms):
Trace[106020775]: ---"initial value restored" 137ms (05:39:12.801)
Trace[106020775]: ---"Transaction prepared" 312ms (05:39:13.114)
Trace[106020775]: ---"Txn call finished" err:<nil> 1588ms (05:39:14.703)
Trace[106020775]: [2.108129802s] [2.108129802s] END
I1122 05:39:15.979819       1 trace.go:205] Trace[1788584667]: "Get" url:/api/v1/namespaces/default/endpoints/kubernetes,user-agent:kube-apiserver/v1.25.3 (linux/amd64) kubernetes/434bfd8,audit-id:66496372-7fb8-41fe-b857-f09594405f90,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (22-Nov-2022 05:39:14.895) (total time: 1083ms):
Trace[1788584667]: ---"About to write a response" 1080ms (05:39:15.975)
Trace[1788584667]: [1.083965436s] [1.083965436s] END
I1122 05:39:16.778330       1 trace.go:205] Trace[1270488]: "List(recursive=true) etcd3" audit-id:,key:/masterleases/,resourceVersion:0,resourceVersionMatch:NotOlderThan,limit:0,continue: (22-Nov-2022 05:39:16.007) (total time: 770ms):
Trace[1270488]: [770.98164ms] [770.98164ms] END
I1122 05:39:17.337564       1 trace.go:205] Trace[363294067]: "Get" url:/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/kubernetes,user-agent:kube-apiserver/v1.25.3 (linux/amd64) kubernetes/434bfd8,audit-id:a1d66c08-1cc1-4d1b-9b1e-8fc64e5617be,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (22-Nov-2022 05:39:16.794) (total time: 537ms):
Trace[363294067]: ---"About to write a response" 534ms (05:39:17.331)
Trace[363294067]: [537.792851ms] [537.792851ms] END
I1122 05:39:35.555370       1 trace.go:205] Trace[429548214]: "Get" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:6c26a9b7-7374-422d-949b-91c2740e9a8e,client:192.168.64.2,accept:application/json, */*,protocol:HTTP/2.0 (22-Nov-2022 05:39:34.099) (total time: 1372ms):
Trace[429548214]: ---"About to write a response" 1371ms (05:39:35.471)
Trace[429548214]: [1.37269766s] [1.37269766s] END
E1122 05:39:54.377083       1 available_controller.go:524] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.106.89.169:443/apis/metrics.k8s.io/v1beta1: Get "https://10.106.89.169:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.106.89.169:443: connect: connection refused
E1122 05:39:54.415563       1 available_controller.go:524] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.106.89.169:443/apis/metrics.k8s.io/v1beta1: Get "https://10.106.89.169:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.106.89.169:443: connect: connection refused
E1122 05:39:54.483622       1 available_controller.go:524] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.106.89.169:443/apis/metrics.k8s.io/v1beta1: Get "https://10.106.89.169:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.106.89.169:443: connect: connection refused
E1122 05:39:54.539009       1 available_controller.go:524] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.106.89.169:443/apis/metrics.k8s.io/v1beta1: Get "https://10.106.89.169:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.106.89.169:443: connect: connection refused
E1122 05:39:54.630789       1 available_controller.go:524] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.106.89.169:443/apis/metrics.k8s.io/v1beta1: Get "https://10.106.89.169:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.106.89.169:443: connect: connection refused
E1122 05:39:54.669133       1 available_controller.go:524] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.106.89.169:443/apis/metrics.k8s.io/v1beta1: Get "https://10.106.89.169:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.106.89.169:443: connect: connection refused
E1122 05:39:54.743765       1 available_controller.go:524] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.106.89.169:443/apis/metrics.k8s.io/v1beta1: Get "https://10.106.89.169:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.106.89.169:443: connect: connection refused
E1122 05:39:55.091203       1 available_controller.go:524] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.106.89.169:443/apis/metrics.k8s.io/v1beta1: Get "https://10.106.89.169:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.106.89.169:443: connect: connection refused
W1122 05:39:55.382370       1 handler_proxy.go:105] no RequestInfo found in the context
E1122 05:39:55.383484       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
W1122 05:39:55.383472       1 handler_proxy.go:105] no RequestInfo found in the context
E1122 05:39:55.384937       1 controller.go:116] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I1122 05:39:55.385623       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I1122 05:39:55.386158       1 controller.go:129] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I1122 05:40:55.440348       1 trace.go:205] Trace[1008937395]: "Create" url:/apis/authorization.k8s.io/v1/subjectaccessreviews,user-agent:kubelet/v1.25.3 (linux/amd64) kubernetes/434bfd8,audit-id:3d4e017e-21da-4c7d-9aec-fe441b45023d,client:192.168.64.2,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (22-Nov-2022 05:40:54.865) (total time: 574ms):
Trace[1008937395]: ---"About to store object in database" 159ms (05:40:55.040)
Trace[1008937395]: ---"Write to database call finished" len:457,err:<nil> 379ms (05:40:55.422)
Trace[1008937395]: [574.584557ms] [574.584557ms] END
{"level":"warn","ts":"2022-11-22T05:40:56.009Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000b30a80/127.0.0.1:2379","attempt":0,"error":"rpc error: code = Canceled desc = context canceled"}
I1122 05:40:56.045384       1 trace.go:205] Trace[409822014]: "GuaranteedUpdate etcd3" audit-id:c2ba7d67-dacb-4d0c-86e5-583dc59570be,key:/leases/kube-node-lease/minikube,type:*coordination.Lease (22-Nov-2022 05:40:54.375) (total time: 1635ms):
Trace[409822014]: ---"About to Encode" 264ms (05:40:54.639)
Trace[409822014]: ---"Txn call finished" err:context canceled 1369ms (05:40:56.010)
Trace[409822014]: [1.635074446s] [1.635074446s] END
E1122 05:40:56.058426       1 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 32.983µs, panicked: false, err: context canceled, panic-reason: <nil>
E1122 05:40:56.058989       1 writers.go:118] apiserver was unable to write a JSON response: http: Handler timeout
E1122 05:40:56.068970       1 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E1122 05:40:56.071030       1 writers.go:131] apiserver was unable to write a fallback JSON response: http: Handler timeout
I1122 05:40:56.081478       1 trace.go:205] Trace[1182902602]: "Get" url:/api/v1/namespaces/default,user-agent:kube-apiserver/v1.25.3 (linux/amd64) kubernetes/434bfd8,audit-id:d8b90dd5-df9f-475d-b673-b7d1634ccd24,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (22-Nov-2022 05:40:53.876) (total time: 2205ms):
Trace[1182902602]: ---"About to write a response" 2201ms (05:40:56.078)
Trace[1182902602]: [2.20513936s] [2.20513936s] END
I1122 05:40:56.101402       1 trace.go:205] Trace[161712999]: "Update" url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.25.3 (linux/amd64) kubernetes/434bfd8,audit-id:c2ba7d67-dacb-4d0c-86e5-583dc59570be,client:192.168.64.2,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (22-Nov-2022 05:40:54.142) (total time: 1940ms):
Trace[161712999]: ---"Conversion done" 130ms (05:40:54.331)
Trace[161712999]: ---"Write to database call finished" len:478,err:Timeout: request did not complete within requested timeout - context canceled 1674ms (05:40:56.008)
Trace[161712999]: [1.940322387s] [1.940322387s] END
E1122 05:40:56.167927       1 timeout.go:141] post-timeout activity - time-elapsed: 109.545581ms, PUT "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube" result: <nil>
I1122 05:40:58.213332       1 trace.go:205] Trace[237126543]: "Get" url:/api/v1/namespaces/default/services/kubernetes,user-agent:kube-apiserver/v1.25.3 (linux/amd64) kubernetes/434bfd8,audit-id:f4301948-2507-41a1-820d-2010e614feef,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (22-Nov-2022 05:40:56.952) (total time: 1252ms):
Trace[237126543]: ---"About to write a response" 1198ms (05:40:58.204)
Trace[237126543]: [1.252129422s] [1.252129422s] END
I1122 05:40:59.189423       1 trace.go:205] Trace[1421689603]: "Patch" url:/api/v1/namespaces/kube-system/events/coredns-565d847f94-dnz9v.1726f6d1c3d81fb3,user-agent:kubelet/v1.25.3 (linux/amd64) kubernetes/434bfd8,audit-id:6f31bd48-0f69-4f61-8c62-aa41870123c1,client:192.168.64.2,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (22-Nov-2022 05:40:58.436) (total time: 750ms):
Trace[1421689603]: ---"limitedReadBody done" len:201,err:<nil> 361ms (05:40:58.798)
Trace[1421689603]: ---"About to apply patch" 114ms (05:40:58.913)
Trace[1421689603]: ---"Object stored in database" 221ms (05:40:59.182)
Trace[1421689603]: [750.775812ms] [750.775812ms] END

* 
* ==> kube-apiserver [f96b5cbf6d2f] <==
* Trace[783568063]: ---"Object stored in database" 769ms (21:01:06.012)
Trace[783568063]: [1.691148523s] [1.691148523s] END
I1124 21:01:07.630598       1 trace.go:205] Trace[1515617200]: "Create" url:/apis/authorization.k8s.io/v1/subjectaccessreviews,user-agent:Go-http-client/2.0,audit-id:7b552776-a10b-42e7-9716-50ae8ef9127b,client:172.17.0.3,accept:application/json, */*,protocol:HTTP/2.0 (24-Nov-2022 21:01:06.748) (total time: 882ms):
Trace[1515617200]: ---"Conversion done" 210ms (21:01:06.957)
Trace[1515617200]: ---"Write to database call finished" len:386,err:<nil> 651ms (21:01:07.612)
Trace[1515617200]: [882.098698ms] [882.098698ms] END
I1124 21:01:07.683248       1 trace.go:205] Trace[849502442]: "GuaranteedUpdate etcd3" audit-id:3ab9c68e-2698-4d4f-aeb1-33bb55a23161,key:/configmaps/ingress-nginx/ingress-controller-leader,type:*core.ConfigMap (24-Nov-2022 21:01:05.414) (total time: 2268ms):
Trace[849502442]: ---"Txn call finished" err:<nil> 2226ms (21:01:07.682)
Trace[849502442]: [2.268578092s] [2.268578092s] END
I1124 21:01:07.684390       1 trace.go:205] Trace[1909044459]: "Update" url:/api/v1/namespaces/ingress-nginx/configmaps/ingress-controller-leader,user-agent:nginx-ingress-controller/v1.2.1 (linux/amd64) ingress-nginx/08848d69e0c83992c89da18e70ea708752f21d7a,audit-id:3ab9c68e-2698-4d4f-aeb1-33bb55a23161,client:172.17.0.8,accept:application/json, */*,protocol:HTTP/2.0 (24-Nov-2022 21:01:05.413) (total time: 2270ms):
Trace[1909044459]: ---"Write to database call finished" len:741,err:<nil> 2268ms (21:01:07.683)
Trace[1909044459]: [2.270703856s] [2.270703856s] END
I1124 21:01:07.784360       1 trace.go:205] Trace[1151428880]: "Get" url:/api/v1/namespaces/default/services/kubernetes,user-agent:kube-apiserver/v1.25.3 (linux/amd64) kubernetes/434bfd8,audit-id:f1859835-0757-479a-8b1b-3530f1611fc5,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (24-Nov-2022 21:01:05.501) (total time: 2282ms):
Trace[1151428880]: ---"About to write a response" 2275ms (21:01:07.776)
Trace[1151428880]: [2.282720248s] [2.282720248s] END
I1124 21:01:07.814922       1 trace.go:205] Trace[734295015]: "List(recursive=true) etcd3" audit-id:d4d2dacf-17b6-4930-a9f4-3716a341f053,key:/pods/ingress-nginx,resourceVersion:,resourceVersionMatch:,limit:0,continue: (24-Nov-2022 21:01:05.694) (total time: 2119ms):
Trace[734295015]: [2.119864373s] [2.119864373s] END
I1124 21:01:07.825803       1 trace.go:205] Trace[612602997]: "List" url:/api/v1/namespaces/ingress-nginx/pods,user-agent:nginx-ingress-controller/v1.2.1 (linux/amd64) ingress-nginx/08848d69e0c83992c89da18e70ea708752f21d7a,audit-id:d4d2dacf-17b6-4930-a9f4-3716a341f053,client:172.17.0.8,accept:application/json, */*,protocol:HTTP/2.0 (24-Nov-2022 21:01:05.694) (total time: 2130ms):
Trace[612602997]: ---"Listing from storage done" 2120ms (21:01:07.815)
Trace[612602997]: [2.13093408s] [2.13093408s] END
I1124 21:01:08.954569       1 trace.go:205] Trace[1200630395]: "GuaranteedUpdate etcd3" audit-id:35676d79-2e4d-4698-9ef9-cd48345c4a4a,key:/leases/kube-node-lease/minikube,type:*coordination.Lease (24-Nov-2022 21:01:07.866) (total time: 1088ms):
Trace[1200630395]: ---"Txn call finished" err:<nil> 1051ms (21:01:08.954)
Trace[1200630395]: [1.088058228s] [1.088058228s] END
I1124 21:01:08.956825       1 trace.go:205] Trace[2096835403]: "Update" url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.25.3 (linux/amd64) kubernetes/434bfd8,audit-id:35676d79-2e4d-4698-9ef9-cd48345c4a4a,client:192.168.64.2,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (24-Nov-2022 21:01:07.863) (total time: 1092ms):
Trace[2096835403]: ---"Write to database call finished" len:478,err:<nil> 1090ms (21:01:08.955)
Trace[2096835403]: [1.092702158s] [1.092702158s] END
I1124 21:01:08.981806       1 trace.go:205] Trace[728102023]: "Get" url:/api/v1/nodes/minikube,user-agent:nginx-ingress-controller/v1.2.1 (linux/amd64) ingress-nginx/08848d69e0c83992c89da18e70ea708752f21d7a,audit-id:5273018e-842c-4510-b67d-c49a6c14dbe3,client:172.17.0.8,accept:application/json, */*,protocol:HTTP/2.0 (24-Nov-2022 21:01:07.910) (total time: 1070ms):
Trace[728102023]: ---"About to write a response" 1057ms (21:01:08.968)
Trace[728102023]: [1.070681285s] [1.070681285s] END
I1124 21:01:09.061604       1 trace.go:205] Trace[1306414010]: "GuaranteedUpdate etcd3" audit-id:ed5e5c87-36cd-457c-b668-432f6498def7,key:/events/kube-system/kube-apiserver-minikube.172a458ec2f08859,type:*core.Event (24-Nov-2022 21:01:07.552) (total time: 1508ms):
Trace[1306414010]: ---"initial value restored" 1334ms (21:01:08.886)
Trace[1306414010]: ---"Txn call finished" err:<nil> 127ms (21:01:09.060)
Trace[1306414010]: [1.508941748s] [1.508941748s] END
I1124 21:01:09.080416       1 trace.go:205] Trace[771877427]: "Patch" url:/api/v1/namespaces/kube-system/events/kube-apiserver-minikube.172a458ec2f08859,user-agent:kubelet/v1.25.3 (linux/amd64) kubernetes/434bfd8,audit-id:ed5e5c87-36cd-457c-b668-432f6498def7,client:192.168.64.2,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (24-Nov-2022 21:01:07.541) (total time: 1528ms):
Trace[771877427]: ---"About to apply patch" 1335ms (21:01:08.887)
Trace[771877427]: ---"Object stored in database" 142ms (21:01:09.062)
Trace[771877427]: [1.528423083s] [1.528423083s] END
I1124 21:01:09.356104       1 trace.go:205] Trace[1183370921]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.64.2,type:*v1.Endpoints (24-Nov-2022 21:01:07.807) (total time: 1547ms):
Trace[1183370921]: ---"initial value restored" 482ms (21:01:08.290)
Trace[1183370921]: ---"Transaction prepared" 669ms (21:01:08.960)
Trace[1183370921]: ---"Txn call finished" err:<nil> 218ms (21:01:09.179)
Trace[1183370921]: ---"Transaction prepared" 55ms (21:01:09.235)
Trace[1183370921]: ---"Txn call finished" err:<nil> 120ms (21:01:09.355)
Trace[1183370921]: [1.547988257s] [1.547988257s] END
I1124 21:11:43.100747       1 trace.go:205] Trace[39225371]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.64.2,type:*v1.Endpoints (24-Nov-2022 21:11:42.473) (total time: 627ms):
Trace[39225371]: ---"Txn call finished" err:<nil> 561ms (21:11:43.095)
Trace[39225371]: [627.064387ms] [627.064387ms] END
I1124 21:27:55.272424       1 trace.go:205] Trace[534719271]: "GuaranteedUpdate etcd3" audit-id:9531bb2b-7d33-482f-bb91-acc173264a3d,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints (24-Nov-2022 21:27:54.531) (total time: 689ms):
Trace[534719271]: ---"About to Encode" 140ms (21:27:54.671)
Trace[534719271]: ---"Txn call finished" err:<nil> 545ms (21:27:55.220)
Trace[534719271]: [689.527174ms] [689.527174ms] END
I1124 21:27:55.345625       1 trace.go:205] Trace[1025249412]: "Update" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:9531bb2b-7d33-482f-bb91-acc173264a3d,client:192.168.64.2,accept:application/json, */*,protocol:HTTP/2.0 (24-Nov-2022 21:27:54.523) (total time: 821ms):
Trace[1025249412]: ---"Write to database call finished" len:1356,err:<nil> 811ms (21:27:55.342)
Trace[1025249412]: [821.475881ms] [821.475881ms] END
I1124 21:28:30.745304       1 trace.go:205] Trace[1445166460]: "DeltaFIFO Pop Process" ID:v1.certificates.k8s.io,Depth:13,Reason:slow event handlers blocking the queue (24-Nov-2022 21:28:30.601) (total time: 140ms):
Trace[1445166460]: [140.505181ms] [140.505181ms] END
I1124 21:31:01.616177       1 trace.go:205] Trace[1011336834]: "Get" url:/api/v1/namespaces/default/pods/system-design-auth-7848db97b5-2mdpm/log,user-agent:k9s/v0.0.0 (darwin/amd64) kubernetes/$Format,audit-id:f0a51349-8c78-4f3c-9141-b459b7a07be0,client:192.168.64.1,accept:application/json, */*,protocol:HTTP/2.0 (24-Nov-2022 05:28:50.072) (total time: 20456759ms):
Trace[1011336834]: ---"About to write a response" 330ms (05:28:50.404)
Trace[1011336834]: ---"Writing http response done" 20456427ms (21:31:01.615)
Trace[1011336834]: [5h40m56.759714949s] [5h40m56.759714949s] END

* 
* ==> kube-controller-manager [26c72c709165] <==
* W1120 19:44:56.549955       1 garbagecollector.go:754] failed to discover preferred resources: Unauthorized
I1121 00:06:37.962183       1 node_lifecycle_controller.go:1209] Controller detected that all Nodes are not-Ready. Entering master disruption mode.
I1121 00:06:47.967861       1 node_lifecycle_controller.go:1236] Controller detected that some Nodes are Ready. Exiting master disruption mode.
W1121 00:57:41.372703       1 garbagecollector.go:754] failed to discover preferred resources: Unauthorized
E1121 00:57:41.795111       1 resource_quota_controller.go:417] failed to discover resources: Unauthorized
W1121 02:32:06.124871       1 garbagecollector.go:754] failed to discover preferred resources: Unauthorized
E1121 02:32:06.364084       1 resource_quota_controller.go:417] failed to discover resources: Unauthorized
I1121 04:59:47.451281       1 node_lifecycle_controller.go:1209] Controller detected that all Nodes are not-Ready. Entering master disruption mode.
I1121 05:00:05.924786       1 node_lifecycle_controller.go:1236] Controller detected that some Nodes are Ready. Exiting master disruption mode.
I1121 05:00:06.039282       1 event.go:294] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-b74747df5-trpw4" fieldPath="" kind="Pod" apiVersion="" type="Normal" reason="TaintManagerEviction" message="Cancelling deletion of Pod kubernetes-dashboard/dashboard-metrics-scraper-b74747df5-trpw4"
I1121 05:00:06.040304       1 event.go:294] "Event occurred" object="kube-system/metrics-server-769cd898cd-tj8zr" fieldPath="" kind="Pod" apiVersion="" type="Normal" reason="TaintManagerEviction" message="Cancelling deletion of Pod kube-system/metrics-server-769cd898cd-tj8zr"
I1121 05:00:06.040391       1 event.go:294] "Event occurred" object="kube-system/storage-provisioner" fieldPath="" kind="Pod" apiVersion="" type="Normal" reason="TaintManagerEviction" message="Cancelling deletion of Pod kube-system/storage-provisioner"
I1121 05:00:06.040420       1 event.go:294] "Event occurred" object="kube-system/coredns-565d847f94-dnz9v" fieldPath="" kind="Pod" apiVersion="" type="Normal" reason="TaintManagerEviction" message="Cancelling deletion of Pod kube-system/coredns-565d847f94-dnz9v"
I1121 05:00:06.040569       1 event.go:294] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-57bbdc5f89-6jzqz" fieldPath="" kind="Pod" apiVersion="" type="Normal" reason="TaintManagerEviction" message="Cancelling deletion of Pod kubernetes-dashboard/kubernetes-dashboard-57bbdc5f89-6jzqz"
W1121 14:19:52.428174       1 garbagecollector.go:754] failed to discover preferred resources: Unauthorized
E1121 14:19:52.451827       1 resource_quota_controller.go:417] failed to discover resources: Unauthorized
E1121 15:21:34.950292       1 resource_quota_controller.go:417] failed to discover resources: Unauthorized
W1121 15:21:36.170595       1 garbagecollector.go:754] failed to discover preferred resources: Unauthorized
E1121 18:12:33.058014       1 resource_quota_controller.go:417] failed to discover resources: Unauthorized
W1121 18:12:34.405979       1 garbagecollector.go:754] failed to discover preferred resources: Unauthorized
E1121 19:14:05.581447       1 resource_quota_controller.go:417] failed to discover resources: Unauthorized
W1121 19:14:06.333069       1 garbagecollector.go:754] failed to discover preferred resources: Unauthorized
E1121 23:30:10.703684       1 resource_quota_controller.go:417] failed to discover resources: Get "https://192.168.64.2:8443/api": http2: client connection lost
W1121 23:30:10.770237       1 garbagecollector.go:754] failed to discover preferred resources: Get "https://192.168.64.2:8443/api": http2: client connection lost
http2: server: error reading preface from client 127.0.0.1:33268: read tcp 127.0.0.1:10257->127.0.0.1:33268: read: connection reset by peer
I1121 23:30:40.517961       1 event.go:294] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="NodeNotReady" message="Node minikube status is now: NodeNotReady"
I1121 23:30:42.871444       1 event.go:294] "Event occurred" object="kube-system/kube-scheduler-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
E1121 23:30:42.864417       1 resource_quota_controller.go:417] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1121 23:30:44.057786       1 garbagecollector.go:752] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
I1121 23:30:44.731812       1 event.go:294] "Event occurred" object="kube-system/kube-controller-manager-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1121 23:30:48.016834       1 event.go:294] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-b74747df5-trpw4" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1121 23:30:49.855678       1 event.go:294] "Event occurred" object="kube-system/coredns-565d847f94-dnz9v" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1121 23:30:50.760566       1 event.go:294] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-57bbdc5f89-6jzqz" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1121 23:30:51.592534       1 event.go:294] "Event occurred" object="kube-system/etcd-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1121 23:30:52.448501       1 event.go:294] "Event occurred" object="kube-system/kube-apiserver-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1121 23:30:53.548546       1 event.go:294] "Event occurred" object="kube-system/storage-provisioner" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1121 23:30:54.201384       1 node_lifecycle_controller.go:1209] Controller detected that all Nodes are not-Ready. Entering master disruption mode.
I1121 23:30:54.202579       1 event.go:294] "Event occurred" object="kube-system/kube-proxy-tvntg" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1121 23:30:59.203738       1 node_lifecycle_controller.go:1236] Controller detected that some Nodes are Ready. Exiting master disruption mode.
E1121 23:31:13.197144       1 resource_quota_controller.go:417] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1121 23:31:14.737811       1 garbagecollector.go:752] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
E1122 02:28:51.942432       1 resource_quota_controller.go:417] failed to discover resources: Unauthorized
W1122 02:28:52.933065       1 garbagecollector.go:754] failed to discover preferred resources: Unauthorized
W1122 05:15:55.865384       1 garbagecollector.go:754] failed to discover preferred resources: Get "https://192.168.64.2:8443/api": http2: client connection lost
E1122 05:15:55.895160       1 resource_quota_controller.go:417] failed to discover resources: Get "https://192.168.64.2:8443/api": http2: client connection lost
I1122 05:16:06.519854       1 node_lifecycle_controller.go:1209] Controller detected that all Nodes are not-Ready. Entering master disruption mode.
I1122 05:16:17.007612       1 node_lifecycle_controller.go:1236] Controller detected that some Nodes are Ready. Exiting master disruption mode.
I1122 05:16:17.048065       1 event.go:294] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-b74747df5-trpw4" fieldPath="" kind="Pod" apiVersion="" type="Normal" reason="TaintManagerEviction" message="Cancelling deletion of Pod kubernetes-dashboard/dashboard-metrics-scraper-b74747df5-trpw4"
I1122 05:16:17.050823       1 event.go:294] "Event occurred" object="kube-system/metrics-server-769cd898cd-tj8zr" fieldPath="" kind="Pod" apiVersion="" type="Normal" reason="TaintManagerEviction" message="Cancelling deletion of Pod kube-system/metrics-server-769cd898cd-tj8zr"
I1122 05:16:17.056319       1 event.go:294] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-57bbdc5f89-6jzqz" fieldPath="" kind="Pod" apiVersion="" type="Normal" reason="TaintManagerEviction" message="Cancelling deletion of Pod kubernetes-dashboard/kubernetes-dashboard-57bbdc5f89-6jzqz"
I1122 05:16:17.057169       1 event.go:294] "Event occurred" object="kube-system/storage-provisioner" fieldPath="" kind="Pod" apiVersion="" type="Normal" reason="TaintManagerEviction" message="Cancelling deletion of Pod kube-system/storage-provisioner"
I1122 05:16:17.057507       1 event.go:294] "Event occurred" object="kube-system/coredns-565d847f94-dnz9v" fieldPath="" kind="Pod" apiVersion="" type="Normal" reason="TaintManagerEviction" message="Cancelling deletion of Pod kube-system/coredns-565d847f94-dnz9v"
E1122 05:37:57.660559       1 resource_quota_controller.go:417] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1122 05:38:18.666149       1 endpointslice_controller.go:306] Error syncing endpoint slices for service "kube-system/metrics-server", retrying. Error: EndpointSlice informer cache is out of date
E1122 05:38:28.645742       1 resource_quota_controller.go:417] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1122 05:38:29.222564       1 garbagecollector.go:752] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
E1122 05:38:59.282597       1 resource_quota_controller.go:417] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1122 05:39:01.316650       1 garbagecollector.go:752] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
E1122 05:39:29.953165       1 resource_quota_controller.go:417] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1122 05:39:32.459963       1 garbagecollector.go:752] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]

* 
* ==> kube-controller-manager [e7d461b3fb9d] <==
* I1124 05:29:06.359691       1 event.go:294] "Event occurred" object="kube-system/etcd-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1124 05:29:06.798157       1 event.go:294] "Event occurred" object="kube-system/kube-apiserver-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1124 05:29:06.812388       1 node_lifecycle_controller.go:1209] Controller detected that all Nodes are not-Ready. Entering master disruption mode.
I1124 05:29:11.814049       1 node_lifecycle_controller.go:1236] Controller detected that some Nodes are Ready. Exiting master disruption mode.
E1124 05:29:19.211867       1 resource_quota_controller.go:417] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1124 05:29:19.931282       1 garbagecollector.go:752] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
E1124 15:52:06.413504       1 resource_quota_controller.go:417] failed to discover resources: Unauthorized
W1124 15:52:06.822595       1 garbagecollector.go:754] failed to discover preferred resources: Unauthorized
I1124 19:27:12.284332       1 event.go:294] "Event occurred" object="default/system-design-converter" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set system-design-converter-75875d8745 to 4"
I1124 19:27:12.615504       1 event.go:294] "Event occurred" object="default/system-design-converter-75875d8745" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: system-design-converter-75875d8745-jbtjk"
I1124 19:27:12.699837       1 event.go:294] "Event occurred" object="default/system-design-converter-75875d8745" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: system-design-converter-75875d8745-pz5f5"
I1124 19:27:12.820065       1 event.go:294] "Event occurred" object="default/system-design-converter-75875d8745" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: system-design-converter-75875d8745-btg98"
I1124 19:27:12.960738       1 event.go:294] "Event occurred" object="default/system-design-converter-75875d8745" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: system-design-converter-75875d8745-vx66w"
I1124 19:40:23.615928       1 event.go:294] "Event occurred" object="default/system-design-converter" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set system-design-converter-688d4c979c to 4"
I1124 19:40:23.847143       1 event.go:294] "Event occurred" object="default/system-design-converter-688d4c979c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: system-design-converter-688d4c979c-wgqrm"
I1124 19:40:23.963518       1 event.go:294] "Event occurred" object="default/system-design-converter-688d4c979c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: system-design-converter-688d4c979c-wj2cq"
I1124 19:40:24.001826       1 event.go:294] "Event occurred" object="default/system-design-converter-688d4c979c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: system-design-converter-688d4c979c-2jnm7"
I1124 19:40:24.134936       1 event.go:294] "Event occurred" object="default/system-design-converter-688d4c979c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: system-design-converter-688d4c979c-zrlxw"
E1124 20:00:02.519437       1 resource_quota_controller.go:417] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
I1124 20:00:09.611276       1 event.go:294] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="NodeNotReady" message="Node minikube status is now: NodeNotReady"
I1124 20:00:12.116487       1 event.go:294] "Event occurred" object="kube-system/kube-proxy-tvntg" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1124 20:00:12.612263       1 event.go:294] "Event occurred" object="kube-system/kube-controller-manager-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1124 20:00:13.042073       1 event.go:294] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-b74747df5-trpw4" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1124 20:00:13.434679       1 event.go:294] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-57bbdc5f89-6jzqz" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1124 20:00:15.001204       1 event.go:294] "Event occurred" object="kube-system/coredns-565d847f94-dnz9v" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1124 20:00:16.626007       1 event.go:294] "Event occurred" object="kube-system/kube-scheduler-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1124 20:00:17.070122       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-5959f988fd-4vs7d" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1124 20:00:18.011342       1 event.go:294] "Event occurred" object="default/system-design-gateway-578d8dcc55-2ww66" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1124 20:00:18.802820       1 event.go:294] "Event occurred" object="kube-system/metrics-server-769cd898cd-tj8zr" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1124 20:00:19.685574       1 event.go:294] "Event occurred" object="kube-system/storage-provisioner" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1124 20:00:20.249587       1 event.go:294] "Event occurred" object="kube-system/etcd-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1124 20:00:20.602302       1 event.go:294] "Event occurred" object="kube-system/kube-apiserver-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1124 20:00:20.961382       1 event.go:294] "Event occurred" object="default/system-design-auth-7848db97b5-4d7q7" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1124 20:00:21.599659       1 event.go:294] "Event occurred" object="default/system-design-rabbitmq-0" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1124 20:00:22.487471       1 event.go:294] "Event occurred" object="default/system-design-auth-7848db97b5-2mdpm" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1124 20:00:23.706539       1 event.go:294] "Event occurred" object="default/system-design-gateway-578d8dcc55-l7wqz" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1124 20:00:23.706748       1 node_lifecycle_controller.go:1209] Controller detected that all Nodes are not-Ready. Entering master disruption mode.
E1124 20:00:32.924230       1 resource_quota_controller.go:417] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
I1124 20:00:33.711462       1 node_lifecycle_controller.go:1236] Controller detected that some Nodes are Ready. Exiting master disruption mode.
W1124 20:00:36.623163       1 garbagecollector.go:752] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
I1124 20:21:17.115472       1 event.go:294] "Event occurred" object="default/system-design-converter" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set system-design-converter-688d4c979c to 4"
I1124 20:21:17.283296       1 event.go:294] "Event occurred" object="default/system-design-converter-688d4c979c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: system-design-converter-688d4c979c-p5n97"
I1124 20:21:17.391436       1 event.go:294] "Event occurred" object="default/system-design-converter-688d4c979c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: system-design-converter-688d4c979c-tn4dq"
I1124 20:21:17.395109       1 event.go:294] "Event occurred" object="default/system-design-converter-688d4c979c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: system-design-converter-688d4c979c-c7nz8"
I1124 20:21:17.497033       1 event.go:294] "Event occurred" object="default/system-design-converter-688d4c979c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: system-design-converter-688d4c979c-t94hh"
I1124 20:48:14.115076       1 event.go:294] "Event occurred" object="default/system-design-converter" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set system-design-converter-688d4c979c to 4"
I1124 20:48:14.207602       1 event.go:294] "Event occurred" object="default/system-design-converter-688d4c979c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: system-design-converter-688d4c979c-sp8fb"
I1124 20:48:14.323469       1 event.go:294] "Event occurred" object="default/system-design-converter-688d4c979c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: system-design-converter-688d4c979c-l2f2m"
I1124 20:48:14.376013       1 event.go:294] "Event occurred" object="default/system-design-converter-688d4c979c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: system-design-converter-688d4c979c-7mjcp"
I1124 20:48:14.546005       1 event.go:294] "Event occurred" object="default/system-design-converter-688d4c979c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: system-design-converter-688d4c979c-cm8jw"
I1124 21:18:35.269109       1 event.go:294] "Event occurred" object="default/system-design-converter" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set system-design-converter-688d4c979c to 4"
I1124 21:18:35.451529       1 event.go:294] "Event occurred" object="default/system-design-converter-688d4c979c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: system-design-converter-688d4c979c-fqzll"
I1124 21:18:35.553394       1 event.go:294] "Event occurred" object="default/system-design-converter-688d4c979c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: system-design-converter-688d4c979c-rh4fs"
I1124 21:18:35.561350       1 event.go:294] "Event occurred" object="default/system-design-converter-688d4c979c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: system-design-converter-688d4c979c-62ppr"
I1124 21:18:35.752669       1 event.go:294] "Event occurred" object="default/system-design-converter-688d4c979c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: system-design-converter-688d4c979c-7skbm"
I1124 21:22:00.292254       1 event.go:294] "Event occurred" object="default/system-design-converter" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set system-design-converter-688d4c979c to 4"
I1124 21:22:00.485802       1 event.go:294] "Event occurred" object="default/system-design-converter-688d4c979c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: system-design-converter-688d4c979c-vnnb9"
I1124 21:22:00.576098       1 event.go:294] "Event occurred" object="default/system-design-converter-688d4c979c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: system-design-converter-688d4c979c-9gqj9"
I1124 21:22:00.644379       1 event.go:294] "Event occurred" object="default/system-design-converter-688d4c979c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: system-design-converter-688d4c979c-6rd58"
I1124 21:22:00.764929       1 event.go:294] "Event occurred" object="default/system-design-converter-688d4c979c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: system-design-converter-688d4c979c-2qk9c"

* 
* ==> kube-proxy [0bb512b86d29] <==
* W1118 20:48:05.916675       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1118 20:48:06.081603       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1118 20:48:06.481851       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I1119 03:21:32.540896       1 trace.go:205] Trace[1996787735]: "iptables ChainExists" (19-Nov-2022 03:21:10.193) (total time: 22343ms):
Trace[1996787735]: [22.343708738s] [22.343708738s] END
I1119 04:20:10.069274       1 trace.go:205] Trace[510140531]: "iptables ChainExists" (19-Nov-2022 04:20:07.807) (total time: 2261ms):
Trace[510140531]: [2.261936099s] [2.261936099s] END
I1119 08:59:18.270781       1 trace.go:205] Trace[208594914]: "iptables ChainExists" (19-Nov-2022 08:59:09.040) (total time: 9220ms):
Trace[208594914]: [9.220581004s] [9.220581004s] END
I1119 09:00:44.213615       1 trace.go:205] Trace[507549975]: "iptables ChainExists" (19-Nov-2022 09:00:41.849) (total time: 2299ms):
Trace[507549975]: [2.299299675s] [2.299299675s] END
I1119 15:01:56.044296       1 trace.go:205] Trace[651148272]: "iptables ChainExists" (19-Nov-2022 15:01:51.415) (total time: 4628ms):
Trace[651148272]: [4.628330071s] [4.628330071s] END
I1119 15:08:24.312791       1 trace.go:205] Trace[757941460]: "iptables ChainExists" (19-Nov-2022 15:08:22.127) (total time: 2148ms):
Trace[757941460]: [2.148501538s] [2.148501538s] END
I1119 15:13:24.874661       1 trace.go:205] Trace[391298352]: "iptables ChainExists" (19-Nov-2022 15:13:21.248) (total time: 3625ms):
Trace[391298352]: [3.625168432s] [3.625168432s] END
I1119 15:49:57.499422       1 trace.go:205] Trace[635788252]: "iptables ChainExists" (19-Nov-2022 15:49:54.579) (total time: 2863ms):
Trace[635788252]: [2.863493288s] [2.863493288s] END
I1119 16:31:39.862074       1 trace.go:205] Trace[119938156]: "iptables ChainExists" (19-Nov-2022 16:31:23.297) (total time: 16499ms):
Trace[119938156]: [16.499990039s] [16.499990039s] END
I1119 18:05:16.510433       1 trace.go:205] Trace[1315842168]: "iptables ChainExists" (19-Nov-2022 18:04:55.231) (total time: 15500ms):
Trace[1315842168]: [15.500688181s] [15.500688181s] END
I1119 18:05:23.880846       1 trace.go:205] Trace[1252947444]: "iptables ChainExists" (19-Nov-2022 18:05:21.175) (total time: 2704ms):
Trace[1252947444]: [2.704772145s] [2.704772145s] END
I1119 18:11:06.146725       1 trace.go:205] Trace[2121275682]: "iptables ChainExists" (19-Nov-2022 18:11:01.013) (total time: 5132ms):
Trace[2121275682]: [5.132542846s] [5.132542846s] END
I1119 18:12:51.592109       1 trace.go:205] Trace[2055773527]: "iptables ChainExists" (19-Nov-2022 18:12:48.908) (total time: 2683ms):
Trace[2055773527]: [2.683235373s] [2.683235373s] END
I1119 18:13:49.877669       1 trace.go:205] Trace[122040235]: "iptables ChainExists" (19-Nov-2022 18:13:46.835) (total time: 3042ms):
Trace[122040235]: [3.042466515s] [3.042466515s] END
I1120 00:27:57.112996       1 trace.go:205] Trace[381358250]: "iptables ChainExists" (19-Nov-2022 18:14:56.316) (total time: 28463ms):
Trace[381358250]: [28.463153265s] [28.463153265s] END
I1120 01:09:50.408655       1 trace.go:205] Trace[1309910617]: "iptables ChainExists" (20-Nov-2022 01:09:45.968) (total time: 4440ms):
Trace[1309910617]: [4.44010047s] [4.44010047s] END
I1120 01:10:51.778579       1 trace.go:205] Trace[1440508446]: "iptables ChainExists" (20-Nov-2022 01:10:46.073) (total time: 5704ms):
Trace[1440508446]: [5.704828883s] [5.704828883s] END
I1120 01:18:51.759561       1 trace.go:205] Trace[701744319]: "iptables ChainExists" (20-Nov-2022 01:18:49.103) (total time: 2620ms):
Trace[701744319]: [2.620641623s] [2.620641623s] END
I1120 14:32:17.384248       1 trace.go:205] Trace[1655306623]: "iptables ChainExists" (20-Nov-2022 14:32:08.134) (total time: 9246ms):
Trace[1655306623]: [9.246784395s] [9.246784395s] END
I1121 00:06:33.614736       1 trace.go:205] Trace[2110390197]: "iptables ChainExists" (21-Nov-2022 00:06:25.897) (total time: 7687ms):
Trace[2110390197]: [7.687925214s] [7.687925214s] END
W1121 23:31:03.139826       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I1121 23:31:03.287289       1 trace.go:205] Trace[198338450]: "iptables ChainExists" (21-Nov-2022 23:29:32.414) (total time: 90055ms):
Trace[198338450]: [1m30.055237426s] [1m30.055237426s] END
W1121 23:31:03.287364       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1121 23:31:03.287419       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I1122 05:16:00.784958       1 trace.go:205] Trace[1304763530]: "iptables ChainExists" (22-Nov-2022 05:15:56.921) (total time: 3509ms):
Trace[1304763530]: [3.509637715s] [3.509637715s] END
I1122 05:17:53.621798       1 trace.go:205] Trace[1933817919]: "iptables ChainExists" (22-Nov-2022 05:17:49.855) (total time: 3629ms):
Trace[1933817919]: [3.62951643s] [3.62951643s] END
I1122 05:37:56.990485       1 trace.go:205] Trace[1155358463]: "iptables ChainExists" (22-Nov-2022 05:18:03.352) (total time: 8630ms):
Trace[1155358463]: [8.63086003s] [8.63086003s] END
I1122 05:38:30.650982       1 trace.go:205] Trace[691816887]: "iptables ChainExists" (22-Nov-2022 05:38:27.382) (total time: 3266ms):
Trace[691816887]: [3.266470347s] [3.266470347s] END
I1122 05:39:22.950840       1 trace.go:205] Trace[301689882]: "iptables ChainExists" (22-Nov-2022 05:39:17.549) (total time: 5391ms):
Trace[301689882]: [5.391333032s] [5.391333032s] END
I1122 05:40:59.458051       1 trace.go:205] Trace[2014466131]: "iptables ChainExists" (22-Nov-2022 05:40:52.410) (total time: 7046ms):
Trace[2014466131]: [7.046983675s] [7.046983675s] END

* 
* ==> kube-proxy [7f71ad8cd10d] <==
* I1123 03:37:32.727150       1 server_others.go:206] "Using iptables Proxier"
I1123 03:37:32.728897       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I1123 03:37:32.731182       1 server.go:661] "Version info" version="v1.25.3"
I1123 03:37:32.731198       1 server.go:663] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1123 03:37:32.734301       1 config.go:226] "Starting endpoint slice config controller"
I1123 03:37:32.734360       1 shared_informer.go:255] Waiting for caches to sync for endpoint slice config
I1123 03:37:32.734519       1 config.go:317] "Starting service config controller"
I1123 03:37:32.734528       1 shared_informer.go:255] Waiting for caches to sync for service config
I1123 03:37:32.746854       1 config.go:444] "Starting node config controller"
I1123 03:37:32.746930       1 shared_informer.go:255] Waiting for caches to sync for node config
I1123 03:37:32.834628       1 shared_informer.go:262] Caches are synced for service config
I1123 03:37:32.834709       1 shared_informer.go:262] Caches are synced for endpoint slice config
I1123 03:37:32.849032       1 shared_informer.go:262] Caches are synced for node config
I1123 17:23:54.502697       1 trace.go:205] Trace[957855164]: "iptables ChainExists" (23-Nov-2022 17:23:28.689) (total time: 25648ms):
Trace[957855164]: [25.648582999s] [25.648582999s] END
W1123 21:04:32.003139       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1123 21:04:32.008822       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1123 21:04:32.009065       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I1123 21:04:32.104278       1 trace.go:205] Trace[1871215118]: "iptables ChainExists" (23-Nov-2022 21:03:59.328) (total time: 32775ms):
Trace[1871215118]: [32.775557323s] [32.775557323s] END
W1123 21:10:14.073804       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1123 21:10:14.909750       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1123 21:10:15.034652       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I1123 21:10:15.075162       1 trace.go:205] Trace[10485574]: "iptables ChainExists" (23-Nov-2022 21:04:55.325) (total time: 48925ms):
Trace[10485574]: [48.92521028s] [48.92521028s] END
W1123 21:19:15.184481       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1123 21:19:15.229429       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1123 21:19:15.284860       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I1123 21:19:15.451021       1 trace.go:205] Trace[880055232]: "iptables ChainExists" (23-Nov-2022 21:18:50.331) (total time: 25119ms):
Trace[880055232]: [25.119501467s] [25.119501467s] END
I1123 21:25:07.645378       1 trace.go:205] Trace[127759890]: "iptables ChainExists" (23-Nov-2022 21:24:32.188) (total time: 32884ms):
Trace[127759890]: [32.884943217s] [32.884943217s] END
I1123 21:28:23.020168       1 trace.go:205] Trace[1065573596]: "iptables ChainExists" (23-Nov-2022 21:28:03.065) (total time: 7536ms):
Trace[1065573596]: [7.53672953s] [7.53672953s] END
I1123 21:41:30.713966       1 trace.go:205] Trace[1995873212]: "iptables ChainExists" (23-Nov-2022 21:41:11.628) (total time: 19084ms):
Trace[1995873212]: [19.084462696s] [19.084462696s] END
I1123 21:47:19.268833       1 trace.go:205] Trace[832748344]: "iptables ChainExists" (23-Nov-2022 21:46:44.737) (total time: 21063ms):
Trace[832748344]: [21.063678695s] [21.063678695s] END
W1123 21:50:37.391082       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1123 21:50:39.913281       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I1123 21:50:40.460540       1 trace.go:205] Trace[1988716672]: "iptables ChainExists" (23-Nov-2022 21:50:27.638) (total time: 7624ms):
Trace[1988716672]: [7.624918094s] [7.624918094s] END
I1123 22:01:24.023474       1 trace.go:205] Trace[195919386]: "iptables ChainExists" (23-Nov-2022 22:00:54.173) (total time: 29849ms):
Trace[195919386]: [29.849705153s] [29.849705153s] END
I1123 22:09:34.526739       1 trace.go:205] Trace[481013714]: "iptables ChainExists" (23-Nov-2022 22:04:17.078) (total time: 6774ms):
Trace[481013714]: [6.774636488s] [6.774636488s] END
W1123 22:21:03.319708       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1123 22:21:08.506981       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1123 22:21:08.507956       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I1123 22:21:09.496383       1 trace.go:205] Trace[1457288633]: "iptables ChainExists" (23-Nov-2022 22:20:34.098) (total time: 35396ms):
Trace[1457288633]: [35.396194573s] [35.396194573s] END
W1123 22:34:13.489278       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1123 22:34:15.007454       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1123 22:34:16.893715       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I1123 22:34:17.267530       1 trace.go:205] Trace[1075510750]: "iptables ChainExists" (23-Nov-2022 22:33:54.580) (total time: 22686ms):
Trace[1075510750]: [22.686722764s] [22.686722764s] END
I1124 05:29:09.916871       1 trace.go:205] Trace[155807292]: "iptables ChainExists" (24-Nov-2022 05:29:07.071) (total time: 2830ms):
Trace[155807292]: [2.83052038s] [2.83052038s] END
I1124 20:00:28.750539       1 trace.go:205] Trace[924038952]: "iptables ChainExists" (24-Nov-2022 20:00:12.872) (total time: 5283ms):
Trace[924038952]: [5.283882988s] [5.283882988s] END

* 
* ==> kube-scheduler [799f3e27e3a5] <==
* I1123 03:37:12.963180       1 serving.go:348] Generated self-signed cert in-memory
W1123 03:37:23.197282       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1123 03:37:23.197406       1 authentication.go:346] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1123 03:37:23.197428       1 authentication.go:347] Continuing without authentication configuration. This may treat all requests as anonymous.
W1123 03:37:23.197440       1 authentication.go:348] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1123 03:37:23.388289       1 server.go:148] "Starting Kubernetes Scheduler" version="v1.25.3"
I1123 03:37:23.388536       1 server.go:150] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1123 03:37:23.396595       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I1123 03:37:23.404540       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1123 03:37:23.405381       1 shared_informer.go:255] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1123 03:37:23.406128       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1123 03:37:23.507318       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
W1123 08:17:56.868404       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.StorageClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1123 08:17:56.893426       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.CSIStorageCapacity ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1123 08:17:56.927455       1 reflector.go:347] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1123 08:17:56.927798       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1123 08:17:57.095530       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.ReplicaSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1123 08:17:57.107238       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.CSINode ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1123 08:17:57.125629       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.PersistentVolume ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1123 08:17:57.141602       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.ReplicationController ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1123 08:17:57.142440       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1123 08:17:57.157449       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.PersistentVolumeClaim ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1123 08:17:57.165832       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.StatefulSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1123 08:17:57.177337       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.PodDisruptionBudget ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1123 08:17:57.177794       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1123 08:17:57.107757       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.CSIDriver ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1123 08:17:57.194351       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I1123 18:00:42.105072       1 trace.go:205] Trace[738780173]: "Scheduling" namespace:default,name:system-design-auth-7848db97b5-4d7q7 (23-Nov-2022 18:00:41.972) (total time: 119ms):
Trace[738780173]: ---"Computing predicates done" 106ms (18:00:42.088)
Trace[738780173]: [119.008019ms] [119.008019ms] END
W1123 21:04:18.387927       1 reflector.go:347] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1123 21:41:22.112088       1 reflector.go:347] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1123 21:50:29.277649       1 reflector.go:347] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E1124 04:25:46.483407       1 event_broadcaster.go:253] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"system-design-rabbitmq-0.172a6ab6958bad00", GenerateName:"", Namespace:"default", SelfLink:"", UID:"4300fbbb-43ab-4af1-a05c-281cbf0d24e6", ResourceVersion:"292288", Generation:0, CreationTimestamp:time.Date(2022, time.November, 24, 4, 25, 44, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"kube-scheduler", Operation:"Update", APIVersion:"events.k8s.io/v1", Time:time.Date(2022, time.November, 24, 4, 25, 44, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0009a1128), Subresource:""}}}, EventTime:time.Date(2022, time.November, 24, 4, 25, 44, 319716301, time.Local), Series:(*v1.EventSeries)(0xc0000998c0), ReportingController:"default-scheduler", ReportingInstance:"default-scheduler-minikube", Action:"Scheduling", Reason:"FailedScheduling", Regarding:v1.ObjectReference{Kind:"Pod", Namespace:"default", Name:"system-design-rabbitmq-0", UID:"031e9159-ca10-4491-9c0f-c4ca5487b021", APIVersion:"v1", ResourceVersion:"292166", FieldPath:""}, Related:(*v1.ObjectReference)(nil), Note:"0/1 nodes are available: 1 persistentvolumeclaim \"rabbitmq-pvc\" not found. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.", Type:"Warning", DeprecatedSource:v1.EventSource{Component:"", Host:""}, DeprecatedFirstTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeprecatedLastTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeprecatedCount:0}': 'Event "system-design-rabbitmq-0.172a6ab6958bad00" is invalid: series.count: Invalid value: "": should be at least 2' (will not retry!)
W1124 05:27:33.712609       1 reflector.go:347] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I1124 05:28:37.642699       1 trace.go:205] Trace[1475619183]: "Reflector ListAndWatch" name:pkg/server/dynamiccertificates/configmap_cafile_content.go:206 (24-Nov-2022 05:27:35.239) (total time: 62005ms):
Trace[1475619183]: ---"Objects listed" error:<nil> 61987ms (05:28:37.227)
Trace[1475619183]: [1m2.005778631s] [1m2.005778631s] END

* 
* ==> kube-scheduler [eddc3882ef12] <==
* Trace[1726969655]: [10.668971745s] [10.668971745s] END
I1114 06:21:12.363186       1 trace.go:205] Trace[738036903]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (14-Nov-2022 06:21:02.120) (total time: 10242ms):
Trace[738036903]: ---"Objects listed" error:<nil> 10018ms (06:21:12.138)
Trace[738036903]: [10.242203136s] [10.242203136s] END
I1114 06:21:12.668349       1 trace.go:205] Trace[1765835601]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (14-Nov-2022 06:21:01.940) (total time: 10727ms):
Trace[1765835601]: ---"Objects listed" error:<nil> 10727ms (06:21:12.667)
Trace[1765835601]: [10.727903665s] [10.727903665s] END
I1114 06:21:13.661389       1 trace.go:205] Trace[129136128]: "Reflector ListAndWatch" name:pkg/server/dynamiccertificates/configmap_cafile_content.go:206 (14-Nov-2022 06:20:55.756) (total time: 17904ms):
Trace[129136128]: ---"Objects listed" error:<nil> 17904ms (06:21:13.660)
Trace[129136128]: [17.904397623s] [17.904397623s] END
I1114 06:21:13.748620       1 trace.go:205] Trace[236906434]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (14-Nov-2022 06:21:02.082) (total time: 11665ms):
Trace[236906434]: ---"Objects listed" error:<nil> 11665ms (06:21:13.748)
Trace[236906434]: [11.66584386s] [11.66584386s] END
I1114 06:21:13.786751       1 trace.go:205] Trace[342936726]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (14-Nov-2022 06:21:02.366) (total time: 11420ms):
Trace[342936726]: ---"Objects listed" error:<nil> 11404ms (06:21:13.770)
Trace[342936726]: [11.420493396s] [11.420493396s] END
I1114 06:21:13.907633       1 trace.go:205] Trace[684644833]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (14-Nov-2022 06:21:02.968) (total time: 10938ms):
Trace[684644833]: ---"Objects listed" error:<nil> 10937ms (06:21:13.907)
Trace[684644833]: [10.938214257s] [10.938214257s] END
I1114 06:21:13.911877       1 trace.go:205] Trace[1170881818]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (14-Nov-2022 06:21:02.152) (total time: 11758ms):
Trace[1170881818]: ---"Objects listed" error:<nil> 11748ms (06:21:13.901)
Trace[1170881818]: [11.758950184s] [11.758950184s] END
I1114 06:21:13.927508       1 trace.go:205] Trace[316620557]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (14-Nov-2022 06:21:02.720) (total time: 11206ms):
Trace[316620557]: ---"Objects listed" error:<nil> 11192ms (06:21:13.913)
Trace[316620557]: [11.206466864s] [11.206466864s] END
I1114 06:21:13.983085       1 trace.go:205] Trace[1396205259]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (14-Nov-2022 06:21:01.941) (total time: 11996ms):
Trace[1396205259]: ---"Objects listed" error:<nil> 11996ms (06:21:13.925)
Trace[1396205259]: [11.996881958s] [11.996881958s] END
I1114 06:21:14.003800       1 trace.go:205] Trace[1824421914]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (14-Nov-2022 06:21:01.808) (total time: 12188ms):
Trace[1824421914]: ---"Objects listed" error:<nil> 12188ms (06:21:13.997)
Trace[1824421914]: [12.188889607s] [12.188889607s] END
W1116 01:52:47.194710       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.CSIStorageCapacity ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1116 01:52:47.345338       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.ReplicaSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1116 01:52:47.214108       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.PersistentVolumeClaim ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1116 01:52:47.441299       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.StorageClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1116 01:52:47.412570       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1116 01:52:47.256976       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.ReplicationController ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1116 01:52:47.290127       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1116 01:52:47.290527       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.CSINode ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1116 01:52:47.370411       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.PersistentVolume ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1116 01:52:47.385809       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.StatefulSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1116 01:52:47.392507       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.PodDisruptionBudget ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1116 01:52:47.393486       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1116 01:52:47.393693       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1116 01:52:47.153680       1 reflector.go:347] vendor/k8s.io/client-go/informers/factory.go:134: watch of *v1.CSIDriver ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1116 01:52:47.431163       1 reflector.go:347] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
http2: server: error reading preface from client 127.0.0.1:45936: read tcp 127.0.0.1:10259->127.0.0.1:45936: read: connection reset by peer
W1118 01:01:33.215634       1 reflector.go:347] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1118 11:33:09.481930       1 reflector.go:347] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1118 12:31:27.194502       1 reflector.go:347] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1118 17:54:19.687590       1 reflector.go:347] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1119 08:59:08.449635       1 reflector.go:347] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1119 18:12:24.948215       1 reflector.go:347] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I1119 18:12:48.837202       1 trace.go:205] Trace[937853651]: "Reflector ListAndWatch" name:pkg/server/dynamiccertificates/configmap_cafile_content.go:206 (19-Nov-2022 18:12:27.884) (total time: 20934ms):
Trace[937853651]: ---"Objects listed" error:<nil> 20927ms (18:12:48.811)
Trace[937853651]: [20.934449505s] [20.934449505s] END
W1119 18:13:37.137571       1 reflector.go:347] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
http2: server: error reading preface from client 127.0.0.1:45036: read tcp 127.0.0.1:10259->127.0.0.1:45036: read: connection reset by peer
W1121 00:06:25.507973       1 reflector.go:347] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W1122 05:15:54.273235       1 reflector.go:347] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding

* 
* ==> kubelet <==
* -- Journal begins at Thu 2022-11-24 02:37:43 UTC, ends at Thu 2022-11-24 21:32:12 UTC. --
Nov 24 21:22:00 minikube kubelet[1307]: I1124 21:22:00.672469    1307 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-jrtrn\" (UniqueName: \"kubernetes.io/projected/4a29ccd6-c4dd-42e8-809e-082ac3f439cd-kube-api-access-jrtrn\") pod \"system-design-converter-688d4c979c-vnnb9\" (UID: \"4a29ccd6-c4dd-42e8-809e-082ac3f439cd\") " pod="default/system-design-converter-688d4c979c-vnnb9"
Nov 24 21:22:00 minikube kubelet[1307]: I1124 21:22:00.716967    1307 topology_manager.go:205] "Topology Admit Handler"
Nov 24 21:22:00 minikube kubelet[1307]: I1124 21:22:00.811990    1307 topology_manager.go:205] "Topology Admit Handler"
Nov 24 21:22:00 minikube kubelet[1307]: I1124 21:22:00.918470    1307 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-c9pms\" (UniqueName: \"kubernetes.io/projected/59ae44fd-e348-4a99-bcd8-fb4fe16453fe-kube-api-access-c9pms\") pod \"system-design-converter-688d4c979c-9gqj9\" (UID: \"59ae44fd-e348-4a99-bcd8-fb4fe16453fe\") " pod="default/system-design-converter-688d4c979c-9gqj9"
Nov 24 21:22:00 minikube kubelet[1307]: I1124 21:22:00.960903    1307 topology_manager.go:205] "Topology Admit Handler"
Nov 24 21:22:01 minikube kubelet[1307]: I1124 21:22:01.024166    1307 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-bhzhp\" (UniqueName: \"kubernetes.io/projected/4486fb98-98a1-4213-a4a2-48b7431c87b0-kube-api-access-bhzhp\") pod \"system-design-converter-688d4c979c-6rd58\" (UID: \"4486fb98-98a1-4213-a4a2-48b7431c87b0\") " pod="default/system-design-converter-688d4c979c-6rd58"
Nov 24 21:22:01 minikube kubelet[1307]: I1124 21:22:01.024841    1307 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-bw6fn\" (UniqueName: \"kubernetes.io/projected/013bf399-ea93-4f04-a0af-b10ed7fabcd9-kube-api-access-bw6fn\") pod \"system-design-converter-688d4c979c-2qk9c\" (UID: \"013bf399-ea93-4f04-a0af-b10ed7fabcd9\") " pod="default/system-design-converter-688d4c979c-2qk9c"
Nov 24 21:22:08 minikube kubelet[1307]: E1124 21:22:08.906266    1307 remote_image.go:222] "PullImage from image service failed" err="rpc error: code = Unknown desc = context deadline exceeded" image="minh123456/system-design-converter:latest"
Nov 24 21:22:08 minikube kubelet[1307]: E1124 21:22:08.906419    1307 kuberuntime_image.go:51] "Failed to pull image" err="rpc error: code = Unknown desc = context deadline exceeded" image="minh123456/system-design-converter:latest"
Nov 24 21:22:08 minikube kubelet[1307]: E1124 21:22:08.907244    1307 kuberuntime_manager.go:862] container &Container{Name:system-design-converter,Image:minh123456/system-design-converter,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qpzqv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:converter-configmap,},Optional:nil,},SecretRef:nil,},EnvFromSource{Prefix:,ConfigMapRef:nil,SecretRef:&SecretEnvSource{LocalObjectReference:LocalObjectReference{Name:converter-secret,},Optional:nil,},},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod system-design-converter-688d4c979c-sp8fb_default(bd80c4bd-9844-4644-b3e2-1e219a220987): ErrImagePull: rpc error: code = Unknown desc = context deadline exceeded
Nov 24 21:22:08 minikube kubelet[1307]: E1124 21:22:08.907404    1307 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"system-design-converter\" with ErrImagePull: \"rpc error: code = Unknown desc = context deadline exceeded\"" pod="default/system-design-converter-688d4c979c-sp8fb" podUID=bd80c4bd-9844-4644-b3e2-1e219a220987
Nov 24 21:22:12 minikube kubelet[1307]: I1124 21:22:12.638821    1307 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="a456e4ecf649dd2336c0f8c897ba481e681bf468e2a45c6df856db3431d09cc9"
Nov 24 21:22:13 minikube kubelet[1307]: I1124 21:22:13.526210    1307 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="eb1b5e985be350b2dcdbd7aa933273b89e5f82031051a9399f3aa5d3de346430"
Nov 24 21:22:13 minikube kubelet[1307]: I1124 21:22:13.582614    1307 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="ee8258d15acc3dbc3f67a8ac9b9543b6a44683e2859f17e79b8e601ab9a72374"
Nov 24 21:22:13 minikube kubelet[1307]: I1124 21:22:13.960330    1307 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="9b766eefbf80229cc6ea5cf1dafb1ddf1478a4744d726b43cd77f36e2134f904"
Nov 24 21:22:16 minikube kubelet[1307]: I1124 21:22:16.149538    1307 reconciler.go:211] "operationExecutor.UnmountVolume started for volume \"kube-api-access-qpzqv\" (UniqueName: \"kubernetes.io/projected/bd80c4bd-9844-4644-b3e2-1e219a220987-kube-api-access-qpzqv\") pod \"bd80c4bd-9844-4644-b3e2-1e219a220987\" (UID: \"bd80c4bd-9844-4644-b3e2-1e219a220987\") "
Nov 24 21:22:16 minikube kubelet[1307]: I1124 21:22:16.164193    1307 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/bd80c4bd-9844-4644-b3e2-1e219a220987-kube-api-access-qpzqv" (OuterVolumeSpecName: "kube-api-access-qpzqv") pod "bd80c4bd-9844-4644-b3e2-1e219a220987" (UID: "bd80c4bd-9844-4644-b3e2-1e219a220987"). InnerVolumeSpecName "kube-api-access-qpzqv". PluginName "kubernetes.io/projected", VolumeGidValue ""
Nov 24 21:22:16 minikube kubelet[1307]: I1124 21:22:16.250877    1307 reconciler.go:399] "Volume detached for volume \"kube-api-access-qpzqv\" (UniqueName: \"kubernetes.io/projected/bd80c4bd-9844-4644-b3e2-1e219a220987-kube-api-access-qpzqv\") on node \"minikube\" DevicePath \"\""
Nov 24 21:22:17 minikube kubelet[1307]: I1124 21:22:17.712726    1307 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=bd80c4bd-9844-4644-b3e2-1e219a220987 path="/var/lib/kubelet/pods/bd80c4bd-9844-4644-b3e2-1e219a220987/volumes"
Nov 24 21:24:07 minikube kubelet[1307]: E1124 21:24:07.919172    1307 remote_image.go:222] "PullImage from image service failed" err="rpc error: code = Unknown desc = context deadline exceeded" image="minh123456/system-design-converter:latest"
Nov 24 21:24:07 minikube kubelet[1307]: E1124 21:24:07.919321    1307 kuberuntime_image.go:51] "Failed to pull image" err="rpc error: code = Unknown desc = context deadline exceeded" image="minh123456/system-design-converter:latest"
Nov 24 21:24:07 minikube kubelet[1307]: E1124 21:24:07.919852    1307 kuberuntime_manager.go:862] container &Container{Name:system-design-converter,Image:minh123456/system-design-converter,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mx2ff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:converter-configmap,},Optional:nil,},SecretRef:nil,},EnvFromSource{Prefix:,ConfigMapRef:nil,SecretRef:&SecretEnvSource{LocalObjectReference:LocalObjectReference{Name:converter-secret,},Optional:nil,},},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod system-design-converter-688d4c979c-cm8jw_default(527362c2-2146-4d69-b691-1326e8a8c6c9): ErrImagePull: rpc error: code = Unknown desc = context deadline exceeded
Nov 24 21:24:07 minikube kubelet[1307]: E1124 21:24:07.920039    1307 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"system-design-converter\" with ErrImagePull: \"rpc error: code = Unknown desc = context deadline exceeded\"" pod="default/system-design-converter-688d4c979c-cm8jw" podUID=527362c2-2146-4d69-b691-1326e8a8c6c9
Nov 24 21:24:09 minikube kubelet[1307]: I1124 21:24:09.785637    1307 reconciler.go:211] "operationExecutor.UnmountVolume started for volume \"kube-api-access-mx2ff\" (UniqueName: \"kubernetes.io/projected/527362c2-2146-4d69-b691-1326e8a8c6c9-kube-api-access-mx2ff\") pod \"527362c2-2146-4d69-b691-1326e8a8c6c9\" (UID: \"527362c2-2146-4d69-b691-1326e8a8c6c9\") "
Nov 24 21:24:09 minikube kubelet[1307]: I1124 21:24:09.869117    1307 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/527362c2-2146-4d69-b691-1326e8a8c6c9-kube-api-access-mx2ff" (OuterVolumeSpecName: "kube-api-access-mx2ff") pod "527362c2-2146-4d69-b691-1326e8a8c6c9" (UID: "527362c2-2146-4d69-b691-1326e8a8c6c9"). InnerVolumeSpecName "kube-api-access-mx2ff". PluginName "kubernetes.io/projected", VolumeGidValue ""
Nov 24 21:24:09 minikube kubelet[1307]: I1124 21:24:09.897444    1307 reconciler.go:399] "Volume detached for volume \"kube-api-access-mx2ff\" (UniqueName: \"kubernetes.io/projected/527362c2-2146-4d69-b691-1326e8a8c6c9-kube-api-access-mx2ff\") on node \"minikube\" DevicePath \"\""
Nov 24 21:24:11 minikube kubelet[1307]: I1124 21:24:11.777103    1307 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=527362c2-2146-4d69-b691-1326e8a8c6c9 path="/var/lib/kubelet/pods/527362c2-2146-4d69-b691-1326e8a8c6c9/volumes"
Nov 24 21:26:06 minikube kubelet[1307]: E1124 21:26:06.930059    1307 remote_image.go:222] "PullImage from image service failed" err="rpc error: code = Unknown desc = context deadline exceeded" image="minh123456/system-design-converter:latest"
Nov 24 21:26:06 minikube kubelet[1307]: E1124 21:26:06.930227    1307 kuberuntime_image.go:51] "Failed to pull image" err="rpc error: code = Unknown desc = context deadline exceeded" image="minh123456/system-design-converter:latest"
Nov 24 21:26:06 minikube kubelet[1307]: E1124 21:26:06.932132    1307 kuberuntime_manager.go:862] container &Container{Name:system-design-converter,Image:minh123456/system-design-converter,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bdhbq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:converter-configmap,},Optional:nil,},SecretRef:nil,},EnvFromSource{Prefix:,ConfigMapRef:nil,SecretRef:&SecretEnvSource{LocalObjectReference:LocalObjectReference{Name:converter-secret,},Optional:nil,},},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod system-design-converter-688d4c979c-7skbm_default(56c4fc60-cae5-4226-b324-992a585cb5b1): ErrImagePull: rpc error: code = Unknown desc = context deadline exceeded
Nov 24 21:26:06 minikube kubelet[1307]: E1124 21:26:06.934516    1307 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"system-design-converter\" with ErrImagePull: \"rpc error: code = Unknown desc = context deadline exceeded\"" pod="default/system-design-converter-688d4c979c-7skbm" podUID=56c4fc60-cae5-4226-b324-992a585cb5b1
Nov 24 21:26:10 minikube kubelet[1307]: I1124 21:26:10.614025    1307 reconciler.go:211] "operationExecutor.UnmountVolume started for volume \"kube-api-access-bdhbq\" (UniqueName: \"kubernetes.io/projected/56c4fc60-cae5-4226-b324-992a585cb5b1-kube-api-access-bdhbq\") pod \"56c4fc60-cae5-4226-b324-992a585cb5b1\" (UID: \"56c4fc60-cae5-4226-b324-992a585cb5b1\") "
Nov 24 21:26:10 minikube kubelet[1307]: I1124 21:26:10.635862    1307 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/56c4fc60-cae5-4226-b324-992a585cb5b1-kube-api-access-bdhbq" (OuterVolumeSpecName: "kube-api-access-bdhbq") pod "56c4fc60-cae5-4226-b324-992a585cb5b1" (UID: "56c4fc60-cae5-4226-b324-992a585cb5b1"). InnerVolumeSpecName "kube-api-access-bdhbq". PluginName "kubernetes.io/projected", VolumeGidValue ""
Nov 24 21:26:10 minikube kubelet[1307]: I1124 21:26:10.716910    1307 reconciler.go:399] "Volume detached for volume \"kube-api-access-bdhbq\" (UniqueName: \"kubernetes.io/projected/56c4fc60-cae5-4226-b324-992a585cb5b1-kube-api-access-bdhbq\") on node \"minikube\" DevicePath \"\""
Nov 24 21:26:13 minikube kubelet[1307]: I1124 21:26:13.661000    1307 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=56c4fc60-cae5-4226-b324-992a585cb5b1 path="/var/lib/kubelet/pods/56c4fc60-cae5-4226-b324-992a585cb5b1/volumes"
Nov 24 21:28:05 minikube kubelet[1307]: E1124 21:28:05.972470    1307 remote_image.go:222] "PullImage from image service failed" err="rpc error: code = Unknown desc = context deadline exceeded" image="minh123456/system-design-converter:latest"
Nov 24 21:28:05 minikube kubelet[1307]: E1124 21:28:05.972601    1307 kuberuntime_image.go:51] "Failed to pull image" err="rpc error: code = Unknown desc = context deadline exceeded" image="minh123456/system-design-converter:latest"
Nov 24 21:28:05 minikube kubelet[1307]: E1124 21:28:05.973361    1307 kuberuntime_manager.go:862] container &Container{Name:system-design-converter,Image:minh123456/system-design-converter,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5vl7x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:converter-configmap,},Optional:nil,},SecretRef:nil,},EnvFromSource{Prefix:,ConfigMapRef:nil,SecretRef:&SecretEnvSource{LocalObjectReference:LocalObjectReference{Name:converter-secret,},Optional:nil,},},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod system-design-converter-688d4c979c-rh4fs_default(c72534eb-f71a-4a50-a779-2449dd59d3d2): ErrImagePull: rpc error: code = Unknown desc = context deadline exceeded
Nov 24 21:28:05 minikube kubelet[1307]: E1124 21:28:05.981864    1307 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"system-design-converter\" with ErrImagePull: \"rpc error: code = Unknown desc = context deadline exceeded\"" pod="default/system-design-converter-688d4c979c-rh4fs" podUID=c72534eb-f71a-4a50-a779-2449dd59d3d2
Nov 24 21:28:07 minikube kubelet[1307]: I1124 21:28:07.840788    1307 reconciler.go:211] "operationExecutor.UnmountVolume started for volume \"kube-api-access-5vl7x\" (UniqueName: \"kubernetes.io/projected/c72534eb-f71a-4a50-a779-2449dd59d3d2-kube-api-access-5vl7x\") pod \"c72534eb-f71a-4a50-a779-2449dd59d3d2\" (UID: \"c72534eb-f71a-4a50-a779-2449dd59d3d2\") "
Nov 24 21:28:07 minikube kubelet[1307]: I1124 21:28:07.872164    1307 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/c72534eb-f71a-4a50-a779-2449dd59d3d2-kube-api-access-5vl7x" (OuterVolumeSpecName: "kube-api-access-5vl7x") pod "c72534eb-f71a-4a50-a779-2449dd59d3d2" (UID: "c72534eb-f71a-4a50-a779-2449dd59d3d2"). InnerVolumeSpecName "kube-api-access-5vl7x". PluginName "kubernetes.io/projected", VolumeGidValue ""
Nov 24 21:28:07 minikube kubelet[1307]: I1124 21:28:07.944984    1307 reconciler.go:399] "Volume detached for volume \"kube-api-access-5vl7x\" (UniqueName: \"kubernetes.io/projected/c72534eb-f71a-4a50-a779-2449dd59d3d2-kube-api-access-5vl7x\") on node \"minikube\" DevicePath \"\""
Nov 24 21:28:09 minikube kubelet[1307]: I1124 21:28:09.710962    1307 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=c72534eb-f71a-4a50-a779-2449dd59d3d2 path="/var/lib/kubelet/pods/c72534eb-f71a-4a50-a779-2449dd59d3d2/volumes"
Nov 24 21:30:05 minikube kubelet[1307]: E1124 21:30:05.011585    1307 remote_image.go:222] "PullImage from image service failed" err="rpc error: code = Unknown desc = context deadline exceeded" image="minh123456/system-design-converter:latest"
Nov 24 21:30:05 minikube kubelet[1307]: E1124 21:30:05.012578    1307 kuberuntime_image.go:51] "Failed to pull image" err="rpc error: code = Unknown desc = context deadline exceeded" image="minh123456/system-design-converter:latest"
Nov 24 21:30:05 minikube kubelet[1307]: E1124 21:30:05.029431    1307 kuberuntime_manager.go:862] container &Container{Name:system-design-converter,Image:minh123456/system-design-converter,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sz2nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:converter-configmap,},Optional:nil,},SecretRef:nil,},EnvFromSource{Prefix:,ConfigMapRef:nil,SecretRef:&SecretEnvSource{LocalObjectReference:LocalObjectReference{Name:converter-secret,},Optional:nil,},},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod system-design-converter-688d4c979c-62ppr_default(4065fc46-f8fe-420f-a35e-aaf42ff7530f): ErrImagePull: rpc error: code = Unknown desc = context deadline exceeded
Nov 24 21:30:05 minikube kubelet[1307]: E1124 21:30:05.029699    1307 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"system-design-converter\" with ErrImagePull: \"rpc error: code = Unknown desc = context deadline exceeded\"" pod="default/system-design-converter-688d4c979c-62ppr" podUID=4065fc46-f8fe-420f-a35e-aaf42ff7530f
Nov 24 21:30:08 minikube kubelet[1307]: I1124 21:30:08.039912    1307 reconciler.go:211] "operationExecutor.UnmountVolume started for volume \"kube-api-access-sz2nm\" (UniqueName: \"kubernetes.io/projected/4065fc46-f8fe-420f-a35e-aaf42ff7530f-kube-api-access-sz2nm\") pod \"4065fc46-f8fe-420f-a35e-aaf42ff7530f\" (UID: \"4065fc46-f8fe-420f-a35e-aaf42ff7530f\") "
Nov 24 21:30:08 minikube kubelet[1307]: I1124 21:30:08.075335    1307 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/4065fc46-f8fe-420f-a35e-aaf42ff7530f-kube-api-access-sz2nm" (OuterVolumeSpecName: "kube-api-access-sz2nm") pod "4065fc46-f8fe-420f-a35e-aaf42ff7530f" (UID: "4065fc46-f8fe-420f-a35e-aaf42ff7530f"). InnerVolumeSpecName "kube-api-access-sz2nm". PluginName "kubernetes.io/projected", VolumeGidValue ""
Nov 24 21:30:08 minikube kubelet[1307]: I1124 21:30:08.142161    1307 reconciler.go:399] "Volume detached for volume \"kube-api-access-sz2nm\" (UniqueName: \"kubernetes.io/projected/4065fc46-f8fe-420f-a35e-aaf42ff7530f-kube-api-access-sz2nm\") on node \"minikube\" DevicePath \"\""
Nov 24 21:30:12 minikube kubelet[1307]: I1124 21:30:12.263552    1307 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=4065fc46-f8fe-420f-a35e-aaf42ff7530f path="/var/lib/kubelet/pods/4065fc46-f8fe-420f-a35e-aaf42ff7530f/volumes"
Nov 24 21:31:01 minikube kubelet[1307]: I1124 21:31:01.649608    1307 log.go:198] http: superfluous response.WriteHeader call from github.com/emicklei/go-restful/v3.(*Response).WriteHeader (response.go:221)
Nov 24 21:32:04 minikube kubelet[1307]: E1124 21:32:04.041196    1307 remote_image.go:222] "PullImage from image service failed" err="rpc error: code = Unknown desc = context deadline exceeded" image="minh123456/system-design-converter:latest"
Nov 24 21:32:04 minikube kubelet[1307]: E1124 21:32:04.041303    1307 kuberuntime_image.go:51] "Failed to pull image" err="rpc error: code = Unknown desc = context deadline exceeded" image="minh123456/system-design-converter:latest"
Nov 24 21:32:04 minikube kubelet[1307]: E1124 21:32:04.042095    1307 kuberuntime_manager.go:862] container &Container{Name:system-design-converter,Image:minh123456/system-design-converter,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r52ml,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:converter-configmap,},Optional:nil,},SecretRef:nil,},EnvFromSource{Prefix:,ConfigMapRef:nil,SecretRef:&SecretEnvSource{LocalObjectReference:LocalObjectReference{Name:converter-secret,},Optional:nil,},},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod system-design-converter-688d4c979c-fqzll_default(7d71aa79-3563-4430-a9a3-866963d86b6d): ErrImagePull: rpc error: code = Unknown desc = context deadline exceeded
Nov 24 21:32:04 minikube kubelet[1307]: E1124 21:32:04.042295    1307 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"system-design-converter\" with ErrImagePull: \"rpc error: code = Unknown desc = context deadline exceeded\"" pod="default/system-design-converter-688d4c979c-fqzll" podUID=7d71aa79-3563-4430-a9a3-866963d86b6d
Nov 24 21:32:09 minikube kubelet[1307]: I1124 21:32:09.389410    1307 reconciler.go:211] "operationExecutor.UnmountVolume started for volume \"kube-api-access-r52ml\" (UniqueName: \"kubernetes.io/projected/7d71aa79-3563-4430-a9a3-866963d86b6d-kube-api-access-r52ml\") pod \"7d71aa79-3563-4430-a9a3-866963d86b6d\" (UID: \"7d71aa79-3563-4430-a9a3-866963d86b6d\") "
Nov 24 21:32:09 minikube kubelet[1307]: I1124 21:32:09.434873    1307 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/7d71aa79-3563-4430-a9a3-866963d86b6d-kube-api-access-r52ml" (OuterVolumeSpecName: "kube-api-access-r52ml") pod "7d71aa79-3563-4430-a9a3-866963d86b6d" (UID: "7d71aa79-3563-4430-a9a3-866963d86b6d"). InnerVolumeSpecName "kube-api-access-r52ml". PluginName "kubernetes.io/projected", VolumeGidValue ""
Nov 24 21:32:09 minikube kubelet[1307]: I1124 21:32:09.506155    1307 reconciler.go:399] "Volume detached for volume \"kube-api-access-r52ml\" (UniqueName: \"kubernetes.io/projected/7d71aa79-3563-4430-a9a3-866963d86b6d-kube-api-access-r52ml\") on node \"minikube\" DevicePath \"\""
Nov 24 21:32:11 minikube kubelet[1307]: I1124 21:32:11.885328    1307 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=7d71aa79-3563-4430-a9a3-866963d86b6d path="/var/lib/kubelet/pods/7d71aa79-3563-4430-a9a3-866963d86b6d/volumes"

* 
* ==> kubernetes-dashboard [32bd48af8396] <==
* 2022/11/23 08:18:55 Using namespace: kubernetes-dashboard
2022/11/23 08:18:55 Using in-cluster config to connect to apiserver
2022/11/23 08:18:55 Using secret token for csrf signing
2022/11/23 08:18:55 Initializing csrf token from kubernetes-dashboard-csrf secret
2022/11/23 08:18:55 Successful initial request to the apiserver, version: v1.25.3
2022/11/23 08:18:55 Generating JWE encryption key
2022/11/23 08:18:55 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2022/11/23 08:18:55 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2022/11/23 08:18:58 Initializing JWE encryption key from synchronized object
2022/11/23 08:18:58 Creating in-cluster Sidecar client
2022/11/23 08:18:58 Successful request to sidecar
2022/11/23 08:18:58 Serving insecurely on HTTP port: 9090
2022/11/23 21:04:32 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2022/11/23 21:10:17 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2022/11/23 21:10:47 Successful request to sidecar
2022/11/23 21:41:35 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2022/11/23 21:47:22 Successful request to sidecar
2022/11/23 22:21:09 Metric client health check failed: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/services/dashboard-metrics-scraper/proxy/healthz": http2: client connection lost. Retrying in 30 seconds.
2022/11/23 22:21:39 Successful request to sidecar
2022/11/24 05:29:10 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2022/11/24 05:29:40 Successful request to sidecar
2022/11/23 08:18:55 Starting overwatch
I1123 21:19:14.346349       1 request.go:601] Waited for 10.101563692s due to client-side throttling, not priority and fairness, request: GET:https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/services/dashboard-metrics-scraper/proxy/healthz
I1123 21:25:08.960905       1 request.go:601] Waited for 2.132181705s due to client-side throttling, not priority and fairness, request: GET:https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/services/dashboard-metrics-scraper/proxy/healthz
I1123 21:50:41.100857       1 request.go:601] Waited for 2.804695223s due to client-side throttling, not priority and fairness, request: GET:https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/services/dashboard-metrics-scraper/proxy/healthz
I1123 22:12:16.801561       1 request.go:601] Waited for 1.623343935s due to client-side throttling, not priority and fairness, request: GET:https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/services/dashboard-metrics-scraper/proxy/healthz

* 
* ==> kubernetes-dashboard [cb360f92f96e] <==
* 2022/11/23 08:18:26 Synchronizer kubernetes-dashboard-key-holder-kubernetes-dashboard exited with error: kubernetes-dashboard-key-holder-kubernetes-dashboard watch ended with timeout
2022/11/23 08:18:28 Restarting synchronizer: kubernetes-dashboard-key-holder-kubernetes-dashboard.
2022/11/23 08:18:28 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2022/11/23 08:18:28 Synchronizer kubernetes-dashboard-key-holder-kubernetes-dashboard exited with error: kubernetes-dashboard-key-holder-kubernetes-dashboard watch ended with timeout
2022/11/23 08:18:30 Restarting synchronizer: kubernetes-dashboard-key-holder-kubernetes-dashboard.
2022/11/23 08:18:30 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2022/11/23 08:18:30 Synchronizer kubernetes-dashboard-key-holder-kubernetes-dashboard exited with error: kubernetes-dashboard-key-holder-kubernetes-dashboard watch ended with timeout
2022/11/23 08:18:32 Restarting synchronizer: kubernetes-dashboard-key-holder-kubernetes-dashboard.
2022/11/23 08:18:32 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2022/11/23 08:18:32 Synchronizer kubernetes-dashboard-key-holder-kubernetes-dashboard exited with error: kubernetes-dashboard-key-holder-kubernetes-dashboard watch ended with timeout
2022/11/23 08:18:34 Restarting synchronizer: kubernetes-dashboard-key-holder-kubernetes-dashboard.
2022/11/23 08:18:34 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2022/11/23 08:18:34 Synchronizer kubernetes-dashboard-key-holder-kubernetes-dashboard exited with error: kubernetes-dashboard-key-holder-kubernetes-dashboard watch ended with timeout
2022/11/23 08:18:50 Restarting synchronizer: kubernetes-dashboard-key-holder-kubernetes-dashboard.
2022/11/23 08:18:50 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2022/11/23 08:18:50 Synchronizer kubernetes-dashboard-key-holder-kubernetes-dashboard exited with error: kubernetes-dashboard-key-holder-kubernetes-dashboard watch ended with timeout
E1123 08:18:52.705920       1 runtime.go:77] Observed a panic: synchronizer kubernetes-dashboard-key-holder-kubernetes-dashboard restart limit execeeded. Restarting pod.
goroutine 19 [running]:
k8s.io/apimachinery/pkg/util/runtime.logPanic({0x16c68e0?, 0xc00012ac40})
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/runtime/runtime.go:75 +0x99
k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0xfffffffe?})
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/runtime/runtime.go:49 +0x75
panic({0x16c68e0, 0xc00012ac40})
	/opt/hostedtoolcache/go/1.19.0/x64/src/runtime/panic.go:884 +0x212
github.com/kubernetes/dashboard/src/app/backend/sync.(*overwatch).monitorRestartEvents.func1()
	/home/runner/work/dashboard/dashboard/src/app/backend/sync/overwatch.go:92 +0x159
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc0002e5f08?)
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/wait/wait.go:157 +0x3e
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0x0?, {0x1c23980, 0xc000375b30}, 0x1, 0xc00008aba0)
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/wait/wait.go:158 +0xb6
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0x0?, 0x0, 0x0, 0x5?, 0xc00005c7d0?)
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/wait/wait.go:135 +0x89
k8s.io/apimachinery/pkg/util/wait.Until(...)
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/wait/wait.go:92
k8s.io/apimachinery/pkg/util/wait.Forever(0x0?, 0xc0002dbb60?)
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/wait/wait.go:83 +0x28
created by github.com/kubernetes/dashboard/src/app/backend/sync.(*overwatch).monitorRestartEvents
	/home/runner/work/dashboard/dashboard/src/app/backend/sync/overwatch.go:88 +0x98
panic: synchronizer kubernetes-dashboard-key-holder-kubernetes-dashboard restart limit execeeded. Restarting pod. [recovered]
	panic: synchronizer kubernetes-dashboard-key-holder-kubernetes-dashboard restart limit execeeded. Restarting pod.

goroutine 19 [running]:
k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0xfffffffe?})
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/runtime/runtime.go:56 +0xd7
panic({0x16c68e0, 0xc00012ac40})
	/opt/hostedtoolcache/go/1.19.0/x64/src/runtime/panic.go:884 +0x212
github.com/kubernetes/dashboard/src/app/backend/sync.(*overwatch).monitorRestartEvents.func1()
	/home/runner/work/dashboard/dashboard/src/app/backend/sync/overwatch.go:92 +0x159
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc0002e5f08?)
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/wait/wait.go:157 +0x3e
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0x0?, {0x1c23980, 0xc000375b30}, 0x1, 0xc00008aba0)
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/wait/wait.go:158 +0xb6
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0x0?, 0x0, 0x0, 0x5?, 0xc00005c7d0?)
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/wait/wait.go:135 +0x89
k8s.io/apimachinery/pkg/util/wait.Until(...)
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/wait/wait.go:92
k8s.io/apimachinery/pkg/util/wait.Forever(0x0?, 0xc0002dbb60?)
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/wait/wait.go:83 +0x28
created by github.com/kubernetes/dashboard/src/app/backend/sync.(*overwatch).monitorRestartEvents
	/home/runner/work/dashboard/dashboard/src/app/backend/sync/overwatch.go:88 +0x98

* 
* ==> storage-provisioner [ab152c6ac6e4] <==
* k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc000358520, 0x18b3d60, 0xc00033a000, 0x1, 0xc000088f60)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc000358520, 0x3b9aca00, 0x0, 0x1, 0xc000088f60)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc000358520, 0x3b9aca00, 0xc000088f60)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x3d6

goroutine 21125 [sync.Cond.Wait, 2 minutes]:
sync.runtime_notifyListWait(0xc0004185c0, 0x6)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc0004185b0)
	/usr/local/go/src/sync/cond.go:56 +0x99
golang.org/x/net/http2.(*pipe).Read(0xc0004185a8, 0xc0005c0601, 0x5ff, 0x5ff, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/pipe.go:65 +0x97
golang.org/x/net/http2.transportResponseBody.Read(0xc000418580, 0xc0005c0601, 0x5ff, 0x5ff, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/transport.go:2108 +0xaf
encoding/json.(*Decoder).refill(0xc000418840, 0xa, 0x9)
	/usr/local/go/src/encoding/json/stream.go:165 +0xeb
encoding/json.(*Decoder).readValue(0xc000418840, 0x0, 0x0, 0x152aee0)
	/usr/local/go/src/encoding/json/stream.go:140 +0x1ff
encoding/json.(*Decoder).Decode(0xc000418840, 0x154a160, 0xc0002b6900, 0x203000, 0x203000)
	/usr/local/go/src/encoding/json/stream.go:63 +0x7c
k8s.io/apimachinery/pkg/util/framer.(*jsonFrameReader).Read(0xc0001945d0, 0xc000416400, 0x400, 0x400, 0x40, 0x38, 0x15b0440)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/framer/framer.go:152 +0x1a8
k8s.io/apimachinery/pkg/runtime/serializer/streaming.(*decoder).Decode(0xc0003f9130, 0x0, 0x18bc168, 0xc00042f140, 0x0, 0x0, 0x461dc0, 0xc0005639e0, 0xc0000abe50)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/runtime/serializer/streaming/streaming.go:77 +0x89
k8s.io/client-go/rest/watch.(*Decoder).Decode(0xc000359460, 0xc0000abef0, 0x8, 0x18baa48, 0xc000274380, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/rest/watch/decoder.go:49 +0x6e
k8s.io/apimachinery/pkg/watch.(*StreamWatcher).receive(0xc00042f8c0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:104 +0x14a
created by k8s.io/apimachinery/pkg/watch.NewStreamWatcher
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:71 +0xbe

goroutine 21576 [sync.Cond.Wait, 2 minutes]:
sync.runtime_notifyListWait(0xc00042a300, 0xc000000000)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc00042a2f0)
	/usr/local/go/src/sync/cond.go:56 +0x99
golang.org/x/net/http2.(*pipe).Read(0xc00042a2e8, 0xc0001e4e00, 0x200, 0x200, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/pipe.go:65 +0x97
golang.org/x/net/http2.transportResponseBody.Read(0xc00042a2c0, 0xc0001e4e00, 0x200, 0x200, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/transport.go:2108 +0xaf
encoding/json.(*Decoder).refill(0xc000285080, 0x2, 0x7f7c654f5528)
	/usr/local/go/src/encoding/json/stream.go:165 +0xeb
encoding/json.(*Decoder).readValue(0xc000285080, 0x0, 0x0, 0x152aee0)
	/usr/local/go/src/encoding/json/stream.go:140 +0x1ff
encoding/json.(*Decoder).Decode(0xc000285080, 0x154a160, 0xc0002b6ab0, 0x203000, 0x203000)
	/usr/local/go/src/encoding/json/stream.go:63 +0x7c
k8s.io/apimachinery/pkg/util/framer.(*jsonFrameReader).Read(0xc00033b7d0, 0xc0002bc800, 0x400, 0x400, 0x40, 0x38, 0x15b0440)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/framer/framer.go:152 +0x1a8
k8s.io/apimachinery/pkg/runtime/serializer/streaming.(*decoder).Decode(0xc0005dff90, 0x0, 0x18bc168, 0xc00042f280, 0xffffffffffffffff, 0xff00000000000020, 0xc0004f44d0, 0x8c0686, 0x179f200)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/runtime/serializer/streaming/streaming.go:77 +0x89
k8s.io/client-go/rest/watch.(*Decoder).Decode(0xc000359020, 0x20, 0xc0004f44d0, 0x8c0613, 0x179f1c8, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/rest/watch/decoder.go:49 +0x6e
k8s.io/apimachinery/pkg/watch.(*StreamWatcher).receive(0xc00042f240)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:104 +0x14a
created by k8s.io/apimachinery/pkg/watch.NewStreamWatcher
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:71 +0xbe

* 
* ==> storage-provisioner [dd368e0e79de] <==
* I1124 20:00:45.334360       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1124 20:00:45.764064       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1124 20:00:45.768425       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1124 20:01:03.521078       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1124 20:01:03.523280       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_ef2cc857-73e0-4f30-ab8d-4d488c6ff2e5!
I1124 20:01:03.550404       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"9cdb1e12-0034-4eef-b68e-446da55d5a6b", APIVersion:"v1", ResourceVersion:"308849", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_ef2cc857-73e0-4f30-ab8d-4d488c6ff2e5 became leader
I1124 20:01:03.825548       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_ef2cc857-73e0-4f30-ab8d-4d488c6ff2e5!

